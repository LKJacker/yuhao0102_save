<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zn-ch">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-center-simple.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="C++,">










<meta name="description" content="前言首先需要对reduce算法进行介绍。reduce算法本质上就是计算x=x0⊗x1⊗x2⊗x3……⊗xn−1⊗xn 。下面本文将详细说明如何在GPU中实现reduce算法并进行深入地优化。 并行算法设计在GPU中，reduce采用了一种树形的计算方式。如下图所示。  从上至下，将数据不断地累加，直到得出最后的结果，即25。但由于GPU没有针对global数据的同步操作，只能针对block的数据进">
<meta name="keywords" content="C++">
<meta property="og:type" content="article">
<meta property="og:title" content="深入浅出GPU优化系列">
<meta property="og:url" content="http://yoursite.com/2022/09/10/GPU上gemm优化/index.html">
<meta property="og:site_name" content="Hao Yu&#39;s blog">
<meta property="og:description" content="前言首先需要对reduce算法进行介绍。reduce算法本质上就是计算x=x0⊗x1⊗x2⊗x3……⊗xn−1⊗xn 。下面本文将详细说明如何在GPU中实现reduce算法并进行深入地优化。 并行算法设计在GPU中，reduce采用了一种树形的计算方式。如下图所示。  从上至下，将数据不断地累加，直到得出最后的结果，即25。但由于GPU没有针对global数据的同步操作，只能针对block的数据进">
<meta property="og:locale" content="zn-ch">
<meta property="og:image" content="http://yoursite.com/img/image-20220910175222544.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910175237258.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910175717727.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910175947839.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910180155032.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910213658483.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910213713143.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910214848982.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910215005453.png">
<meta property="og:updated_time" content="2022-09-10T13:56:53.791Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="深入浅出GPU优化系列">
<meta name="twitter:description" content="前言首先需要对reduce算法进行介绍。reduce算法本质上就是计算x=x0⊗x1⊗x2⊗x3……⊗xn−1⊗xn 。下面本文将详细说明如何在GPU中实现reduce算法并进行深入地优化。 并行算法设计在GPU中，reduce采用了一种树形的计算方式。如下图所示。  从上至下，将数据不断地累加，直到得出最后的结果，即25。但由于GPU没有针对global数据的同步操作，只能针对block的数据进">
<meta name="twitter:image" content="http://yoursite.com/img/image-20220910175222544.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":true,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2022/09/10/GPU上gemm优化/">





  <title>深入浅出GPU优化系列 | Hao Yu's blog</title>
  








 
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zn-ch">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hao Yu's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">The program monkey was eaten by the siege lion.</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/resume.pdf" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-mybetterhalf">
          <a href="/mybetterhalf/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br>
            
            mybetterhalf
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2022/09/10/GPU上gemm优化/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Hao Yu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hao Yu's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">深入浅出GPU优化系列</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2022-09-10T17:51:00+08:00">
                2022-09-10
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>首先需要对reduce算法进行介绍。reduce算法本质上就是计算x=x0⊗x1⊗x2⊗x3……⊗xn−1⊗xn 。下面本文将详细说明如何在GPU中实现reduce算法并进行深入地优化。</p>
<h2 id="并行算法设计"><a href="#并行算法设计" class="headerlink" title="并行算法设计"></a>并行算法设计</h2><p>在GPU中，reduce采用了一种树形的计算方式。如下图所示。</p>
<p><img src="/img/image-20220910175222544.png" alt="image-20220910175222544"></p>
<p>从上至下，将数据不断地累加，直到得出最后的结果，即25。但由于GPU没有针对global数据的同步操作，只能针对block的数据进行同步。所以，一般而言将reduce分为两个阶段，其示意图如下：</p>
<p><img src="/img/image-20220910175237258.png" alt="image-20220910175237258"></p>
<p>我们仔细来看看这个事，假设给定一个长度为N的数组，需要计算该数组的所有元素之和。首先需要将数组分为m个小份。而后，在第一阶段中，开启m个block计算出m个小份的reduce值。最后，在第二阶段中，使用一个block将m个小份再次进行reduce，得到最终的结果。由于第二阶段本质上是可以调用第一个阶段的kernel，所以不做单独说明，本文只是探索<strong>第一阶段</strong>的优化技巧。</p>
<p>所以kernel接口为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__global__ void reduce(T *input, T* output)</span><br></pre></td></tr></table></figure>
<p>其中，input代表输入的数组，即一个长度为N的数组，output代表输出数组，即第一阶段的结果，即长度为M的数组。随后要开始激动人心的coding阶段，但在CUDA编程中，我们首先需要设置三个参数:</p>
<ol>
<li><strong>BlockNum</strong>：即开启的block数量，即上面所说的M，代表需要将数组切分为几份。</li>
<li><strong>Thread_per_block</strong>:每个block中开启的线程数，一般而言，取128，256，512，1024这几个参数会比较多。</li>
<li><strong>Num_per_block</strong>:每个block需要进行reduce操作的长度。</li>
</ol>
<p>其中，<code>BlockNum* Num_per_block=N</code>。</p>
<h1 id="reduce优化"><a href="#reduce优化" class="headerlink" title="reduce优化"></a>reduce优化</h1><h2 id="reduce-baseline算法介绍"><a href="#reduce-baseline算法介绍" class="headerlink" title="reduce baseline算法介绍"></a>reduce baseline算法介绍</h2><p>Baseline算法比较简单，分为三个步骤。第一个步骤是将数据load至shared memory中，第二个步骤是在shared memory中对数据进行reduce操作，第三个步骤是将最后的结果写回global memory中。代码如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">reduce0</span><span class="params">(<span class="keyword">float</span> *d_in,<span class="keyword">float</span> *d_out)</span></span>&#123;</span><br><span class="line">    __shared__ <span class="keyword">float</span> sdata[THREAD_PER_BLOCK];</span><br><span class="line"></span><br><span class="line">    <span class="comment">//each thread loads one element from global memory to shared mem</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> i=blockIdx.x*blockDim.x+threadIdx.x;</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> tid=threadIdx.x;</span><br><span class="line">    sdata[tid]=d_in[i];</span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// do reduction in shared mem</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">unsigned</span> <span class="keyword">int</span> s=<span class="number">1</span>; s&lt;blockDim.x; s*=<span class="number">2</span>)&#123;</span><br><span class="line">        <span class="keyword">if</span>(tid%(<span class="number">2</span>*s) == <span class="number">0</span>)&#123;</span><br><span class="line">            sdata[tid]+=sdata[tid+s];</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// write result for this block to global mem</span></span><br><span class="line">    <span class="keyword">if</span>(tid==<span class="number">0</span>)d_out[blockIdx.x]=sdata[tid];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在进行优化之前，我们需要再来好好地梳理一下这个baseline代码。优化的本质是通过软件榨干硬件资源，所以必须清楚地了解代码在硬件上的执行过程才能更好地进行优化。</p>
<p>在<strong>第一个步骤</strong>中，我们让Num_per_block与Thread_per_block一致，每个block设定为256个线程，一个block负责256个数据的reduce工作。假设需要处理32M的数据，则有128K个block。tid代表线程号，i代表在原始数组中的索引号。第tid号线程将第i号的数据从global中取出，放到shared memory的第tid元素中。比如在第0号block中，0号线程将0号元素取出，放到shared memory的第0号位置。示意图见：</p>
<p><img src="/img/image-20220910175717727.png" alt="image-20220910175717727"></p>
<p>从硬件角度来分析一下代码。为了执行代码，GPU需要分配两种资源，一个是<strong>存储资源</strong>，一个是<strong>计算资源</strong>。<strong>存储资源</strong>包括在global memory中分配的一块<code>32M× sizeof(float)</code>的空间以及在shared memory中分配的<code>256× sizeof(float)</code>的空间。需要注意的是，<strong>shared memory存在bank冲突的问题，因而需要格外小心</strong>。 <strong>计算资源</strong>其实是根据thread数量来确定的，一个block中分配256个thread线程，32个线程为一组，绑定在一个SIMD单元。所以256个线程可以简单地理解为分配了8组SIMD单元。</p>
<p>（但实际的硬件资源分配不是这样，因为一个SM的计算资源有限，不可能真的给每一个block都分配这么多的SIMD单元。）总而言之，在第一个阶段，就是tid号线程将i号数据从global memory中取出，再放进shared memory中，严谨一点的话，中间是走一遍寄存器再到shared memory中的。</p>
<p>到了<strong>第二个阶段</strong>，block中需要计算的256个元素已经全部被存储在了shared memory中，此时需要对其进行reduce操作。这个过程需要进行多轮迭代，在第一轮迭代中，如果tid%2 ==0, 则第tid号线程将shared memory中第tid号位置的值和第tid+1号的值进行相加，而后放在第tid号位置。</p>
<p>在第二轮迭代中，如果tid%4==0,则第tid号线程将shared memory中第tid号位置的值和第tid+2号的值进行相加，而后放在第tid号位置。不断迭代，则所有元素都将被累加到第0号位置。其示意图如下。其中，红色的线程代表符合if条件的线程，只有它们有任务，需要干活。</p>
<p><img src="/img/image-20220910175947839.png" alt="image-20220910175947839"></p>
<p>在<strong>第三个阶段</strong>中，block负责的256个元素之和都放置在shared memory的0号位置，此时，只需要将0号位置的元素写回即可。</p>
<h2 id="优化技巧1：解决warp-divergence"><a href="#优化技巧1：解决warp-divergence" class="headerlink" title="优化技巧1：解决warp divergence"></a>优化技巧1：解决warp divergence</h2><h3 id="现有问题"><a href="#现有问题" class="headerlink" title="现有问题"></a>现有问题</h3><p>目前reduce0存在的最大问题就是<strong>warp divergent</strong>的问题。对于一个block而言，它所有的thread都是执行同一条指令。如果存在if-else这样的分支情况的话，thread会执行所有的分支。只是不满足条件的分支，所产生的结果不会记录下来。可以在上图中看到，在每一轮迭代中都会产生两个分支，分别是红色和橙色的分支。这严重影响了代码执行的效率。</p>
<h3 id="解决方式"><a href="#解决方式" class="headerlink" title="解决方式"></a>解决方式</h3><p>解决的方式也比较明了，就是尽可能地让所有线程走到同一个分支里面。代码示意如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">reduce1</span><span class="params">(<span class="keyword">float</span> *d_in,<span class="keyword">float</span> *d_out)</span></span>&#123;</span><br><span class="line">    __shared__ <span class="keyword">float</span> sdata[THREAD_PER_BLOCK];</span><br><span class="line"></span><br><span class="line">    <span class="comment">//each thread loads one element from global memory to shared mem</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> i=blockIdx.x*blockDim.x+threadIdx.x;</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> tid=threadIdx.x;</span><br><span class="line">    sdata[tid]=d_in[i];</span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// do reduction in shared mem</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">unsigned</span> <span class="keyword">int</span> s=<span class="number">1</span>; s&lt;blockDim.x; s*=<span class="number">2</span>)&#123;</span><br><span class="line">        <span class="keyword">int</span> index = <span class="number">2</span>*s*tid;</span><br><span class="line">        <span class="keyword">if</span>(index &lt; blockDim.x)&#123;</span><br><span class="line">            sdata[index]+=sdata[index+s];</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// write result for this block to global mem</span></span><br><span class="line">    <span class="keyword">if</span>(tid==<span class="number">0</span>)d_out[blockIdx.x]=sdata[tid];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>虽然代码依旧存在着if语句，但是却与reduce0代码有所不同。我们继续假定block中存在256个thread，即拥有256/32=8个warp。当进行<strong>第1次迭代</strong>时，0-3号warp的<code>index&lt;blockDim.x</code>， 4-7号warp的<code>index&gt;=blockDim.x</code>。对于每个warp而言，都只是进入到一个分支内，所以并不会存在warp divergence的情况。</p>
<p>当进行<strong>第2次迭代</strong>时，0、1号两个warp进入计算分支。当进行<strong>第3次迭代</strong>时，只有0号warp进入计算分支。当进行<strong>第4次迭代</strong>时，只有0号warp的前16个线程进入分支。此时开始产生warp divergence。通过这种方式，我们消除了前3次迭代的warp divergence。</p>
<h2 id="优化技巧2：解决bank冲突"><a href="#优化技巧2：解决bank冲突" class="headerlink" title="优化技巧2：解决bank冲突"></a>优化技巧2：解决bank冲突</h2><h3 id="现有问题-1"><a href="#现有问题-1" class="headerlink" title="现有问题"></a>现有问题</h3><p>reduce1的最大问题是<strong>bank冲突</strong>。我们把目光聚焦在这个for循环中。并且只聚焦在<strong>0号warp</strong>。在<strong>第一次迭代</strong>中，0号线程需要去load shared memory的0号地址以及1号地址的数，然后写回到0号地址。而此时，这个warp中的16号线程，需要去load shared memory中的32号地址和33号地址。可以发现，0号地址跟32号地址产生了<strong>2路的bank冲突</strong>。</p>
<p>在<strong>第2次迭代</strong>中，0号线程需要去load shared memory中的0号地址和2号地址。这个warp中的8号线程需要load shared memory中的32号地址以及34号地址，16号线程需要load shared memory中的64号地址和68号地址，24号线程需要load shared memory中的96号地址和100号地址。</p>
<p>又因为0、32、64、96号地址对应着同一个bank，所以此时产生了<strong>4路的bank冲突</strong>。现在，可以继续算下去，8路bank冲突，16路bank冲突。由于bank冲突，所以reduce1性能受限。下图说明了在load第一个数据时所产生的bank冲突。</p>
<p><img src="/img/image-20220910180155032.png" alt="image-20220910180155032"></p>
<h3 id="解决方式-1"><a href="#解决方式-1" class="headerlink" title="解决方式"></a>解决方式</h3><p>在reduce中，解决bank冲突的方式就是把for循环逆着来。原来stride从0到256，现在stride从128到0。其伪代码如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">reduce2</span><span class="params">(<span class="keyword">float</span> *d_in,<span class="keyword">float</span> *d_out)</span></span>&#123;</span><br><span class="line">    __shared__ <span class="keyword">float</span> sdata[THREAD_PER_BLOCK];</span><br><span class="line"></span><br><span class="line">    <span class="comment">//each thread loads one element from global memory to shared mem</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> i=blockIdx.x*blockDim.x+threadIdx.x;</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> tid=threadIdx.x;</span><br><span class="line">    sdata[tid]=d_in[i];</span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// do reduction in shared mem</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">unsigned</span> <span class="keyword">int</span> s=blockDim.x/<span class="number">2</span>; s&gt;<span class="number">0</span>; s&gt;&gt;=<span class="number">1</span>)&#123;</span><br><span class="line">        <span class="keyword">if</span>(tid &lt; s)&#123;</span><br><span class="line">            sdata[tid]+=sdata[tid+s];</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// write result for this block to global mem</span></span><br><span class="line">    <span class="keyword">if</span>(tid==<span class="number">0</span>)d_out[blockIdx.x]=sdata[tid];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>那为什么通过这么一个小小的改变就能消除bank冲突呢，我们继续进行分析。</p>
<p>把目光继续看到这个for循环中，并且只分析0号warp。0号线程需要load shared memory的0号元素以及128号元素。1号线程需要load shared memory中的1号元素和129号元素。这一轮迭代中，在读取第一个数时，warp中的32个线程刚好load 一行shared memory数据。再分析第2轮迭代，0号线程load 0号元素和64号元素，1号线程load 1号元素和65号元素。</p>
<p>咦，也是这样，每次load shared memory的一行。再来分析第3轮迭代，0号线程load 0号元素和32号元素，接下来不写了，总之，一个warp load shared memory的一行。没有bank冲突。到了4轮迭代，0号线程load 0号元素和16号元素。那16号线程呢，16号线程啥也不干，因为s=16，16-31号线程啥也不干，跳过去了。示意图如下：</p>
<p><img src="/img/image-20220910213658483.png" alt="image-20220910213658483"></p>
<h2 id="优化技巧3：解决idle线程"><a href="#优化技巧3：解决idle线程" class="headerlink" title="优化技巧3：解决idle线程"></a>优化技巧3：解决idle线程</h2><h3 id="现有问题-2"><a href="#现有问题-2" class="headerlink" title="现有问题"></a>现有问题</h3><p><strong>reduce2</strong>最大的问题就是线程的浪费。可以看到我们启动了256个线程，但是在第1轮迭代时只有128个线程在干活，第2轮迭代只有64个线程在干活，每次干活的线程都会减少一半。第一轮迭代示意图如下，只有前128个线程在load数据。后128个线程啥也不干，光看着。</p>
<p><img src="/img/image-20220910213713143.png" alt="image-20220910213713143"></p>
<h3 id="解决方式-2"><a href="#解决方式-2" class="headerlink" title="解决方式"></a>解决方式</h3><p>对于HPC从业者而言，我们希望变成GPU的资本家，去尽可能地压榨GPU。但是呢，在这里，每一次迭代有一半的线程不干活。而且，128-255号线程最过分，它娘的，没有任何贡献，啥也不干。想来想去，能不能让它们干点活呢。想来想去，那这样吧，让它好歹做一次加法。除了去global memory中取数外，再做一次加法。当然为了实现这个，block数就得改一改了。Block数量减少，Num_per_block增加一倍。也就是说原来一个block只需要管256个数就行，现在得管512个数了。代码如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">reduce3</span><span class="params">(<span class="keyword">float</span> *d_in,<span class="keyword">float</span> *d_out)</span></span>&#123;</span><br><span class="line">    __shared__ <span class="keyword">float</span> sdata[THREAD_PER_BLOCK];</span><br><span class="line"></span><br><span class="line">    <span class="comment">//each thread loads one element from global memory to shared mem</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> i=blockIdx.x*(blockDim.x*<span class="number">2</span>)+threadIdx.x;</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> tid=threadIdx.x;</span><br><span class="line">    sdata[tid]=d_in[i] + d_in[i+blockDim.x];</span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// do reduction in shared mem</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">unsigned</span> <span class="keyword">int</span> s=blockDim.x/<span class="number">2</span>; s&gt;<span class="number">0</span>; s&gt;&gt;=<span class="number">1</span>)&#123;</span><br><span class="line">        <span class="keyword">if</span>(tid &lt; s)&#123;</span><br><span class="line">            sdata[tid]+=sdata[tid+s];</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// write result for this block to global mem</span></span><br><span class="line">    <span class="keyword">if</span>(tid==<span class="number">0</span>)d_out[blockIdx.x]=sdata[tid];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>通过这种方式，将一些idle的线程给利用起来了。</p>
<h2 id="优化技巧4：展开最后一维减少同步"><a href="#优化技巧4：展开最后一维减少同步" class="headerlink" title="优化技巧4：展开最后一维减少同步"></a>优化技巧4：展开最后一维减少同步</h2><h3 id="现有问题-3"><a href="#现有问题-3" class="headerlink" title="现有问题"></a>现有问题</h3><p>对于reduce3来说，性能已经算是比较好了。但是依旧没有达到我们想要的效果。我们再来仔细地看看还有什么可以改进的地方。我们发现，当进行到最后几轮迭代时，此时的block中只有warp0在干活时，线程还在进行<strong>同步</strong>操作。这一条语句造成了极大的浪费。</p>
<h3 id="解决方式-3"><a href="#解决方式-3" class="headerlink" title="解决方式"></a>解决方式</h3><p>由于一个warp中的32个线程其实是在一个SIMD单元上，这32个线程每次都是执行同一条指令，这天然地保持了同步状态，因而当s=32时，即只有一个SIMD单元在工作时，完全可以将__syncthreads()这条同步代码去掉。所以我们将最后一维进行展开以减少同步。伪代码如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">__<span class="function">device__ <span class="keyword">void</span> <span class="title">warpReduce</span><span class="params">(<span class="keyword">volatile</span> <span class="keyword">float</span>* cache,<span class="keyword">int</span> tid)</span></span>&#123;</span><br><span class="line">    cache[tid]+=cache[tid+<span class="number">32</span>];</span><br><span class="line">    cache[tid]+=cache[tid+<span class="number">16</span>];</span><br><span class="line">    cache[tid]+=cache[tid+<span class="number">8</span>];</span><br><span class="line">    cache[tid]+=cache[tid+<span class="number">4</span>];</span><br><span class="line">    cache[tid]+=cache[tid+<span class="number">2</span>];</span><br><span class="line">    cache[tid]+=cache[tid+<span class="number">1</span>];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">reduce4</span><span class="params">(<span class="keyword">float</span> *d_in,<span class="keyword">float</span> *d_out)</span></span>&#123;</span><br><span class="line">    __shared__ <span class="keyword">float</span> sdata[THREAD_PER_BLOCK];</span><br><span class="line"></span><br><span class="line">    <span class="comment">//each thread loads one element from global memory to shared mem</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> i=blockIdx.x*(blockDim.x*<span class="number">2</span>)+threadIdx.x;</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> tid=threadIdx.x;</span><br><span class="line">    sdata[tid]=d_in[i] + d_in[i+blockDim.x];</span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// do reduction in shared mem</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">unsigned</span> <span class="keyword">int</span> s=blockDim.x/<span class="number">2</span>; s&gt;<span class="number">32</span>; s&gt;&gt;=<span class="number">1</span>)&#123;</span><br><span class="line">        <span class="keyword">if</span>(tid &lt; s)&#123;</span><br><span class="line">            sdata[tid]+=sdata[tid+s];</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// write result for this block to global mem</span></span><br><span class="line">    <span class="keyword">if</span>(tid&lt;<span class="number">32</span>)warpReduce(sdata,tid);</span><br><span class="line">    <span class="keyword">if</span>(tid==<span class="number">0</span>)d_out[blockIdx.x]=sdata[tid];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以通过下面的示意图更好地了解，warp0会被绑定在一个SIMD单元上，上面有thread0-thread31。warp1会被绑在另外一个SIMD单元上，上面有thread32-thread63。由于在一个SIMD单元上，然后不管啥时候thread0和thread7肯定是同一状态，不需要同步。而thread0和thread34就不能保证同步，必须用<code>__syncthreads()</code>来保证同步操作。</p>
<h2 id="优化技巧5：完全展开减少计算"><a href="#优化技巧5：完全展开减少计算" class="headerlink" title="优化技巧5：完全展开减少计算"></a>优化技巧5：完全展开减少计算</h2><h3 id="现有问题-4"><a href="#现有问题-4" class="headerlink" title="现有问题"></a>现有问题</h3><p>其实到了这一步，reduce的效率已经足够高了。再进一步优化其实已经非常困难了。为了探索极致的性能表现，Mharris接下来给出的办法是<strong>对for循环进行完全展开</strong>。我觉得这里主要是减少for循环的开销。Mharris的实验表明这种方式有着1.41x的加速比。但是用的机器是G80，十几年前的卡。性能数据也比较老了，至于能不能真的有这么好的加速比，我们拭目以待。</p>
<h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><p>我们将整个for循环进行展开，非常暴力，代码如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">unsigned</span> <span class="keyword">int</span> blockSize&gt;</span><br><span class="line">__<span class="function">device__ <span class="keyword">void</span> <span class="title">warpReduce</span><span class="params">(<span class="keyword">volatile</span> <span class="keyword">float</span>* cache,<span class="keyword">int</span> tid)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(blockSize &gt;= <span class="number">64</span>)cache[tid]+=cache[tid+<span class="number">32</span>];</span><br><span class="line">    <span class="keyword">if</span>(blockSize &gt;= <span class="number">32</span>)cache[tid]+=cache[tid+<span class="number">16</span>];</span><br><span class="line">    <span class="keyword">if</span>(blockSize &gt;= <span class="number">16</span>)cache[tid]+=cache[tid+<span class="number">8</span>];</span><br><span class="line">    <span class="keyword">if</span>(blockSize &gt;= <span class="number">8</span>)cache[tid]+=cache[tid+<span class="number">4</span>];</span><br><span class="line">    <span class="keyword">if</span>(blockSize &gt;= <span class="number">4</span>)cache[tid]+=cache[tid+<span class="number">2</span>];</span><br><span class="line">    <span class="keyword">if</span>(blockSize &gt;= <span class="number">2</span>)cache[tid]+=cache[tid+<span class="number">1</span>];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">unsigned</span> <span class="keyword">int</span> blockSize&gt;</span><br><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">reduce5</span><span class="params">(<span class="keyword">float</span> *d_in,<span class="keyword">float</span> *d_out)</span></span>&#123;</span><br><span class="line">    __shared__ <span class="keyword">float</span> sdata[THREAD_PER_BLOCK];</span><br><span class="line"></span><br><span class="line">    <span class="comment">//each thread loads one element from global memory to shared mem</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> i=blockIdx.x*(blockDim.x*<span class="number">2</span>)+threadIdx.x;</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> tid=threadIdx.x;</span><br><span class="line">    sdata[tid]=d_in[i] + d_in[i+blockDim.x];</span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// do reduction in shared mem</span></span><br><span class="line">    <span class="keyword">if</span>(blockSize&gt;=<span class="number">512</span>)&#123;</span><br><span class="line">        <span class="keyword">if</span>(tid&lt;<span class="number">256</span>)&#123;</span><br><span class="line">            sdata[tid]+=sdata[tid+<span class="number">256</span>];</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(blockSize&gt;=<span class="number">256</span>)&#123;</span><br><span class="line">        <span class="keyword">if</span>(tid&lt;<span class="number">128</span>)&#123;</span><br><span class="line">            sdata[tid]+=sdata[tid+<span class="number">128</span>];</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(blockSize&gt;=<span class="number">128</span>)&#123;</span><br><span class="line">        <span class="keyword">if</span>(tid&lt;<span class="number">64</span>)&#123;</span><br><span class="line">            sdata[tid]+=sdata[tid+<span class="number">64</span>];</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// write result for this block to global mem</span></span><br><span class="line">    <span class="keyword">if</span>(tid&lt;<span class="number">32</span>)warpReduce&lt;blockSize&gt;(sdata,tid);</span><br><span class="line">    <span class="keyword">if</span>(tid==<span class="number">0</span>)d_out[blockIdx.x]=sdata[tid];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="优化技巧6：合理设置block数量"><a href="#优化技巧6：合理设置block数量" class="headerlink" title="优化技巧6：合理设置block数量"></a>优化技巧6：合理设置block数量</h2><h3 id="现有问题-5"><a href="#现有问题-5" class="headerlink" title="现有问题"></a>现有问题</h3><p>当走到这一步的时候，能调的东西已经基本上调完了。我们再把眼光放在block和thread的设置上。之前默认了Num_per_block=Thread_per_block。也就是说，一个block开启256个线程时，这个block负责256个元素的reduce操作。那可不可以让一个block多管点数。这样的话，开启的block数量少一些。以此<strong>对block设置进行调整</strong>，获得最优block取值，这样或许能够带来一些性能收益？</p>
<h3 id="解决方式-4"><a href="#解决方式-4" class="headerlink" title="解决方式"></a>解决方式</h3><p>这样需要再思考一下block的取值。对于GPU而言，block的取值到底是多更好，还是少更好。如此对CUDA编程熟悉的同学，肯定会毫不犹豫地说：“那肯定是多更好啦。Block数量多，block可以进行快速地切换，去掩盖访存的延时。”这个问题按下不表，我们看看Mharris是怎么说的。</p>
<p>如果一个线程被分配更多的work时，可能会更好地覆盖延时。这一点比较好理解。如果线程有更多的work时，对于编译器而言，就可能有更多的机会对相关指令进行重排，从而去覆盖访存时的巨大延时。虽然这句话并没有很好地说明在某种程度上而言，block少一些会更好。但是，有一点不可否认,<strong>block需要进行合理地设置</strong>。唠唠叨叨说了很多，现在把代码贴一下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">unsigned</span> <span class="keyword">int</span> blockSize&gt;</span><br><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">reduce6</span><span class="params">(<span class="keyword">float</span> *d_in,<span class="keyword">float</span> *d_out)</span></span>&#123;</span><br><span class="line">    __shared__ <span class="keyword">float</span> sdata[THREAD_PER_BLOCK];</span><br><span class="line"></span><br><span class="line">    <span class="comment">//each thread loads one element from global memory to shared mem</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> i=blockIdx.x*(blockDim.x*<span class="number">2</span>)+threadIdx.x;</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> tid=threadIdx.x;</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> gridSize = blockSize * <span class="number">2</span> * gridDim.x;</span><br><span class="line">    sdata[tid] = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span>(i&lt;n)&#123;</span><br><span class="line">        sdata[tid] +=d_in[i]+d_in[i+blockSize];</span><br><span class="line">        i+=gridSize;</span><br><span class="line">    &#125;</span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// do reduction in shared mem</span></span><br><span class="line">    <span class="keyword">if</span>(blockSize&gt;=<span class="number">512</span>)&#123;</span><br><span class="line">        <span class="keyword">if</span>(tid&lt;<span class="number">256</span>)&#123;</span><br><span class="line">            sdata[tid]+=sdata[tid+<span class="number">256</span>];</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(blockSize&gt;=<span class="number">256</span>)&#123;</span><br><span class="line">        <span class="keyword">if</span>(tid&lt;<span class="number">128</span>)&#123;</span><br><span class="line">            sdata[tid]+=sdata[tid+<span class="number">128</span>];</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(blockSize&gt;=<span class="number">128</span>)&#123;</span><br><span class="line">        <span class="keyword">if</span>(tid&lt;<span class="number">64</span>)&#123;</span><br><span class="line">            sdata[tid]+=sdata[tid+<span class="number">64</span>];</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// write result for this block to global mem</span></span><br><span class="line">    <span class="keyword">if</span>(tid&lt;<span class="number">32</span>)warpReduce&lt;blockSize&gt;(sdata,tid);</span><br><span class="line">    <span class="keyword">if</span>(tid==<span class="number">0</span>)d_out[blockIdx.x]=sdata[tid];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="优化技巧7：使用shuffle指令"><a href="#优化技巧7：使用shuffle指令" class="headerlink" title="优化技巧7：使用shuffle指令"></a>优化技巧7：使用shuffle指令</h2><h3 id="现有问题-6"><a href="#现有问题-6" class="headerlink" title="现有问题"></a>现有问题</h3><p>其实，对于Mharris的讲义。reduce优化就到此结束了。但是NV后来出了Shuffle指令，对于reduce优化有着非常好的效果。目前绝大多数访存类算子，像是softmax，batch_norm，reduce等，都是用Shuffle实现。所以，在这里谈一下这么把shuffle指令用在reduce优化上。</p>
<p>Shuffle指令是一组针对warp的指令。Shuffle指令最重要的特性就是<strong>warp内的寄存器可以相互访问</strong>。在没有shuffle指令的时候，各个线程在进行通信时只能通过shared memory来访问彼此的寄存器。而采用了shuffle指令之后，warp内的线程可以直接对其他线程的寄存器进行访存。通过这种方式可以减少访存的延时。除此之外，带来的最大好处就是可编程性提高了，在某些场景下，就不用shared memory了。毕竟，开发者要自己去控制 shared memory还是挺麻烦的一个事。</p>
<p>伪代码如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">unsigned</span> <span class="keyword">int</span> blockSize&gt;</span><br><span class="line">__device__ __<span class="function">forceinline__ <span class="keyword">float</span> <span class="title">warpReduceSum</span><span class="params">(<span class="keyword">float</span> sum)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(blockSize &gt;= <span class="number">32</span>)sum += __shfl_down_sync(<span class="number">0xffffffff</span>,sum,<span class="number">16</span>);</span><br><span class="line">    <span class="keyword">if</span>(blockSize &gt;= <span class="number">16</span>)sum += __shfl_down_sync(<span class="number">0xffffffff</span>,sum,<span class="number">8</span>);</span><br><span class="line">    <span class="keyword">if</span>(blockSize &gt;= <span class="number">8</span>)sum += __shfl_down_sync(<span class="number">0xffffffff</span>,sum,<span class="number">4</span>);</span><br><span class="line">    <span class="keyword">if</span>(blockSize &gt;= <span class="number">4</span>)sum += __shfl_down_sync(<span class="number">0xffffffff</span>,sum,<span class="number">2</span>);</span><br><span class="line">    <span class="keyword">if</span>(blockSize &gt;= <span class="number">2</span>)sum += __shfl_down_sync(<span class="number">0xffffffff</span>,sum,<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">return</span> sum;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">unsigned</span> <span class="keyword">int</span> blockSize&gt;</span><br><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">reduce7</span><span class="params">(<span class="keyword">float</span> *d_in,<span class="keyword">float</span> *d_out, <span class="keyword">unsigned</span> <span class="keyword">int</span> n)</span></span>&#123;</span><br><span class="line">    <span class="keyword">float</span> sum = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//each thread loads one element from global memory to shared mem</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> i=blockIdx.x*(blockDim.x*<span class="number">2</span>)+threadIdx.x;</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> tid=threadIdx.x;</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> gridSize = blockSize * <span class="number">2</span> * gridDim.x;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span>(i&lt;n)&#123;</span><br><span class="line">        sdata[tid] +=d_in[i]+d_in[i+blockSize];</span><br><span class="line">        i+=gridSize;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// shared mem for partial sums(one per warp in the block</span></span><br><span class="line">    <span class="keyword">static</span> __shared__ <span class="keyword">float</span> warpLevelSums[WARP_SIZE];</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> laneId = threadIdx.x % WARP_SIZE;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> warpId = threadIdx.x / WARP_SIZE;</span><br><span class="line"></span><br><span class="line">    sum = warpReduceSum&lt;blockSize&gt;(sum);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(laneId == <span class="number">0</span>)warpLevelSums[warpId]=sum;</span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    sum = (threadIdx.x &lt; blockDim.x / WARP_SIZE)? warpLevelSums[laneId]:<span class="number">0</span>;</span><br><span class="line">    <span class="comment">// Final reduce using first warp</span></span><br><span class="line">    <span class="keyword">if</span>(warpId == <span class="number">0</span>)sum = warpReduceSum&lt;blockSize/WARP_SIZE&gt;(sum);</span><br><span class="line">    <span class="comment">// write result for this block to global mem</span></span><br><span class="line">    <span class="keyword">if</span>(tid==<span class="number">0</span>)d_out[blockIdx.x]=sum;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="GEMM优化"><a href="#GEMM优化" class="headerlink" title="GEMM优化"></a>GEMM优化</h1><h2 id="前言-1"><a href="#前言-1" class="headerlink" title="前言"></a>前言</h2><p>在高性能领域，对于<strong>矩阵乘（GEMM）的优化</strong>是一个非常重要的课题。GEMM可以非常广泛地应用于航空航天、流体力学等科学计算领域，这也是之前HPC的主要应用场景。后来深度学习开展地如火如荼，由于对高算力的需要，也成为HPC的主要应用场景之一。这些年涌现了一系列的深度学习模型。模型里面最耗时的东西，包括卷积、全连接层、attention，都可以转换成GEMM操作。所以说，GEMM优化的重要性，怎么突出都不过分。</p>
<p>本篇文章主要介绍GEMM中的数据分块和如何在多级存储进行数据搬运。这也是<strong>HPC优化的核心思想，怎么样让数据放在更近的存储上来掩盖计算的延时，从而减少存储墙的影响</strong>。文章分为四个方面进行叙述，首先介绍在global memory层面如何进行分块以及数据搬运，随后介绍在shared memory层面如何进行分块以及数据搬运，而后介绍在register层面如何进行分块以及避免bank冲突，最后介绍如何进行prefetch以更好地掩盖访存时延。</p>
<h2 id="从global-memory到shared-memory"><a href="#从global-memory到shared-memory" class="headerlink" title="从global memory到shared memory"></a>从global memory到shared memory</h2><p>假设有矩阵A,B，需要计算矩阵A和B的乘，即矩阵C。A、B、C三个矩阵的维度分别为，，m∗k，k∗n，m∗n ，且三个矩阵中的数据都是单精度浮点数。对于C中每一个元素，C[i][j]，可以看作是A的一行和B的一列进行一次归约操作。采用最naive的GEMM算法，在GPU中，一共开启m∗n 个线程，每个线程需要读取矩阵A的一行与矩阵B的一列，而后将计算结果写回至矩阵C中。因而，完成计算一共需要从global memory中进行2mnk 次读操作和m*n次写操作。大量的访存操作使得GEMM效率难以提高，因而考虑global memory中进行分块，并将矩阵块放置到shared memory中。其示意图如下：</p>
<p><img src="/img/image-20220910214848982.png" alt="image-20220910214848982"></p>
<p>对global memory进行分块的GEMM算法示意图见上图右侧。首先将A、B、C三个矩阵划分为多个维度为，，bm∗bk，bk∗bn，bm∗bn 的小矩阵块。三个矩阵形成M∗K，K∗N，M∗N 的小矩阵网格。其中M=m/bm，N=n/bn，K=k/bk 。随后在GPU中开启M∗N 个block，每个block负责C中一个维度为bm∗bn 的小矩阵块的计算。计算中一共有K次迭代，每一次迭代都需要读取A中一个维度为bm∗bk 的小矩阵块和B中一个维度为bk∗bn 的小矩阵块，并将其放置在shared memory中。因而，完成C中所有元素的计算一共需要从global memory中读取M∗N∗K∗（bm∗bk+bk∗bn） ，即m∗n∗k（1/bm+1/bn） 个单精度浮点数。相比于naive的GEMM算法，访存量减少为原来的1/2∗(1/bm+1/bn) 。通过global memory中分块算法极大地减少了对global memory的访存量。并且，相比于naive算法，对global进行分块可以更充分地利用数据局部性。在naive算法中，每一个线程都需要直接从global memory中取数，其时延非常长，计算性能非常差。而进行分块后，将维度为bm∗bk，bk∗bn 的小矩阵块先存储到shared memory之中。而后计算单元进行计算时可以直接从shared memory中取数，大大减少了访存所需要的时延。</p>
<h2 id="从shared-memory到register"><a href="#从shared-memory到register" class="headerlink" title="从shared memory到register"></a>从shared memory到register</h2><p>随后，我们进一步考虑从shared memory到register的过程。在这里，只分析<strong>一个block</strong>中的计算。当进行K轮迭代中某一轮迭代时，GPU将维度为bm∗bk，bk∗bn 的小矩阵块存储到shared memory中，而后各个线程将shared memory中的数据存入register中进行计算。</p>
<p><img src="/img/image-20220910215005453.png" alt="image-20220910215005453"></p>
<p>在<strong>不对shared memory分块</strong>时，一个block中含有bm∗bn 个线程，<strong>每一个线程负责C中一个元素的计算</strong>。则一个block一共需要对shared memory进行2∗bm∗bn∗bk 次读操作。而后<strong>考虑对shared memory进行分块</strong>，对bm∗bn 的小矩阵进行再一次划分，将其划分为多个维度为rm∗rn 的子矩阵。则一个block需要负责X∗Y 个子矩阵。其中，X=bmrm ，Y=bnrn 。随后，在一个block中开启X∗Y 个线程，<strong>每个线程负责一个维度为rm∗rn 的子矩阵的计算</strong>。在计算中，一个block一共需要从shared memory读取X∗Y∗(rm+rn)∗bk ，即bm∗bn∗bk∗(1rm+1rn) 个单精度浮点数。相比于未分块的算法，对于shared memory中的访存量减少为原来的1/2∗(1rm+1rn) 。并且，由于将数据放入register中，可以直接对数据进行运算，减少了从shared memory中取数的时延。</p>
<h2 id="register分块"><a href="#register分块" class="headerlink" title="register分块"></a>register分块</h2><p>在这里，我们考虑最后一层，即register中的计算，并且只分析一个thread。在完成以上的过程后，对于一个线程而言，它现在拥有：rm 个A矩阵的寄存器值，rn 个B矩阵的寄存器值，以及rm∗rn 个C矩阵的寄存器值。通过这些寄存器的值，需要计算rm∗rn 个数。这需要rm∗rn 条FFMA指令。</p>
<p>这个时候会涉及到寄存器的bank conflict。在NV的GPU中，每个SM不仅会产生shared memroy之间的bank 冲突，也会产生寄存器之间的bank冲突。这一点对于计算密集型的算子十分重要。像shared memory一样，寄存器的Register File也会被分为几个bank，如果一条指令的的源寄存器有2个以上来自同一bank，就会产生冲突。指令会重发射，浪费一个cycle。PS：这个地方是从<a href="https://zhuanlan.zhihu.com/p/410278370" target="_blank" rel="noopener">旷视的博客</a>中看的。然后对于maxwell架构的GPU而言，bank数为4，寄存器<strong>id%4</strong>即所属bank。</p>
<p>我们假设对这个thread来说，rm=4,rn=4 。并且计算C的寄存器以一种非常naive的情况分配，如下图左侧所示。则需要产生16条FFMA指令，列举如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">FFMA R0, R16, R20, R0</span><br><span class="line">FFMA R1, R16, R21, R1</span><br><span class="line">……</span><br></pre></td></tr></table></figure>
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/C/" rel="tag"># C++</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2022/09/07/colab上使用cuda/" rel="next" title="Colab上使用cuda">
                <i class="fa fa-chevron-left"></i> Colab上使用cuda
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/header.jpg" alt="Hao Yu">
            
              <p class="site-author-name" itemprop="name">Hao Yu</p>
              <p class="site-description motion-element" itemprop="description">Introduce something interesting and recode learning process, some articles are written by others, the original link has been given as much as possible, thanks to the original author</p>
          </div>
          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">292</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">13</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>
        	<audio controls="controls" loop="loop" preload="auto" src="/resource/xiaomeihao.mp3">
	        </audio>
	

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/yuhao0102" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:yuh18@mails.tsinghua.edu.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#前言"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#并行算法设计"><span class="nav-number">2.</span> <span class="nav-text">并行算法设计</span></a></li></ol><li class="nav-item nav-level-1"><a class="nav-link" href="#reduce优化"><span class="nav-number"></span> <span class="nav-text">reduce优化</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#reduce-baseline算法介绍"><span class="nav-number">1.</span> <span class="nav-text">reduce baseline算法介绍</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#优化技巧1：解决warp-divergence"><span class="nav-number">2.</span> <span class="nav-text">优化技巧1：解决warp divergence</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#现有问题"><span class="nav-number">2.1.</span> <span class="nav-text">现有问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#解决方式"><span class="nav-number">2.2.</span> <span class="nav-text">解决方式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#优化技巧2：解决bank冲突"><span class="nav-number">3.</span> <span class="nav-text">优化技巧2：解决bank冲突</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#现有问题-1"><span class="nav-number">3.1.</span> <span class="nav-text">现有问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#解决方式-1"><span class="nav-number">3.2.</span> <span class="nav-text">解决方式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#优化技巧3：解决idle线程"><span class="nav-number">4.</span> <span class="nav-text">优化技巧3：解决idle线程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#现有问题-2"><span class="nav-number">4.1.</span> <span class="nav-text">现有问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#解决方式-2"><span class="nav-number">4.2.</span> <span class="nav-text">解决方式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#优化技巧4：展开最后一维减少同步"><span class="nav-number">5.</span> <span class="nav-text">优化技巧4：展开最后一维减少同步</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#现有问题-3"><span class="nav-number">5.1.</span> <span class="nav-text">现有问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#解决方式-3"><span class="nav-number">5.2.</span> <span class="nav-text">解决方式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#优化技巧5：完全展开减少计算"><span class="nav-number">6.</span> <span class="nav-text">优化技巧5：完全展开减少计算</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#现有问题-4"><span class="nav-number">6.1.</span> <span class="nav-text">现有问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#解决方法"><span class="nav-number">6.2.</span> <span class="nav-text">解决方法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#优化技巧6：合理设置block数量"><span class="nav-number">7.</span> <span class="nav-text">优化技巧6：合理设置block数量</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#现有问题-5"><span class="nav-number">7.1.</span> <span class="nav-text">现有问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#解决方式-4"><span class="nav-number">7.2.</span> <span class="nav-text">解决方式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#优化技巧7：使用shuffle指令"><span class="nav-number">8.</span> <span class="nav-text">优化技巧7：使用shuffle指令</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#现有问题-6"><span class="nav-number">8.1.</span> <span class="nav-text">现有问题</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#GEMM优化"><span class="nav-number"></span> <span class="nav-text">GEMM优化</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#前言-1"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#从global-memory到shared-memory"><span class="nav-number">2.</span> <span class="nav-text">从global memory到shared memory</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#从shared-memory到register"><span class="nav-number">3.</span> <span class="nav-text">从shared memory到register</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#register分块"><span class="nav-number">4.</span> <span class="nav-text">register分块</span></a></li></ol></li></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>
  <div id="DvelopmentTarget">     
  </div>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart" aria-hidden="false"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hao Yu</span>

  
</div>

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<span id="busuanzi_container_site_uv"> 
  本站访客数<span id="busuanzi_value_site_uv"></span>人次
</span>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  


  

  


  <script type="text/javascript" src="/js/src/love.js"></script>

</body>
</html>
