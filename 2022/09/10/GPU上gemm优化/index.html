<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zn-ch">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-center-simple.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="C++,">










<meta name="description" content="前言首先需要对reduce算法进行介绍。reduce算法本质上就是计算x=x0⊗x1⊗x2⊗x3……⊗xn−1⊗xn 。下面本文将详细说明如何在GPU中实现reduce算法并进行深入地优化。 并行算法设计在GPU中，reduce采用了一种树形的计算方式。如下图所示。  从上至下，将数据不断地累加，直到得出最后的结果，即25。但由于GPU没有针对global数据的同步操作，只能针对block的数据进">
<meta name="keywords" content="C++">
<meta property="og:type" content="article">
<meta property="og:title" content="深入浅出GPU优化系列">
<meta property="og:url" content="http://yoursite.com/2022/09/10/GPU上gemm优化/index.html">
<meta property="og:site_name" content="Hao Yu&#39;s blog">
<meta property="og:description" content="前言首先需要对reduce算法进行介绍。reduce算法本质上就是计算x=x0⊗x1⊗x2⊗x3……⊗xn−1⊗xn 。下面本文将详细说明如何在GPU中实现reduce算法并进行深入地优化。 并行算法设计在GPU中，reduce采用了一种树形的计算方式。如下图所示。  从上至下，将数据不断地累加，直到得出最后的结果，即25。但由于GPU没有针对global数据的同步操作，只能针对block的数据进">
<meta property="og:locale" content="zn-ch">
<meta property="og:image" content="http://yoursite.com/img/image-20220910175222544.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910175237258.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910175717727.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910175947839.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910180155032.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910213658483.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910213713143.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910214848982.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910215005453.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910220109982.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910220136539.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910220741253.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910220755159.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910220841079.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910221016698.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910221600420.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910221941317.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910221952729.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910222351625.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910222431933.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910222443696.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910222453184.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910222504575.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910222717437.png">
<meta property="og:updated_time" content="2022-09-10T14:50:04.047Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="深入浅出GPU优化系列">
<meta name="twitter:description" content="前言首先需要对reduce算法进行介绍。reduce算法本质上就是计算x=x0⊗x1⊗x2⊗x3……⊗xn−1⊗xn 。下面本文将详细说明如何在GPU中实现reduce算法并进行深入地优化。 并行算法设计在GPU中，reduce采用了一种树形的计算方式。如下图所示。  从上至下，将数据不断地累加，直到得出最后的结果，即25。但由于GPU没有针对global数据的同步操作，只能针对block的数据进">
<meta name="twitter:image" content="http://yoursite.com/img/image-20220910175222544.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":true,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2022/09/10/GPU上gemm优化/">





  <title>深入浅出GPU优化系列 | Hao Yu's blog</title>
  








 
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zn-ch">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hao Yu's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">The program monkey was eaten by the siege lion.</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/resume.pdf" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-mybetterhalf">
          <a href="/mybetterhalf/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br>
            
            mybetterhalf
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2022/09/10/GPU上gemm优化/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Hao Yu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hao Yu's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">深入浅出GPU优化系列</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2022-09-10T17:51:00+08:00">
                2022-09-10
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>首先需要对reduce算法进行介绍。reduce算法本质上就是计算x=x0⊗x1⊗x2⊗x3……⊗xn−1⊗xn 。下面本文将详细说明如何在GPU中实现reduce算法并进行深入地优化。</p>
<h2 id="并行算法设计"><a href="#并行算法设计" class="headerlink" title="并行算法设计"></a>并行算法设计</h2><p>在GPU中，reduce采用了一种树形的计算方式。如下图所示。</p>
<p><img src="/img/image-20220910175222544.png" alt="image-20220910175222544"></p>
<p>从上至下，将数据不断地累加，直到得出最后的结果，即25。但由于GPU没有针对global数据的同步操作，只能针对block的数据进行同步。所以，一般而言将reduce分为两个阶段，其示意图如下：</p>
<p><img src="/img/image-20220910175237258.png" alt="image-20220910175237258"></p>
<p>我们仔细来看看这个事，假设给定一个长度为N的数组，需要计算该数组的所有元素之和。首先需要将数组分为m个小份。而后，在第一阶段中，开启m个block计算出m个小份的reduce值。最后，在第二阶段中，使用一个block将m个小份再次进行reduce，得到最终的结果。由于第二阶段本质上是可以调用第一个阶段的kernel，所以不做单独说明，本文只是探索<strong>第一阶段</strong>的优化技巧。</p>
<p>所以kernel接口为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__global__ void reduce(T *input, T* output)</span><br></pre></td></tr></table></figure>
<p>其中，input代表输入的数组，即一个长度为N的数组，output代表输出数组，即第一阶段的结果，即长度为M的数组。随后要开始激动人心的coding阶段，但在CUDA编程中，我们首先需要设置三个参数:</p>
<ol>
<li><strong>BlockNum</strong>：即开启的block数量，即上面所说的M，代表需要将数组切分为几份。</li>
<li><strong>Thread_per_block</strong>:每个block中开启的线程数，一般而言，取128，256，512，1024这几个参数会比较多。</li>
<li><strong>Num_per_block</strong>:每个block需要进行reduce操作的长度。</li>
</ol>
<p>其中，<code>BlockNum* Num_per_block=N</code>。</p>
<h1 id="reduce优化"><a href="#reduce优化" class="headerlink" title="reduce优化"></a>reduce优化</h1><h2 id="reduce-baseline算法介绍"><a href="#reduce-baseline算法介绍" class="headerlink" title="reduce baseline算法介绍"></a>reduce baseline算法介绍</h2><p>Baseline算法比较简单，分为三个步骤。第一个步骤是将数据load至shared memory中，第二个步骤是在shared memory中对数据进行reduce操作，第三个步骤是将最后的结果写回global memory中。代码如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">reduce0</span><span class="params">(<span class="keyword">float</span> *d_in,<span class="keyword">float</span> *d_out)</span></span>&#123;</span><br><span class="line">    __shared__ <span class="keyword">float</span> sdata[THREAD_PER_BLOCK];</span><br><span class="line"></span><br><span class="line">    <span class="comment">//each thread loads one element from global memory to shared mem</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> i=blockIdx.x*blockDim.x+threadIdx.x;</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> tid=threadIdx.x;</span><br><span class="line">    sdata[tid]=d_in[i];</span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// do reduction in shared mem</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">unsigned</span> <span class="keyword">int</span> s=<span class="number">1</span>; s&lt;blockDim.x; s*=<span class="number">2</span>)&#123;</span><br><span class="line">        <span class="keyword">if</span>(tid%(<span class="number">2</span>*s) == <span class="number">0</span>)&#123;</span><br><span class="line">            sdata[tid]+=sdata[tid+s];</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// write result for this block to global mem</span></span><br><span class="line">    <span class="keyword">if</span>(tid==<span class="number">0</span>)d_out[blockIdx.x]=sdata[tid];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在进行优化之前，我们需要再来好好地梳理一下这个baseline代码。优化的本质是通过软件榨干硬件资源，所以必须清楚地了解代码在硬件上的执行过程才能更好地进行优化。</p>
<p>在<strong>第一个步骤</strong>中，我们让Num_per_block与Thread_per_block一致，每个block设定为256个线程，一个block负责256个数据的reduce工作。假设需要处理32M的数据，则有128K个block。tid代表线程号，i代表在原始数组中的索引号。第tid号线程将第i号的数据从global中取出，放到shared memory的第tid元素中。比如在第0号block中，0号线程将0号元素取出，放到shared memory的第0号位置。示意图见：</p>
<p><img src="/img/image-20220910175717727.png" alt="image-20220910175717727"></p>
<p>从硬件角度来分析一下代码。为了执行代码，GPU需要分配两种资源，一个是<strong>存储资源</strong>，一个是<strong>计算资源</strong>。<strong>存储资源</strong>包括在global memory中分配的一块<code>32M× sizeof(float)</code>的空间以及在shared memory中分配的<code>256× sizeof(float)</code>的空间。需要注意的是，<strong>shared memory存在bank冲突的问题，因而需要格外小心</strong>。 <strong>计算资源</strong>其实是根据thread数量来确定的，一个block中分配256个thread线程，32个线程为一组，绑定在一个SIMD单元。所以256个线程可以简单地理解为分配了8组SIMD单元。</p>
<p>（但实际的硬件资源分配不是这样，因为一个SM的计算资源有限，不可能真的给每一个block都分配这么多的SIMD单元。）总而言之，在第一个阶段，就是tid号线程将i号数据从global memory中取出，再放进shared memory中，严谨一点的话，中间是走一遍寄存器再到shared memory中的。</p>
<p>到了<strong>第二个阶段</strong>，block中需要计算的256个元素已经全部被存储在了shared memory中，此时需要对其进行reduce操作。这个过程需要进行多轮迭代，在第一轮迭代中，如果tid%2 ==0, 则第tid号线程将shared memory中第tid号位置的值和第tid+1号的值进行相加，而后放在第tid号位置。</p>
<p>在第二轮迭代中，如果tid%4==0,则第tid号线程将shared memory中第tid号位置的值和第tid+2号的值进行相加，而后放在第tid号位置。不断迭代，则所有元素都将被累加到第0号位置。其示意图如下。其中，红色的线程代表符合if条件的线程，只有它们有任务，需要干活。</p>
<p><img src="/img/image-20220910175947839.png" alt="image-20220910175947839"></p>
<p>在<strong>第三个阶段</strong>中，block负责的256个元素之和都放置在shared memory的0号位置，此时，只需要将0号位置的元素写回即可。</p>
<h2 id="优化技巧1：解决warp-divergence"><a href="#优化技巧1：解决warp-divergence" class="headerlink" title="优化技巧1：解决warp divergence"></a>优化技巧1：解决warp divergence</h2><h3 id="现有问题"><a href="#现有问题" class="headerlink" title="现有问题"></a>现有问题</h3><p>目前reduce0存在的最大问题就是<strong>warp divergent</strong>的问题。对于一个block而言，它所有的thread都是执行同一条指令。如果存在if-else这样的分支情况的话，thread会执行所有的分支。只是不满足条件的分支，所产生的结果不会记录下来。可以在上图中看到，在每一轮迭代中都会产生两个分支，分别是红色和橙色的分支。这严重影响了代码执行的效率。</p>
<h3 id="解决方式"><a href="#解决方式" class="headerlink" title="解决方式"></a>解决方式</h3><p>解决的方式也比较明了，就是尽可能地让所有线程走到同一个分支里面。代码示意如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">reduce1</span><span class="params">(<span class="keyword">float</span> *d_in,<span class="keyword">float</span> *d_out)</span></span>&#123;</span><br><span class="line">    __shared__ <span class="keyword">float</span> sdata[THREAD_PER_BLOCK];</span><br><span class="line"></span><br><span class="line">    <span class="comment">//each thread loads one element from global memory to shared mem</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> i=blockIdx.x*blockDim.x+threadIdx.x;</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> tid=threadIdx.x;</span><br><span class="line">    sdata[tid]=d_in[i];</span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// do reduction in shared mem</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">unsigned</span> <span class="keyword">int</span> s=<span class="number">1</span>; s&lt;blockDim.x; s*=<span class="number">2</span>)&#123;</span><br><span class="line">        <span class="keyword">int</span> index = <span class="number">2</span>*s*tid;</span><br><span class="line">        <span class="keyword">if</span>(index &lt; blockDim.x)&#123;</span><br><span class="line">            sdata[index]+=sdata[index+s];</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// write result for this block to global mem</span></span><br><span class="line">    <span class="keyword">if</span>(tid==<span class="number">0</span>)d_out[blockIdx.x]=sdata[tid];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>虽然代码依旧存在着if语句，但是却与reduce0代码有所不同。我们继续假定block中存在256个thread，即拥有256/32=8个warp。当进行<strong>第1次迭代</strong>时，0-3号warp的<code>index&lt;blockDim.x</code>， 4-7号warp的<code>index&gt;=blockDim.x</code>。对于每个warp而言，都只是进入到一个分支内，所以并不会存在warp divergence的情况。</p>
<p>当进行<strong>第2次迭代</strong>时，0、1号两个warp进入计算分支。当进行<strong>第3次迭代</strong>时，只有0号warp进入计算分支。当进行<strong>第4次迭代</strong>时，只有0号warp的前16个线程进入分支。此时开始产生warp divergence。通过这种方式，我们消除了前3次迭代的warp divergence。</p>
<h2 id="优化技巧2：解决bank冲突"><a href="#优化技巧2：解决bank冲突" class="headerlink" title="优化技巧2：解决bank冲突"></a>优化技巧2：解决bank冲突</h2><h3 id="现有问题-1"><a href="#现有问题-1" class="headerlink" title="现有问题"></a>现有问题</h3><p>reduce1的最大问题是<strong>bank冲突</strong>。我们把目光聚焦在这个for循环中。并且只聚焦在<strong>0号warp</strong>。在<strong>第一次迭代</strong>中，0号线程需要去load shared memory的0号地址以及1号地址的数，然后写回到0号地址。而此时，这个warp中的16号线程，需要去load shared memory中的32号地址和33号地址。可以发现，0号地址跟32号地址产生了<strong>2路的bank冲突</strong>。</p>
<p>在<strong>第2次迭代</strong>中，0号线程需要去load shared memory中的0号地址和2号地址。这个warp中的8号线程需要load shared memory中的32号地址以及34号地址，16号线程需要load shared memory中的64号地址和68号地址，24号线程需要load shared memory中的96号地址和100号地址。</p>
<p>又因为0、32、64、96号地址对应着同一个bank，所以此时产生了<strong>4路的bank冲突</strong>。现在，可以继续算下去，8路bank冲突，16路bank冲突。由于bank冲突，所以reduce1性能受限。下图说明了在load第一个数据时所产生的bank冲突。</p>
<p><img src="/img/image-20220910180155032.png" alt="image-20220910180155032"></p>
<h3 id="解决方式-1"><a href="#解决方式-1" class="headerlink" title="解决方式"></a>解决方式</h3><p>在reduce中，解决bank冲突的方式就是把for循环逆着来。原来stride从0到256，现在stride从128到0。其伪代码如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">reduce2</span><span class="params">(<span class="keyword">float</span> *d_in,<span class="keyword">float</span> *d_out)</span></span>&#123;</span><br><span class="line">    __shared__ <span class="keyword">float</span> sdata[THREAD_PER_BLOCK];</span><br><span class="line"></span><br><span class="line">    <span class="comment">//each thread loads one element from global memory to shared mem</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> i=blockIdx.x*blockDim.x+threadIdx.x;</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> tid=threadIdx.x;</span><br><span class="line">    sdata[tid]=d_in[i];</span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// do reduction in shared mem</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">unsigned</span> <span class="keyword">int</span> s=blockDim.x/<span class="number">2</span>; s&gt;<span class="number">0</span>; s&gt;&gt;=<span class="number">1</span>)&#123;</span><br><span class="line">        <span class="keyword">if</span>(tid &lt; s)&#123;</span><br><span class="line">            sdata[tid]+=sdata[tid+s];</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// write result for this block to global mem</span></span><br><span class="line">    <span class="keyword">if</span>(tid==<span class="number">0</span>)d_out[blockIdx.x]=sdata[tid];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>那为什么通过这么一个小小的改变就能消除bank冲突呢，我们继续进行分析。</p>
<p>把目光继续看到这个for循环中，并且只分析0号warp。0号线程需要load shared memory的0号元素以及128号元素。1号线程需要load shared memory中的1号元素和129号元素。这一轮迭代中，在读取第一个数时，warp中的32个线程刚好load 一行shared memory数据。再分析第2轮迭代，0号线程load 0号元素和64号元素，1号线程load 1号元素和65号元素。</p>
<p>咦，也是这样，每次load shared memory的一行。再来分析第3轮迭代，0号线程load 0号元素和32号元素，接下来不写了，总之，一个warp load shared memory的一行。没有bank冲突。到了4轮迭代，0号线程load 0号元素和16号元素。那16号线程呢，16号线程啥也不干，因为s=16，16-31号线程啥也不干，跳过去了。示意图如下：</p>
<p><img src="/img/image-20220910213658483.png" alt="image-20220910213658483"></p>
<h2 id="优化技巧3：解决idle线程"><a href="#优化技巧3：解决idle线程" class="headerlink" title="优化技巧3：解决idle线程"></a>优化技巧3：解决idle线程</h2><h3 id="现有问题-2"><a href="#现有问题-2" class="headerlink" title="现有问题"></a>现有问题</h3><p><strong>reduce2</strong>最大的问题就是线程的浪费。可以看到我们启动了256个线程，但是在第1轮迭代时只有128个线程在干活，第2轮迭代只有64个线程在干活，每次干活的线程都会减少一半。第一轮迭代示意图如下，只有前128个线程在load数据。后128个线程啥也不干，光看着。</p>
<p><img src="/img/image-20220910213713143.png" alt="image-20220910213713143"></p>
<h3 id="解决方式-2"><a href="#解决方式-2" class="headerlink" title="解决方式"></a>解决方式</h3><p>对于HPC从业者而言，我们希望变成GPU的资本家，去尽可能地压榨GPU。但是呢，在这里，每一次迭代有一半的线程不干活。而且，128-255号线程最过分，它娘的，没有任何贡献，啥也不干。想来想去，能不能让它们干点活呢。想来想去，那这样吧，让它好歹做一次加法。除了去global memory中取数外，再做一次加法。当然为了实现这个，block数就得改一改了。Block数量减少，Num_per_block增加一倍。也就是说原来一个block只需要管256个数就行，现在得管512个数了。代码如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">reduce3</span><span class="params">(<span class="keyword">float</span> *d_in,<span class="keyword">float</span> *d_out)</span></span>&#123;</span><br><span class="line">    __shared__ <span class="keyword">float</span> sdata[THREAD_PER_BLOCK];</span><br><span class="line"></span><br><span class="line">    <span class="comment">//each thread loads one element from global memory to shared mem</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> i=blockIdx.x*(blockDim.x*<span class="number">2</span>)+threadIdx.x;</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> tid=threadIdx.x;</span><br><span class="line">    sdata[tid]=d_in[i] + d_in[i+blockDim.x];</span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// do reduction in shared mem</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">unsigned</span> <span class="keyword">int</span> s=blockDim.x/<span class="number">2</span>; s&gt;<span class="number">0</span>; s&gt;&gt;=<span class="number">1</span>)&#123;</span><br><span class="line">        <span class="keyword">if</span>(tid &lt; s)&#123;</span><br><span class="line">            sdata[tid]+=sdata[tid+s];</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// write result for this block to global mem</span></span><br><span class="line">    <span class="keyword">if</span>(tid==<span class="number">0</span>)d_out[blockIdx.x]=sdata[tid];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>通过这种方式，将一些idle的线程给利用起来了。</p>
<h2 id="优化技巧4：展开最后一维减少同步"><a href="#优化技巧4：展开最后一维减少同步" class="headerlink" title="优化技巧4：展开最后一维减少同步"></a>优化技巧4：展开最后一维减少同步</h2><h3 id="现有问题-3"><a href="#现有问题-3" class="headerlink" title="现有问题"></a>现有问题</h3><p>对于reduce3来说，性能已经算是比较好了。但是依旧没有达到我们想要的效果。我们再来仔细地看看还有什么可以改进的地方。我们发现，当进行到最后几轮迭代时，此时的block中只有warp0在干活时，线程还在进行<strong>同步</strong>操作。这一条语句造成了极大的浪费。</p>
<h3 id="解决方式-3"><a href="#解决方式-3" class="headerlink" title="解决方式"></a>解决方式</h3><p>由于一个warp中的32个线程其实是在一个SIMD单元上，这32个线程每次都是执行同一条指令，这天然地保持了同步状态，因而当s=32时，即只有一个SIMD单元在工作时，完全可以将__syncthreads()这条同步代码去掉。所以我们将最后一维进行展开以减少同步。伪代码如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">__<span class="function">device__ <span class="keyword">void</span> <span class="title">warpReduce</span><span class="params">(<span class="keyword">volatile</span> <span class="keyword">float</span>* cache,<span class="keyword">int</span> tid)</span></span>&#123;</span><br><span class="line">    cache[tid]+=cache[tid+<span class="number">32</span>];</span><br><span class="line">    cache[tid]+=cache[tid+<span class="number">16</span>];</span><br><span class="line">    cache[tid]+=cache[tid+<span class="number">8</span>];</span><br><span class="line">    cache[tid]+=cache[tid+<span class="number">4</span>];</span><br><span class="line">    cache[tid]+=cache[tid+<span class="number">2</span>];</span><br><span class="line">    cache[tid]+=cache[tid+<span class="number">1</span>];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">reduce4</span><span class="params">(<span class="keyword">float</span> *d_in,<span class="keyword">float</span> *d_out)</span></span>&#123;</span><br><span class="line">    __shared__ <span class="keyword">float</span> sdata[THREAD_PER_BLOCK];</span><br><span class="line"></span><br><span class="line">    <span class="comment">//each thread loads one element from global memory to shared mem</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> i=blockIdx.x*(blockDim.x*<span class="number">2</span>)+threadIdx.x;</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> tid=threadIdx.x;</span><br><span class="line">    sdata[tid]=d_in[i] + d_in[i+blockDim.x];</span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// do reduction in shared mem</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">unsigned</span> <span class="keyword">int</span> s=blockDim.x/<span class="number">2</span>; s&gt;<span class="number">32</span>; s&gt;&gt;=<span class="number">1</span>)&#123;</span><br><span class="line">        <span class="keyword">if</span>(tid &lt; s)&#123;</span><br><span class="line">            sdata[tid]+=sdata[tid+s];</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// write result for this block to global mem</span></span><br><span class="line">    <span class="keyword">if</span>(tid&lt;<span class="number">32</span>)warpReduce(sdata,tid);</span><br><span class="line">    <span class="keyword">if</span>(tid==<span class="number">0</span>)d_out[blockIdx.x]=sdata[tid];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以通过下面的示意图更好地了解，warp0会被绑定在一个SIMD单元上，上面有thread0-thread31。warp1会被绑在另外一个SIMD单元上，上面有thread32-thread63。由于在一个SIMD单元上，然后不管啥时候thread0和thread7肯定是同一状态，不需要同步。而thread0和thread34就不能保证同步，必须用<code>__syncthreads()</code>来保证同步操作。</p>
<h2 id="优化技巧5：完全展开减少计算"><a href="#优化技巧5：完全展开减少计算" class="headerlink" title="优化技巧5：完全展开减少计算"></a>优化技巧5：完全展开减少计算</h2><h3 id="现有问题-4"><a href="#现有问题-4" class="headerlink" title="现有问题"></a>现有问题</h3><p>其实到了这一步，reduce的效率已经足够高了。再进一步优化其实已经非常困难了。为了探索极致的性能表现，Mharris接下来给出的办法是<strong>对for循环进行完全展开</strong>。我觉得这里主要是减少for循环的开销。Mharris的实验表明这种方式有着1.41x的加速比。但是用的机器是G80，十几年前的卡。性能数据也比较老了，至于能不能真的有这么好的加速比，我们拭目以待。</p>
<h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><p>我们将整个for循环进行展开，非常暴力，代码如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">unsigned</span> <span class="keyword">int</span> blockSize&gt;</span><br><span class="line">__<span class="function">device__ <span class="keyword">void</span> <span class="title">warpReduce</span><span class="params">(<span class="keyword">volatile</span> <span class="keyword">float</span>* cache,<span class="keyword">int</span> tid)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(blockSize &gt;= <span class="number">64</span>)cache[tid]+=cache[tid+<span class="number">32</span>];</span><br><span class="line">    <span class="keyword">if</span>(blockSize &gt;= <span class="number">32</span>)cache[tid]+=cache[tid+<span class="number">16</span>];</span><br><span class="line">    <span class="keyword">if</span>(blockSize &gt;= <span class="number">16</span>)cache[tid]+=cache[tid+<span class="number">8</span>];</span><br><span class="line">    <span class="keyword">if</span>(blockSize &gt;= <span class="number">8</span>)cache[tid]+=cache[tid+<span class="number">4</span>];</span><br><span class="line">    <span class="keyword">if</span>(blockSize &gt;= <span class="number">4</span>)cache[tid]+=cache[tid+<span class="number">2</span>];</span><br><span class="line">    <span class="keyword">if</span>(blockSize &gt;= <span class="number">2</span>)cache[tid]+=cache[tid+<span class="number">1</span>];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">unsigned</span> <span class="keyword">int</span> blockSize&gt;</span><br><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">reduce5</span><span class="params">(<span class="keyword">float</span> *d_in,<span class="keyword">float</span> *d_out)</span></span>&#123;</span><br><span class="line">    __shared__ <span class="keyword">float</span> sdata[THREAD_PER_BLOCK];</span><br><span class="line"></span><br><span class="line">    <span class="comment">//each thread loads one element from global memory to shared mem</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> i=blockIdx.x*(blockDim.x*<span class="number">2</span>)+threadIdx.x;</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> tid=threadIdx.x;</span><br><span class="line">    sdata[tid]=d_in[i] + d_in[i+blockDim.x];</span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// do reduction in shared mem</span></span><br><span class="line">    <span class="keyword">if</span>(blockSize&gt;=<span class="number">512</span>)&#123;</span><br><span class="line">        <span class="keyword">if</span>(tid&lt;<span class="number">256</span>)&#123;</span><br><span class="line">            sdata[tid]+=sdata[tid+<span class="number">256</span>];</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(blockSize&gt;=<span class="number">256</span>)&#123;</span><br><span class="line">        <span class="keyword">if</span>(tid&lt;<span class="number">128</span>)&#123;</span><br><span class="line">            sdata[tid]+=sdata[tid+<span class="number">128</span>];</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(blockSize&gt;=<span class="number">128</span>)&#123;</span><br><span class="line">        <span class="keyword">if</span>(tid&lt;<span class="number">64</span>)&#123;</span><br><span class="line">            sdata[tid]+=sdata[tid+<span class="number">64</span>];</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// write result for this block to global mem</span></span><br><span class="line">    <span class="keyword">if</span>(tid&lt;<span class="number">32</span>)warpReduce&lt;blockSize&gt;(sdata,tid);</span><br><span class="line">    <span class="keyword">if</span>(tid==<span class="number">0</span>)d_out[blockIdx.x]=sdata[tid];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="优化技巧6：合理设置block数量"><a href="#优化技巧6：合理设置block数量" class="headerlink" title="优化技巧6：合理设置block数量"></a>优化技巧6：合理设置block数量</h2><h3 id="现有问题-5"><a href="#现有问题-5" class="headerlink" title="现有问题"></a>现有问题</h3><p>当走到这一步的时候，能调的东西已经基本上调完了。我们再把眼光放在block和thread的设置上。之前默认了Num_per_block=Thread_per_block。也就是说，一个block开启256个线程时，这个block负责256个元素的reduce操作。那可不可以让一个block多管点数。这样的话，开启的block数量少一些。以此<strong>对block设置进行调整</strong>，获得最优block取值，这样或许能够带来一些性能收益？</p>
<h3 id="解决方式-4"><a href="#解决方式-4" class="headerlink" title="解决方式"></a>解决方式</h3><p>这样需要再思考一下block的取值。对于GPU而言，block的取值到底是多更好，还是少更好。如此对CUDA编程熟悉的同学，肯定会毫不犹豫地说：“那肯定是多更好啦。Block数量多，block可以进行快速地切换，去掩盖访存的延时。”这个问题按下不表，我们看看Mharris是怎么说的。</p>
<p>如果一个线程被分配更多的work时，可能会更好地覆盖延时。这一点比较好理解。如果线程有更多的work时，对于编译器而言，就可能有更多的机会对相关指令进行重排，从而去覆盖访存时的巨大延时。虽然这句话并没有很好地说明在某种程度上而言，block少一些会更好。但是，有一点不可否认,<strong>block需要进行合理地设置</strong>。唠唠叨叨说了很多，现在把代码贴一下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">unsigned</span> <span class="keyword">int</span> blockSize&gt;</span><br><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">reduce6</span><span class="params">(<span class="keyword">float</span> *d_in,<span class="keyword">float</span> *d_out)</span></span>&#123;</span><br><span class="line">    __shared__ <span class="keyword">float</span> sdata[THREAD_PER_BLOCK];</span><br><span class="line"></span><br><span class="line">    <span class="comment">//each thread loads one element from global memory to shared mem</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> i=blockIdx.x*(blockDim.x*<span class="number">2</span>)+threadIdx.x;</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> tid=threadIdx.x;</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> gridSize = blockSize * <span class="number">2</span> * gridDim.x;</span><br><span class="line">    sdata[tid] = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span>(i&lt;n)&#123;</span><br><span class="line">        sdata[tid] +=d_in[i]+d_in[i+blockSize];</span><br><span class="line">        i+=gridSize;</span><br><span class="line">    &#125;</span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// do reduction in shared mem</span></span><br><span class="line">    <span class="keyword">if</span>(blockSize&gt;=<span class="number">512</span>)&#123;</span><br><span class="line">        <span class="keyword">if</span>(tid&lt;<span class="number">256</span>)&#123;</span><br><span class="line">            sdata[tid]+=sdata[tid+<span class="number">256</span>];</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(blockSize&gt;=<span class="number">256</span>)&#123;</span><br><span class="line">        <span class="keyword">if</span>(tid&lt;<span class="number">128</span>)&#123;</span><br><span class="line">            sdata[tid]+=sdata[tid+<span class="number">128</span>];</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(blockSize&gt;=<span class="number">128</span>)&#123;</span><br><span class="line">        <span class="keyword">if</span>(tid&lt;<span class="number">64</span>)&#123;</span><br><span class="line">            sdata[tid]+=sdata[tid+<span class="number">64</span>];</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// write result for this block to global mem</span></span><br><span class="line">    <span class="keyword">if</span>(tid&lt;<span class="number">32</span>)warpReduce&lt;blockSize&gt;(sdata,tid);</span><br><span class="line">    <span class="keyword">if</span>(tid==<span class="number">0</span>)d_out[blockIdx.x]=sdata[tid];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="优化技巧7：使用shuffle指令"><a href="#优化技巧7：使用shuffle指令" class="headerlink" title="优化技巧7：使用shuffle指令"></a>优化技巧7：使用shuffle指令</h2><h3 id="现有问题-6"><a href="#现有问题-6" class="headerlink" title="现有问题"></a>现有问题</h3><p>其实，对于Mharris的讲义。reduce优化就到此结束了。但是NV后来出了Shuffle指令，对于reduce优化有着非常好的效果。目前绝大多数访存类算子，像是softmax，batch_norm，reduce等，都是用Shuffle实现。所以，在这里谈一下这么把shuffle指令用在reduce优化上。</p>
<p>Shuffle指令是一组针对warp的指令。Shuffle指令最重要的特性就是<strong>warp内的寄存器可以相互访问</strong>。在没有shuffle指令的时候，各个线程在进行通信时只能通过shared memory来访问彼此的寄存器。而采用了shuffle指令之后，warp内的线程可以直接对其他线程的寄存器进行访存。通过这种方式可以减少访存的延时。除此之外，带来的最大好处就是可编程性提高了，在某些场景下，就不用shared memory了。毕竟，开发者要自己去控制 shared memory还是挺麻烦的一个事。</p>
<p>伪代码如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">unsigned</span> <span class="keyword">int</span> blockSize&gt;</span><br><span class="line">__device__ __<span class="function">forceinline__ <span class="keyword">float</span> <span class="title">warpReduceSum</span><span class="params">(<span class="keyword">float</span> sum)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(blockSize &gt;= <span class="number">32</span>)sum += __shfl_down_sync(<span class="number">0xffffffff</span>,sum,<span class="number">16</span>);</span><br><span class="line">    <span class="keyword">if</span>(blockSize &gt;= <span class="number">16</span>)sum += __shfl_down_sync(<span class="number">0xffffffff</span>,sum,<span class="number">8</span>);</span><br><span class="line">    <span class="keyword">if</span>(blockSize &gt;= <span class="number">8</span>)sum += __shfl_down_sync(<span class="number">0xffffffff</span>,sum,<span class="number">4</span>);</span><br><span class="line">    <span class="keyword">if</span>(blockSize &gt;= <span class="number">4</span>)sum += __shfl_down_sync(<span class="number">0xffffffff</span>,sum,<span class="number">2</span>);</span><br><span class="line">    <span class="keyword">if</span>(blockSize &gt;= <span class="number">2</span>)sum += __shfl_down_sync(<span class="number">0xffffffff</span>,sum,<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">return</span> sum;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">unsigned</span> <span class="keyword">int</span> blockSize&gt;</span><br><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">reduce7</span><span class="params">(<span class="keyword">float</span> *d_in,<span class="keyword">float</span> *d_out, <span class="keyword">unsigned</span> <span class="keyword">int</span> n)</span></span>&#123;</span><br><span class="line">    <span class="keyword">float</span> sum = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//each thread loads one element from global memory to shared mem</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> i=blockIdx.x*(blockDim.x*<span class="number">2</span>)+threadIdx.x;</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> tid=threadIdx.x;</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> gridSize = blockSize * <span class="number">2</span> * gridDim.x;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span>(i&lt;n)&#123;</span><br><span class="line">        sdata[tid] +=d_in[i]+d_in[i+blockSize];</span><br><span class="line">        i+=gridSize;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// shared mem for partial sums(one per warp in the block</span></span><br><span class="line">    <span class="keyword">static</span> __shared__ <span class="keyword">float</span> warpLevelSums[WARP_SIZE];</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> laneId = threadIdx.x % WARP_SIZE;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> warpId = threadIdx.x / WARP_SIZE;</span><br><span class="line"></span><br><span class="line">    sum = warpReduceSum&lt;blockSize&gt;(sum);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(laneId == <span class="number">0</span>)warpLevelSums[warpId]=sum;</span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    sum = (threadIdx.x &lt; blockDim.x / WARP_SIZE)? warpLevelSums[laneId]:<span class="number">0</span>;</span><br><span class="line">    <span class="comment">// Final reduce using first warp</span></span><br><span class="line">    <span class="keyword">if</span>(warpId == <span class="number">0</span>)sum = warpReduceSum&lt;blockSize/WARP_SIZE&gt;(sum);</span><br><span class="line">    <span class="comment">// write result for this block to global mem</span></span><br><span class="line">    <span class="keyword">if</span>(tid==<span class="number">0</span>)d_out[blockIdx.x]=sum;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="GEMM优化"><a href="#GEMM优化" class="headerlink" title="GEMM优化"></a>GEMM优化</h1><h2 id="前言-1"><a href="#前言-1" class="headerlink" title="前言"></a>前言</h2><p>在高性能领域，对于<strong>矩阵乘（GEMM）的优化</strong>是一个非常重要的课题。GEMM可以非常广泛地应用于航空航天、流体力学等科学计算领域，这也是之前HPC的主要应用场景。后来深度学习开展地如火如荼，由于对高算力的需要，也成为HPC的主要应用场景之一。这些年涌现了一系列的深度学习模型。模型里面最耗时的东西，包括卷积、全连接层、attention，都可以转换成GEMM操作。所以说，GEMM优化的重要性，怎么突出都不过分。</p>
<p>本篇文章主要介绍GEMM中的数据分块和如何在多级存储进行数据搬运。这也是<strong>HPC优化的核心思想，怎么样让数据放在更近的存储上来掩盖计算的延时，从而减少存储墙的影响</strong>。文章分为四个方面进行叙述，首先介绍在global memory层面如何进行分块以及数据搬运，随后介绍在shared memory层面如何进行分块以及数据搬运，而后介绍在register层面如何进行分块以及避免bank冲突，最后介绍如何进行prefetch以更好地掩盖访存时延。</p>
<h2 id="从global-memory到shared-memory"><a href="#从global-memory到shared-memory" class="headerlink" title="从global memory到shared memory"></a>从global memory到shared memory</h2><p>假设有矩阵A,B，需要计算矩阵A和B的乘，即矩阵C。A、B、C三个矩阵的维度分别为，，m∗k，k∗n，m∗n ，且三个矩阵中的数据都是单精度浮点数。对于C中每一个元素，C[i][j]，可以看作是A的一行和B的一列进行一次归约操作。采用最naive的GEMM算法，在GPU中，一共开启m∗n 个线程，每个线程需要读取矩阵A的一行与矩阵B的一列，而后将计算结果写回至矩阵C中。因而，完成计算一共需要从global memory中进行2mnk 次读操作和m*n次写操作。大量的访存操作使得GEMM效率难以提高，因而考虑global memory中进行分块，并将矩阵块放置到shared memory中。其示意图如下：</p>
<p><img src="/img/image-20220910214848982.png" alt="image-20220910214848982"></p>
<p>对global memory进行分块的GEMM算法示意图见上图右侧。首先将A、B、C三个矩阵划分为多个维度为，，bm∗bk，bk∗bn，bm∗bn 的小矩阵块。三个矩阵形成M∗K，K∗N，M∗N 的小矩阵网格。其中M=m/bm，N=n/bn，K=k/bk 。随后在GPU中开启M∗N 个block，每个block负责C中一个维度为bm∗bn 的小矩阵块的计算。计算中一共有K次迭代，每一次迭代都需要读取A中一个维度为bm∗bk 的小矩阵块和B中一个维度为bk∗bn 的小矩阵块，并将其放置在shared memory中。因而，完成C中所有元素的计算一共需要从global memory中读取M∗N∗K∗（bm∗bk+bk∗bn） ，即m∗n∗k（1/bm+1/bn） 个单精度浮点数。相比于naive的GEMM算法，访存量减少为原来的1/2∗(1/bm+1/bn) 。通过global memory中分块算法极大地减少了对global memory的访存量。并且，相比于naive算法，对global进行分块可以更充分地利用数据局部性。在naive算法中，每一个线程都需要直接从global memory中取数，其时延非常长，计算性能非常差。而进行分块后，将维度为bm∗bk，bk∗bn 的小矩阵块先存储到shared memory之中。而后计算单元进行计算时可以直接从shared memory中取数，大大减少了访存所需要的时延。</p>
<h2 id="从shared-memory到register"><a href="#从shared-memory到register" class="headerlink" title="从shared memory到register"></a>从shared memory到register</h2><p>随后，我们进一步考虑从shared memory到register的过程。在这里，只分析<strong>一个block</strong>中的计算。当进行K轮迭代中某一轮迭代时，GPU将维度为bm∗bk，bk∗bn 的小矩阵块存储到shared memory中，而后各个线程将shared memory中的数据存入register中进行计算。</p>
<p><img src="/img/image-20220910215005453.png" alt="image-20220910215005453"></p>
<p>在<strong>不对shared memory分块</strong>时，一个block中含有bm∗bn 个线程，<strong>每一个线程负责C中一个元素的计算</strong>。则一个block一共需要对shared memory进行2∗bm∗bn∗bk 次读操作。而后<strong>考虑对shared memory进行分块</strong>，对bm∗bn 的小矩阵进行再一次划分，将其划分为多个维度为rm∗rn 的子矩阵。则一个block需要负责X∗Y 个子矩阵。其中，X=bmrm ，Y=bnrn 。随后，在一个block中开启X∗Y 个线程，<strong>每个线程负责一个维度为rm∗rn 的子矩阵的计算</strong>。在计算中，一个block一共需要从shared memory读取X∗Y∗(rm+rn)∗bk ，即bm∗bn∗bk∗(1/rm+1/rn) 个单精度浮点数。相比于未分块的算法，对于shared memory中的访存量减少为原来的1/2∗(1/rm+1/rn) 。并且，由于将数据放入register中，可以直接对数据进行运算，减少了从shared memory中取数的时延。</p>
<h2 id="register分块"><a href="#register分块" class="headerlink" title="register分块"></a>register分块</h2><p>在这里，我们考虑最后一层，即register中的计算，并且只分析一个thread。在完成以上的过程后，对于一个线程而言，它现在拥有：rm 个A矩阵的寄存器值，rn 个B矩阵的寄存器值，以及rm∗rn 个C矩阵的寄存器值。通过这些寄存器的值，需要计算rm∗rn 个数。这需要rm∗rn 条FFMA指令。</p>
<p>这个时候会涉及到寄存器的bank conflict。在NV的GPU中，每个SM不仅会产生shared memroy之间的bank 冲突，也会产生寄存器之间的bank冲突。这一点对于计算密集型的算子十分重要。像shared memory一样，寄存器的Register File也会被分为几个bank，如果一条指令的的源寄存器有2个以上来自同一bank，就会产生冲突。指令会重发射，浪费一个cycle。</p>
<p>我们假设对这个thread来说，rm=4,rn=4 。并且计算C的寄存器以一种非常naive的情况分配，如下图左侧所示。则需要产生16条FFMA指令，列举如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">FFMA R0, R16, R20, R0</span><br><span class="line">FFMA R1, R16, R21, R1</span><br><span class="line">……</span><br></pre></td></tr></table></figure>
<p><img src="/img/image-20220910220109982.png" alt="image-20220910220109982"></p>
<p>可以从中看出，这会产生大量的register bank冲突，所以需要对参与计算的寄存器重新进行分配和排布,如上图右侧所示。在有些地方，这种方式也可以叫做register分块。</p>
<h2 id="数据的prefetch"><a href="#数据的prefetch" class="headerlink" title="数据的prefetch"></a>数据的prefetch</h2><p>最后，我们来讲讲如何通过对数据进行prefetch来减少访存的latency。我们再来回顾GEMM的过程，并且仔细地看看这个访存的latency到底是怎么导致的。<strong>对于一个block而言</strong>，需要计算一个bm∗bn 的矩阵块，这个时候需要进行K次迭代，每次迭代都需要先将来自A和B的两个小块送到shared memory中再进行计算。而从global中访存实际上是非常慢的，所以导致了latency。虽然GPU中可以通过block的切换来掩盖这种latency，但是由于分配的shared memory比较多，活跃的block并不太多，这种延时很难被掩盖。<strong>对于一个thread</strong>，需要计算一个rm∗rn 的小矩阵，但是必须先将数据从shared memory传到寄存器上，才能开始进行计算。所以导致了每进行一次迭代，计算单元就需要停下来等待，计算单元不能被喂饱。</p>
<p>为此，需要进行数据的Prefetch来尽可能地掩盖这种latency。思想也比较简单，需要多开一个buffer，进行读写分离。示意图如下。当block进行第2轮迭代时，需要对A2和B2进行计算，在计算单元进行计算的同时，我们将A3和B3提前放置到shared memory。而后，在进行第3轮迭代时，就可以直接对shared memory中的A3和B3进行计算，而不需要等待从global memory搬运到shared memory的时间。寄存器上的Prefetch也是同理。</p>
<p><img src="/img/image-20220910220136539.png" alt="image-20220910220136539"></p>
<h1 id="GEMM算法概述"><a href="#GEMM算法概述" class="headerlink" title="GEMM算法概述"></a>GEMM算法概述</h1><p>这个章节里主要来说一下GEMM的一个计算流程，其实这一点已经在GEMM优化（一）中提及。但上一篇文章主要说得是原理，关于具体计算逻辑，还是不太直观，所以我们在这里再提一下。然后这个具体的计算逻辑分为两个阶段介绍，分别是不采用数据预取和采用数据预取，这主要是考虑到直接说数据预取，有读者可能会看得云里雾里，比较难受，所以先把不采用数据预取这个内容说明白，然后再来讲这个数据预取。</p>
<h2 id="不采用数据预取"><a href="#不采用数据预取" class="headerlink" title="不采用数据预取"></a>不采用数据预取</h2><p>首先，我们先明确一下GEMM中的具体参数。取bm=128,bn=128,bk=8,rm=8,rn=8。当这几个参数选定之后先来直观地感受一下这几个参数意义，假定给了三个矩阵，A，B，C，其维度都是2048×2048。要求解C=A×B。那么我们需要开启（2048/128）×（2048/128）=<strong>256个block</strong>，每个block里面有（128/8）×（128/8）=<strong>256个线程</strong>，每个线程需要负责计算C矩阵中8×8=64个元素的结果，每个block负责256×64=16384个元素的结果。</p>
<p>明确了上面的参数之后，我们来仔细地观察其中一个block的计算逻辑。对于这个block而言，它需要进行2048/8=256次迭代，我们先把这个迭代称为<strong>大迭代</strong>，每一次大迭代都需要把A里面128×8=1024个元素和B里面8×128=1024个元素先放到shared memory中。然后这个block中的256个线程把结果计算出来。计算完之后，再进入下一次大迭代。不断重复该过程，直至这个block负责的16384个元素的结果被求解出。大迭代示意图如下：</p>
<p><img src="/img/image-20220910220741253.png" alt="image-20220910220741253"></p>
<p>随后再具体看看每一个大迭代中，block中的线程的计算逻辑。在进行一个大迭代时，shared memory中有128×8=1024个A矩阵元素和8×128=1024个B矩阵元素。随后，每个线程需要进行8次迭代，我们把这个迭代成为<strong>小迭代</strong>。bk=8，所以有8次小迭代。每一次小迭代中，每个线程需要从shared memory中拿到A矩阵的一小列和B矩阵的一小行，即8个A的元素和8个B的元素。线程将这8+8=16个元素放置在寄存器中。每个线程需要负责8×8=64个元素的计算，一共会产生64条FFMA指令。小迭代示意图如下：</p>
<p><img src="/img/image-20220910220755159.png" alt="image-20220910220755159"></p>
<p>以上就是不采用数据预取的GEMM算法计算逻辑。总的来说，<strong>对于一个block而言，有256个大迭代，每个大迭代中又有8个小迭代</strong>。这是后续内容的基础，如果还是不太清楚的话，可以再仔细看看，把这个过程完全搞清楚后，我们再继续接下来的内容，即采用数据预取后的GEMM算法计算逻辑。</p>
<h2 id="采用数据预取"><a href="#采用数据预取" class="headerlink" title="采用数据预取"></a>采用数据预取</h2><p>采用数据预取的GEMM计算流程稍有差异。这个差异主要是体现在两个方面，第一个是开启的shared memory和寄存器数量，第二个是需要提前将一些数据放置到shared memory和寄存器中。下面来仔细说说这个流程。</p>
<p>为了实现数据预取，<strong>需要开启两倍的shared memory和寄存器</strong>。当然也可以将原来shared memory切分成两块，也就是将bm×bk和bk×bn的矩阵一分为二。以A中的小矩阵而言，变成了两个bm×bk/2。然后大迭代次数由原来的256变成了512。很多地方把这个技术叫做<strong>双缓冲</strong>，我感觉跟预取是同一个事情。无非是针对参数bk的大小换不同说法。所以在这里统一叫做数据预取。废话说得有点多。总之，我们还是开启两倍的shared memory和寄存器数据。在一个block中，原来在shared memory中需要存储的数据是bm×bk+bk×bn。现在变成了bm×bk×2+bk×bn×2。在一个thread中，为了存储A和B的数据，原来需要使用rm+rn个寄存器，现在需要使用2×(rm+rn)个寄存器。为了后续方便介绍，我们用<strong>read SM</strong>和<strong>write SM</strong>代表用来读写的两块共享内存，并用<strong>read REG</strong>和<strong>write REG</strong>来表示用来读写的两块寄存器。</p>
<p>把共享内存和寄存器的事情说明白之后，我们来看看具体的计算逻辑。在执行256次大迭代之前，我们需要提前将第0次大迭代的数据存到write SM中，并且将第0次小迭代的数据存到write REG中。在完成这一个预取过程之后，我们再来仔细地看看第0个大迭代。需要注意的是，<strong>上一轮大迭代的write SM就是这一轮迭代的read SM。上一轮小迭代的write REG就是这一轮迭代的read REG</strong>。所以在进行第0个大迭代时，上面write SM就变成了read SM。然后我们首先需要将下一轮大迭代的数据存到write SM中。由于从global memory中取数的时钟周期非常多。所以在等待数据取回的同时，对read SM中的数据进行计算。也就是我们在等待的同时，需要开启8次小迭代来进行计算。而小迭代中也存在着读写分离，在对read REG进行计算之前，需要先执行write REG的操作，通过这种方式来<strong>掩盖访存的latency</strong>。所以整体的计算逻辑如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">for k in 256 big_loop:</span><br><span class="line">	prefetch next loop data to write_SM</span><br><span class="line">	// compute in read_SM</span><br><span class="line">	for iter in 8 small_loop:</span><br><span class="line">		prefecth next loop data to write_REG</span><br><span class="line">		compute in read_REG</span><br></pre></td></tr></table></figure>
<p><img src="/img/image-20220910220841079.png" alt="image-20220910220841079"></p>
<h2 id="GEMM代码解析"><a href="#GEMM代码解析" class="headerlink" title="GEMM代码解析"></a>GEMM代码解析</h2><p>在上一节中已经将GEMM算法的流程再次回顾了一遍，接下来进入到代码解析环节。这里主要是解析采用了数据预取的GEMM。由于将数据从global memroy中搬运到shared memory中还经过了寄存器，所以对prefetch过程进行了细化，这个跟前面的伪代码稍有差异。</p>
<h3 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a>参数说明</h3><p>首先需要说明的是<strong>模板参数</strong>，这也是后续<strong>对GEMM性能进行调参的最主要参数</strong>，往往不同的参数选择对最终的GEMM性能影响极大。后面的实验会展示在不同的参数下的性能比较。前三个参数，BLOCK_SIZE_M、BLOCK_SIZE_K、BLOCK_SIZE_N分别代表上文中的<strong>bm、bk、bn</strong>。中间两个参数，THREAD_SIZE_Y、THREAD_SIZE_X代表上文中的<strong>rm、rn</strong>。最后的参数ENABLE_DOUBLE_BUFFER代表是否采用双缓冲，即是否采用数据预取，在这里，我们只讨论采用数据预取，即开启双缓冲的情况。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">template &lt;</span><br><span class="line">    const int BLOCK_SIZE_M,  // height of block of C that each  block calculate</span><br><span class="line">    const int BLOCK_SIZE_K,  // width of block of A that each  block load into shared memory</span><br><span class="line">    const int BLOCK_SIZE_N,  // width of block of C that each  block calculate</span><br><span class="line">    const int THREAD_SIZE_Y, // height of block of C that each thread calculate</span><br><span class="line">    const int THREAD_SIZE_X,  // width of block of C that each thread calculate</span><br><span class="line">    const bool ENABLE_DOUBLE_BUFFER // whether enable double buffering or not</span><br><span class="line">    &gt;</span><br></pre></td></tr></table></figure>
<p>接下来是<strong>线程类的参数</strong>。整个计算流程需要开启256个block，这256个block按照二维形态排布。而一个block中开启了256个线程，这256个线程按照二维形态进行排布。<strong>bx</strong>代表横向的block坐标，<strong>by</strong>代表竖向的block坐标。而<strong>tx</strong>代表横向的线程坐标，<strong>ty</strong>代表竖向的线程坐标。这是CUDA的基础内容，看不明白的同学可以找一些博客多理解一下，务必搞清楚。<strong>THREAD_X_PER_BLOCK</strong>代表在一个block中有多少个横向的线程，在这里等于16。<strong>THREAD_Y_PER_BLOCK</strong>代表在一个block中有多少个竖向的线程，在这里等于16。<strong>THREAD_NUM_PER_BLOCK</strong>代表在一个block中有多少个线程，在这里等于256。<strong>tid</strong>则代表当前线程在这256个线程中的id号。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">// Block index</span><br><span class="line">int bx = blockIdx.x;</span><br><span class="line">int by = blockIdx.y;</span><br><span class="line"></span><br><span class="line">// Thread index</span><br><span class="line">int tx = threadIdx.x;</span><br><span class="line">int ty = threadIdx.y;</span><br><span class="line"></span><br><span class="line">// the threads number in Block of X,Y</span><br><span class="line">const int THREAD_X_PER_BLOCK = BLOCK_SIZE_N / THREAD_SIZE_X;</span><br><span class="line">const int THREAD_Y_PER_BLOCK = BLOCK_SIZE_M / THREAD_SIZE_Y;</span><br><span class="line">const int THREAD_NUM_PER_BLOCK = THREAD_X_PER_BLOCK * THREAD_Y_PER_BLOCK;</span><br><span class="line"></span><br><span class="line">// thread id in cur Block</span><br><span class="line">const int tid = ty * THREAD_X_PER_BLOCK + tx;</span><br></pre></td></tr></table></figure>
<p>随后说明开启的<strong>shared memory和register</strong>数量。<strong>As</strong>代表为了存储A矩阵中的数据所需要开启的shared memory。在一轮迭代中需要使用bm×bk的数据，为了加快后续的访存，所以需要进行一次转置。并且为了预取，开了两倍的大小，一半用来读数据，一半用来写数据。所以一共需要2×BLOCK_SIZE_K×BLOCK_SIZE_M的空间。而<strong>Bs</strong>同理，但是载入数据时并不需要转置。<strong>accum</strong>用来临时存储C的计算结果。<strong>frag_a</strong>用来加载<strong>As</strong>中的<strong>rm</strong>个数据，为了预取也开启了双倍的空间。<strong>frag_b</strong>同理。<strong>ldg_num_a</strong>稍微有点费解，需要解释一下。为了将global memory的数据块搬运到shared memory中，需要先经过寄存器。也就是说，这个数据搬运过程其实是global memory-&gt;register-&gt;shared memory。所以为了临时存储A中的数据，需要开启一定量的寄存器。在一次大迭代中，我们总共需要搬运<strong>BLOCK_SIZE_M × BLOCK_SIZE_K</strong>个float数据，然后一个block中有<strong>THREAD_NUM_PER_BLOCK</strong>个线程，采用float4进行取数，即一个线程一次取4个数。则一共需要BLOCK_SIZE_M × BLOCK_SIZE_K/(THREAD_NUM_PER_BLOCK×4)次搬运就能把所有的数搬运到寄存器上。这个搬运次数用<strong>ldg_num_a</strong>表示。为了存储BLOCK_SIZE_M <em> BLOCK_SIZE_K的数据块，每个线程需要额外开启<em>*ldg_a_reg</em></em>个寄存器进行存储。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// shared memory</span></span><br><span class="line">__shared__ <span class="keyword">float</span> As[<span class="number">2</span>][BLOCK_SIZE_K][BLOCK_SIZE_M];</span><br><span class="line">__shared__ <span class="keyword">float</span> Bs[<span class="number">2</span>][BLOCK_SIZE_K][BLOCK_SIZE_N];</span><br><span class="line"><span class="comment">// registers for C</span></span><br><span class="line"><span class="keyword">float</span> accum[THREAD_SIZE_Y][THREAD_SIZE_X] = &#123;<span class="number">0</span>&#125;;</span><br><span class="line"><span class="comment">// registers for A and B</span></span><br><span class="line"><span class="keyword">float</span> frag_a[<span class="number">2</span>][THREAD_SIZE_Y];</span><br><span class="line"><span class="keyword">float</span> frag_b[<span class="number">2</span>][THREAD_SIZE_X];</span><br><span class="line"><span class="comment">// registers load global memory</span></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> ldg_num_a = BLOCK_SIZE_M * BLOCK_SIZE_K / (THREAD_NUM_PER_BLOCK * <span class="number">4</span>);</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> ldg_num_b = BLOCK_SIZE_K * BLOCK_SIZE_N / (THREAD_NUM_PER_BLOCK * <span class="number">4</span>);</span><br><span class="line"><span class="keyword">float</span> ldg_a_reg[<span class="number">4</span>*ldg_num_a];</span><br><span class="line"><span class="keyword">float</span> ldg_b_reg[<span class="number">4</span>*ldg_num_b];</span><br></pre></td></tr></table></figure>
<p>最后需要说明的参数是在<code>global-&gt;shared memory</code>阶段用到。我们开启了256个线程，在一次大迭代中需要将128×8个元素搬运到shared memory中。我们用下面的参数说明了这个搬运的逻辑。<strong>A_TILE_THREAD_PER_ROW</strong>代表把搬运一行数据需要使用多少个线程，为了搬运A的一行，需要使用2个线程。</p>
<p><strong>A_TILE_ROW_START</strong>代表在这个维度为bm×bk的数据块中，当前线程需要搬运的数据的竖向坐标，而<strong>A_TILE_COL</strong>代表需要搬运的数据的横向坐标。对3号线程而言，由于它要搬运（1，1）号数据块中的4个元素。所以，A_TILE_ROW_START是1，A_TILE_COL是4。<strong>A_TILE_ROW_STRIDE</strong>代表在进行多次搬运时需要跨越的行。假设As是一块256×8的数据块（这个设置跟前面不一样），256个线程进行搬运，一次搬运4个数，所以要搬运两次。对于3号线程而言，分别搬运下图中的绿色数据块。</p>
<p><img src="/img/image-20220910221016698.png" alt="image-20220910221016698"></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// threads number in one row</span></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> A_TILE_THREAD_PER_ROW = BLOCK_SIZE_K / <span class="number">4</span>;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> B_TILE_THREAD_PER_ROW = BLOCK_SIZE_N / <span class="number">4</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// row number and col number that needs to be loaded by this thread</span></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> A_TILE_ROW_START = tid / A_TILE_THREAD_PER_ROW;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> B_TILE_ROW_START = tid / B_TILE_THREAD_PER_ROW;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> A_TILE_COL = tid % A_TILE_THREAD_PER_ROW * <span class="number">4</span>; </span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> B_TILE_COL = tid % B_TILE_THREAD_PER_ROW * <span class="number">4</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// row stride that thread uses to load multiple rows of a tile</span></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> A_TILE_ROW_STRIDE = THREAD_NUM_PER_BLOCK / A_TILE_THREAD_PER_ROW;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> B_TILE_ROW_STRIDE = THREAD_NUM_PER_BLOCK / B_TILE_THREAD_PER_ROW;</span><br></pre></td></tr></table></figure>
<h3 id="大迭代前预取数据"><a href="#大迭代前预取数据" class="headerlink" title="大迭代前预取数据"></a>大迭代前预取数据</h3><p>在介绍完相关参数之后，我们来进入到具体的代码逻辑。为了代码简洁，用float4读取的过程用了两个宏，定义如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> OFFSET(row, col, ld) ((row) * (ld) + (col))</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> FETCH_FLOAT4(pointer) (reinterpret_cast<span class="meta-string">&lt;float4*&gt;(&amp;(pointer))[0])</span></span></span><br></pre></td></tr></table></figure>
<p>迭代前预取数据分为<strong>两个部分</strong>，<strong>第一个部分</strong>是将第一个大迭代的数据从global 预取到shared memroy中。<strong>第二个部分</strong>是将shared memory上的数据预取到寄存器中。先来看看<strong>第一个部分</strong>。这里面分别是将第一个大迭代中需要的A、B数据预取到shared memroy中。对于A矩阵而言，这个for循环代表着block中的线程需要搬运多少次才能将globa中的数据放到shared memory中。由于A需要先进行一次转置，所以先将数据先放置在寄存器中。数据按行取，然后按列存。对于B矩阵而言，数据不用转置，直接按行取，按行存。当然，这个过程中间也要经过寄存器，但是没有写出来的必要了。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// load A from global memory to shared memory</span></span><br><span class="line">    <span class="meta">#<span class="meta-keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> ( <span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; BLOCK_SIZE_M ; i += A_TILE_ROW_STRIDE) &#123;</span><br><span class="line">        <span class="keyword">int</span> ldg_index = i / A_TILE_ROW_STRIDE * <span class="number">4</span>;</span><br><span class="line">        FETCH_FLOAT4(ldg_a_reg[ldg_index]) = FETCH_FLOAT4(A[OFFSET(</span><br><span class="line">            BLOCK_SIZE_M * by + A_TILE_ROW_START + i, <span class="comment">// row</span></span><br><span class="line">            A_TILE_COL, <span class="comment">// col</span></span><br><span class="line">            K )]);</span><br><span class="line">        As[<span class="number">0</span>][A_TILE_COL][A_TILE_ROW_START + i]=ldg_a_reg[ldg_index];</span><br><span class="line">        As[<span class="number">0</span>][A_TILE_COL+<span class="number">1</span>][A_TILE_ROW_START + i]=ldg_a_reg[ldg_index+<span class="number">1</span>];</span><br><span class="line">        As[<span class="number">0</span>][A_TILE_COL+<span class="number">2</span>][A_TILE_ROW_START + i]=ldg_a_reg[ldg_index+<span class="number">2</span>];</span><br><span class="line">        As[<span class="number">0</span>][A_TILE_COL+<span class="number">3</span>][A_TILE_ROW_START + i]=ldg_a_reg[ldg_index+<span class="number">3</span>];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// load B from global memory to shared memory</span></span><br><span class="line">    <span class="meta">#<span class="meta-keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> ( <span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; BLOCK_SIZE_K; i += B_TILE_ROW_STRIDE) &#123;</span><br><span class="line">        FETCH_FLOAT4(Bs[<span class="number">0</span>][B_TILE_ROW_START + i][B_TILE_COL]) = FETCH_FLOAT4(B[OFFSET(</span><br><span class="line">                B_TILE_ROW_START + i, <span class="comment">// row</span></span><br><span class="line">                B_TILE_COL + BLOCK_SIZE_N * bx, <span class="comment">// col</span></span><br><span class="line">                N )]);</span><br><span class="line">    &#125;</span><br><span class="line">    __syncthreads();</span><br></pre></td></tr></table></figure>
<p>然后就是<strong>第二个部分</strong>。将shared memory中的数据存到寄存器中。一共需要取THREAD_SIZE_Y个数，每次取4个数。这个倒没有什么好说的。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// load A from shared memory to register</span></span><br><span class="line">    <span class="meta">#<span class="meta-keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> thread_y = <span class="number">0</span>; thread_y &lt; THREAD_SIZE_Y; thread_y += <span class="number">4</span>) &#123;</span><br><span class="line">        FETCH_FLOAT4(frag_a[<span class="number">0</span>][thread_y]) = FETCH_FLOAT4(As[<span class="number">0</span>][<span class="number">0</span>][THREAD_SIZE_Y * ty + thread_y]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// load B from shared memory to register</span></span><br><span class="line">    <span class="meta">#<span class="meta-keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> thread_x = <span class="number">0</span>; thread_x &lt; THREAD_SIZE_X; thread_x += <span class="number">4</span>) &#123;</span><br><span class="line">        FETCH_FLOAT4(frag_b[<span class="number">0</span>][thread_x]) = FETCH_FLOAT4(Bs[<span class="number">0</span>][<span class="number">0</span>][THREAD_SIZE_X * tx + thread_x]);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h3 id="大迭代逻辑"><a href="#大迭代逻辑" class="headerlink" title="大迭代逻辑"></a>大迭代逻辑</h3><p>在完成上一步后，我们要进入到<strong>大迭代</strong>中，按照前面的参数，我们需要<strong>进行256个大迭代</strong>。先忽略这个迭代里面的具体代码，看看这个框架，如下所示。首先要说的是<strong>write_stage_idx</strong>这个参数。之前定义了<strong>shared</strong> float As[2][BLOCK_SIZE_K][BLOCK_SIZE_M]。为了读写分离，给As开了两块空间。如果write_stage_idx=1，就对As[1]空间进行写操作，对As[0]空间进行读操作。因为我们之前将数据预取到了As[0]这个空间里，所以在第一个大迭代时，对As[0]进行读操作，对As[1]进行写操作，所以write_stage_idx=1。再来看看<strong>tile_idx</strong>这个参数，这个代表大迭代时，在A矩阵的列号。每一次大迭代要读取BLOCK_SIZE_K列，直到完成大迭代，即tile_idx=K为止。再看看循环里面的load_stage_idx，这个和write_stage_idx对应，两者保持二进制位相反即可。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> write_stage_idx = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">int</span> tile_idx = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">do</span>&#123;</span><br><span class="line">    tile_idx += BLOCK_SIZE_K;</span><br><span class="line">    <span class="keyword">int</span> load_stage_idx = write_stage_idx ^ <span class="number">1</span>;</span><br><span class="line">    <span class="comment">// compute</span></span><br><span class="line">    <span class="keyword">if</span>(tile_idx &lt; K)&#123;</span><br><span class="line">        write_stage_idx ^= <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;<span class="keyword">while</span>(tile_idx&lt; K);</span><br></pre></td></tr></table></figure>
<h3 id="大迭代详细解析"><a href="#大迭代详细解析" class="headerlink" title="大迭代详细解析"></a>大迭代详细解析</h3><p>我们在这里开始说明具体的大迭代。下面代码描述的是，如果还有下一个迭代，则将下一个迭代的数据块，搬运到寄存器上，这里面的for循环代表可能需要多次搬运。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">tile_idx += BLOCK_SIZE_K;</span><br><span class="line"><span class="comment">// load next tile from global mem</span></span><br><span class="line"><span class="keyword">if</span>(tile_idx&lt; K)&#123;</span><br><span class="line">    <span class="meta">#<span class="meta-keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> ( <span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; BLOCK_SIZE_M ; i += A_TILE_ROW_STRIDE) &#123;</span><br><span class="line">        <span class="keyword">int</span> ldg_index = i / A_TILE_ROW_STRIDE * <span class="number">4</span>;</span><br><span class="line">        FETCH_FLOAT4(ldg_a_reg[ldg_index]) = FETCH_FLOAT4(A[OFFSET(</span><br><span class="line">            BLOCK_SIZE_M * by + A_TILE_ROW_START + i, <span class="comment">// row</span></span><br><span class="line">            A_TILE_COL + tile_idx, <span class="comment">// col</span></span><br><span class="line">            K )]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">#<span class="meta-keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> ( <span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; BLOCK_SIZE_K; i += B_TILE_ROW_STRIDE) &#123;</span><br><span class="line">        <span class="keyword">int</span> ldg_index = i / A_TILE_ROW_STRIDE * <span class="number">4</span>;</span><br><span class="line">        FETCH_FLOAT4(ldg_b_reg[ldg_index]) = FETCH_FLOAT4(B[OFFSET(</span><br><span class="line">            tile_idx + B_TILE_ROW_START + i, <span class="comment">// row</span></span><br><span class="line">            B_TILE_COL + BLOCK_SIZE_N * bx, <span class="comment">// col</span></span><br><span class="line">            N )]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>随后进入到小迭代的计算逻辑之中，<strong>load_stage_idx</strong>参数代表需要从As的哪个空间进行读数。然后是<strong>BLOCK_SIZE_K-1</strong>次小迭代。按照前面的参数配置，即需要在这里完成<strong>7次小迭代</strong>。由于在小迭代中也采用了双缓冲的方式，需要将下一轮小迭代的数据提前写入到寄存器中，这个过程需要对shared memory访存，会稍微慢点。与此同时，线程需要计算更新<strong>THREAD_SIZE_X x THREAD_SIZE_Y=8×8=64</strong>个C矩阵元素的结果。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> load_stage_idx = write_stage_idx ^ <span class="number">1</span>;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> unroll</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>; j&lt;BLOCK_SIZE_K<span class="number">-1</span>; ++j)&#123;</span><br><span class="line">    <span class="comment">// load next tile from shared mem to register </span></span><br><span class="line">    <span class="comment">// load A from shared memory to register</span></span><br><span class="line">    <span class="meta">#<span class="meta-keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> thread_y = <span class="number">0</span>; thread_y &lt; THREAD_SIZE_Y; thread_y += <span class="number">4</span>) &#123;</span><br><span class="line">        FETCH_FLOAT4(frag_a[(j+<span class="number">1</span>)%<span class="number">2</span>][thread_y]) = FETCH_FLOAT4(As[load_stage_idx][j+<span class="number">1</span>][THREAD_SIZE_Y * ty + thread_y]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// load B from shared memory to register</span></span><br><span class="line">    <span class="meta">#<span class="meta-keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> thread_x = <span class="number">0</span>; thread_x &lt; THREAD_SIZE_X; thread_x += <span class="number">4</span>) &#123;</span><br><span class="line">        FETCH_FLOAT4(frag_b[(j+<span class="number">1</span>)%<span class="number">2</span>][thread_x]) = FETCH_FLOAT4(Bs[load_stage_idx][j+<span class="number">1</span>][THREAD_SIZE_X * tx + thread_x]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// compute C THREAD_SIZE_X x THREAD_SIZE_Y</span></span><br><span class="line">    <span class="meta">#<span class="meta-keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> thread_y = <span class="number">0</span>; thread_y &lt; THREAD_SIZE_Y; ++thread_y) &#123;</span><br><span class="line">        <span class="meta">#<span class="meta-keyword">pragma</span> unroll</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> thread_x = <span class="number">0</span>; thread_x &lt; THREAD_SIZE_X; ++thread_x) &#123;</span><br><span class="line">            accum[thread_y][thread_x] += frag_a[j%<span class="number">2</span>][thread_y] * frag_b[j%<span class="number">2</span>][thread_x];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>而后需要将存储在<strong>临时寄存器</strong>的数据搬运到shared memory中。由于A矩阵需要经过一次转置，所以和B矩阵有一点不一样。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>(tile_idx &lt; K)&#123;</span><br><span class="line">    <span class="meta">#<span class="meta-keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> ( <span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; BLOCK_SIZE_M ; i += A_TILE_ROW_STRIDE) &#123;</span><br><span class="line">        <span class="keyword">int</span> ldg_index = i / A_TILE_ROW_STRIDE * <span class="number">4</span>;</span><br><span class="line">        As[write_stage_idx][A_TILE_COL][A_TILE_ROW_START + i]=ldg_a_reg[ldg_index];</span><br><span class="line">        As[write_stage_idx][A_TILE_COL+<span class="number">1</span>][A_TILE_ROW_START + i]=ldg_a_reg[ldg_index+<span class="number">1</span>];</span><br><span class="line">        As[write_stage_idx][A_TILE_COL+<span class="number">2</span>][A_TILE_ROW_START + i]=ldg_a_reg[ldg_index+<span class="number">2</span>];</span><br><span class="line">        As[write_stage_idx][A_TILE_COL+<span class="number">3</span>][A_TILE_ROW_START + i]=ldg_a_reg[ldg_index+<span class="number">3</span>];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// load B from global memory to shared memory</span></span><br><span class="line">    <span class="meta">#<span class="meta-keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> ( <span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; BLOCK_SIZE_K; i += B_TILE_ROW_STRIDE) &#123;</span><br><span class="line">        <span class="keyword">int</span> ldg_index = i / A_TILE_ROW_STRIDE * <span class="number">4</span>;</span><br><span class="line">        FETCH_FLOAT4(Bs[write_stage_idx][B_TILE_ROW_START + i][B_TILE_COL]) = FETCH_FLOAT4(ldg_b_reg[ldg_index]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// use double buffer, only need one sync</span></span><br><span class="line">    __syncthreads();</span><br><span class="line">    <span class="comment">// switch</span></span><br><span class="line">    write_stage_idx ^= <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>最后完成<strong>寄存器的预取</strong>，并将<strong>最后一个小迭代完成</strong>。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// load A from shared memory to register</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> unroll</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> thread_y = <span class="number">0</span>; thread_y &lt; THREAD_SIZE_Y; thread_y += <span class="number">4</span>) &#123;</span><br><span class="line">    FETCH_FLOAT4(frag_a[<span class="number">0</span>][thread_y]) = FETCH_FLOAT4(As[load_stage_idx^<span class="number">1</span>][<span class="number">0</span>][THREAD_SIZE_Y * ty + thread_y]);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// load B from shared memory to register</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> unroll</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> thread_x = <span class="number">0</span>; thread_x &lt; THREAD_SIZE_X; thread_x += <span class="number">4</span>) &#123;</span><br><span class="line">    FETCH_FLOAT4(frag_b[<span class="number">0</span>][thread_x]) = FETCH_FLOAT4(Bs[load_stage_idx^<span class="number">1</span>][<span class="number">0</span>][THREAD_SIZE_X * tx + thread_x]);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//compute last tile mma THREAD_SIZE_X x THREAD_SIZE_Y</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> unroll</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> thread_y = <span class="number">0</span>; thread_y &lt; THREAD_SIZE_Y; ++thread_y) &#123;</span><br><span class="line">    <span class="meta">#<span class="meta-keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> thread_x = <span class="number">0</span>; thread_x &lt; THREAD_SIZE_X; ++thread_x) &#123;</span><br><span class="line">        accum[thread_y][thread_x] += frag_a[<span class="number">1</span>][thread_y] * frag_b[<span class="number">1</span>][thread_x];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="计算结果写回"><a href="#计算结果写回" class="headerlink" title="计算结果写回"></a>计算结果写回</h3><p>此时，最后的计算结果已经被存储在<code>accum</code>寄存器中，需要将其写回到global memory中。这个代码比较简单，就没啥好说的了。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// store back to C</span></span><br><span class="line">    <span class="meta">#<span class="meta-keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> thread_y = <span class="number">0</span>; thread_y &lt; THREAD_SIZE_Y; ++thread_y) &#123;</span><br><span class="line">        <span class="meta">#<span class="meta-keyword">pragma</span> unroll</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> thread_x = <span class="number">0</span>; thread_x &lt; THREAD_SIZE_X; thread_x+=<span class="number">4</span>) &#123;</span><br><span class="line">            FETCH_FLOAT4(C[OFFSET(</span><br><span class="line">                BLOCK_SIZE_M * by + ty * THREAD_SIZE_Y + thread_y,</span><br><span class="line">                BLOCK_SIZE_N * bx + tx * THREAD_SIZE_X + thread_x,</span><br><span class="line">                N)]) = FETCH_FLOAT4(accum[thread_y][thread_x]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>针对GEMM性能优化，我做了一些实验，主要是想要说明这么两个问题：</p>
<ol>
<li>在<strong>不采用任何汇编的情况</strong>下，手写CUDA代码会比cublas差多少？</li>
<li>bm、bn、bk、rm、rn等相关<strong>参数对GEMM的性能表现</strong>有多大影响？</li>
</ol>
<p><strong>针对第一个问题</strong>，固定了bm、bn、bk、rm、rn的取值为64、8、64、8、8。在V100上测试了不同维度的矩阵（设置M=N=K），并且对比了cublas，其性能结果如下图。横坐标是矩阵维度，纵坐标是GFLOPS。可以在图中看出，在大维度的矩阵下，我们手写的Sgemm大概能达到平均14TFLOPS，<strong>性能表现达到cublas的 91%</strong>。V100的单精度峰值性能是15.7TFLOPS，在<strong>完全不使用汇编，并且有着较好的代码可读性的同时</strong>，我们手写的Sgemm大概能达到<strong>90%</strong>的单精度峰值效率。当然，<strong>如果不考虑代码可读性的话，这个性能可以进一步提高</strong>。在这里可以得出结论，其实也是想消除大家的一个误解。<strong>很多人觉得只有写汇编才能写出高性能的代码</strong>。其实并不是这样，<strong>性能优化中最重要的是并行算法和优化策略，单纯地将代码写成汇编并不会有多少性能提升。</strong></p>
<p><img src="/img/image-20220910221600420.png" alt="image-20220910221600420"></p>
<h2 id="从汇编代码分析程序性能"><a href="#从汇编代码分析程序性能" class="headerlink" title="从汇编代码分析程序性能"></a>从汇编代码分析程序性能</h2><p>我们为什么要去看生成的汇编代码？这主要是由于做完优化之后，我们需要有一个东西来判断机器是否能够真正地按照我们设想的模式运行。使用了float4之后，GPU是不是真的使用了向量化指令。采用循环展开之后，GPU是不是真的会进行展开？另外，CUDA C和汇编代码之间还隔着编译器。只有看最底层的汇编码，才能真正地理解我们所做的优化是在哪个地方起了作用，节省了哪个部分的耗时。</p>
<p>NV的GPU提供了ptx和sass两个层面的汇编码。Ptx本质上是一个伪汇编码，事实上机器真正能够识别的是sass码。Ptx还需要使用ptxas工具再转化成sass码才能被GPU识别。然后nv提供了cuobjdump和nvdisasm两个工具，我们可以通过这两个工具来看到最底层的汇编码。</p>
<p>NV每一代机器的指令集都有所不同。此外，NV的指令还有一个特别有意思的东西，那就是control code，后面直接用控制码表示。通过控制码将一些本来应该在硬件实现的逻辑软件化了，从而在同样大小的电路面积上塞下更大的计算单元。</p>
<p>当我们在看汇编代码的时候，我们到底看的是什么东西。这个话题可以分为两部分介绍，分别是访存密集型的kernel和计算密集型的kernel。</p>
<p>对于<strong>访存密集型的kernel</strong>，正常而言，我们需要关注的是：访问global memory的时候是不是合并访存了，访问shared memory的时候是不是有bank 冲突了。很不幸的是，在汇编代码中，这些东西其实不太能看得出来。我们主要关注的是有没有采用LDG.128的访存指令，以及计算指令的占比是不是太多，#pragma unroll是不是有效展开了。</p>
<p>对于<strong>计算密集型的kernel</strong>而言，我们重点关注计算指令的占比。这个一般跟并行策略会联系在一起。一般而言，如果并行策略不太行，那么计算指令的占比会很低，这样的话，访存所导致的latency很难被计算指令掩盖，计算效率会非常差。如果并行策略比较好，那么计算指令的占比也会非常地高。也只有当计算指令占比非常高的时候，才有可能地去逼近峰值性能。</p>
<h2 id="对于现有sgemm的代码分析及观察"><a href="#对于现有sgemm的代码分析及观察" class="headerlink" title="对于现有sgemm的代码分析及观察"></a>对于现有sgemm的代码分析及观察</h2><p>在分析之前，我们对目前已有的工作先做一个回顾。sgemm是hpc领域的经典问题，目前有大量的论文在针对不同硬件架构，不同矩阵特性进行研究。对于NV的GPU，关于sgemm最著名的工作是scott的<a href="https://link.zhihu.com/?target=https%3A//github.com/NervanaSystems/maxas/wiki/SGEMM" target="_blank" rel="noopener">maxas</a>。在Maxwell架构上的部分卡上能够达到98%的浮点性能，几乎到达极限。也就是从这个工作以后，针对NV的sgemm优化工作基本上就没法做了，关于针对大矩阵的sgemm优化，也没有太多的研究价值了。当然，针对不同硬件架构的sgemm优化还是层出不出，但基本上是一些follow的工作，然后做一些小修小补。</p>
<p>我们来分析一下scott的工作。在CUDA C层面，不涉及汇编的话，优化技巧主要有3个方面：</p>
<p>技巧1，global-&gt;shared memory，采用了texture内存，将线程划分，一半线程只读A，一半线程只读B。</p>
<p>技巧2，shared memory-&gt;register，将8×8的读取变成4个4×4的读取，从而避免bank冲突。</p>
<p><img src="/img/image-20220910221941317.png" alt="image-20220910221941317"></p>
<p>技巧3，Store C矩阵的时候，为了合并访存，采用了一种非常奇怪的方式去store。</p>
<p><img src="/img/image-20220910221952729.png" alt="image-20220910221952729"></p>
<p>针对大矩阵的sgemm计算时。如果k维度足够大，global-&gt;shared memory以及store C的耗时占比会非常小，所以这两个优化技巧在大矩阵中并不能起到很大的作用。所以相对来说，<strong>技巧2会更加具有借鉴意义</strong>。</p>
<p>紧接着，我们来分析一下sgemm中最耗时的部分，也就是最内层的迭代部分。需要计算8×8×8=512次乘加运算。Scott的sgemm在maxwell产生的汇编代码如下图左，为了比较，我们将GEMM（二）中的代码sgemm_v2最后生成的<a href="https://link.zhihu.com/?target=https%3A//github.com/Liu-xiandong/How_to_optimize_in_GPU/blob/master/sgemm/asm/sgemm_pre.sm_70.cuasm" target="_blank" rel="noopener">SASS码</a>放在一起用以比较。</p>
<p><img src="/img/image-20220910222351625.png" alt="image-20220910222351625"></p>
<p>可以从上面看到，512条FFMA和32条LDS指令，最核心的计算指令和访存指令都是一样的。但是GEMM（二）中用编译器产生的汇编码有更多的非计算指令存在。而且如果从上面的链接点进去的话，就会发现，FFMA指令被划到2个代码块中，相对而言，中间会多一个跳转指令。另外一个需要注意的点是scott的代码是针对Maxwell架构，所以将可以用于双发射的指令进行了单独标记。而笔者写的代码是在volta架构上编译运行的，volta架构取消了双发射。但是两个cycle发射一条FFMA指令就可以将所有的fp32 core填满。计算指令和访存指令占据不同的发射端口，计算和访存可以隔一个cycle发射。所以我的猜想是这样的，对于volta架构，t0 cycle的时候发射一条FFMA指令，t1 cycle的时候发射一条LDS指令，而后t2时刻再发射一条FFMA指令。这样的话，FFMA指令隔了2个cycle，中间还发射了一条LDS指令，但fp32的core依旧是被用满的状态。这样的话，即使没有了双发射，理论上也能将fp32 core打满。从volta架构编译出来的控制码中也可以看出一些端倪，如下，FFMA指令stall两个cycle，而LDS指令stall一个cycle。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[R---:B------:R-:W-:-:S02]         /*0cd0*/                   FFMA R115, R39.reuse, R14, R115 ;</span><br><span class="line">[----:B------:R-:W-:-:S02]         /*0ce0*/                   FFMA R114, R39, R15, R114 ;</span><br><span class="line">[----:B------:R-:W1:-:S01]         /*0cf0*/                   LDS.U.128 R36, [R40+0x2410] ;</span><br><span class="line">[R---:B------:R-:W-:-:S02]         /*0d00*/                   FFMA R113, R32.reuse, R12, R113 ;</span><br></pre></td></tr></table></figure>
<p>然后总结一下这小节的内容，从CUDA C和SASS代码的角度分析了现有sgemm实现的不足。进一步的优化工作可以从两个方面进行：1、shared memory-&gt;register，将8×8的读取变成4个4×4的读取。2、尽可能地减少非必要指令的开销，但是这个在CUDA C层面很难控制，毕竟编译器也没那么听话。</p>
<h2 id="汇编级别代码调整"><a href="#汇编级别代码调整" class="headerlink" title="汇编级别代码调整"></a>汇编级别代码调整</h2><p>好了，终于讲到了调汇编的地方。上面小节说了，优化的一个方式是尽可能地减少非必要指令的开销。但是，当我们开始调汇编的时候，还有一个更重要的事情需要做，也是在maxas、KeplerAs等一系列工作的核心，<strong>减少FFMA指令所产生的register bank冲突</strong>。这里面有两个优化技巧，一个是寄存器的重映射，另外一个是调整FFMA顺序，尽可能地在指令中使用.reuse标识以及提高双发射的效率。</p>
<h3 id="寄存器的重映射"><a href="#寄存器的重映射" class="headerlink" title="寄存器的重映射"></a>寄存器的重映射</h3><p>在这里面，由于每代架构中的硬件细节有所不同，所以register的remapping细节也有所不同。首先说一下这里面的硬件细节不同是指，不同的架构中，寄存器到bank的映射方式不同。kepler架构的映射比较奇怪，并不是很规则，如下：</p>
<p><img src="/img/image-20220910222431933.png" alt="image-20220910222431933"></p>
<p>对于Maxwell架构而言，相对来说更加简单一些，bank index即reg_index%4这么一个简单的关系。Pascal架构和Maxwell架构的寄存器bank映射关系一样。而volta架构又有一些不同，在volta之前都是4路的bank，而volta架构变成了2路的bank。</p>
<p>由于架构不一样，针对不同架构的register重映射方式也不一样。对于kepler架构，keplerAs的作者采用的映射方式如下：</p>
<p><img src="/img/image-20220910222443696.png" alt="image-20220910222443696"></p>
<p>对于Maxwell架构，Scott采用的映射方式如下：</p>
<p><img src="/img/image-20220910222453184.png" alt="image-20220910222453184"></p>
<p>上图中间那些带黑框的数字代表不可避免的寄存器冲突，scott随后又使用了指令重排来减缓寄存器的冲突。</p>
<p>而volta架构的话，Dissecting the NVIDIA Volta GPU Architecture via Microbenchmarking作者采用的方式如下，作了一个转置，然后相邻两行进行一个交换。</p>
<p><img src="/img/image-20220910222504575.png" alt="image-20220910222504575"></p>
<h3 id="指令重排"><a href="#指令重排" class="headerlink" title="指令重排"></a>指令重排</h3><p>这里的指令重排主要是针对FFMA指令的重排。作用的话，其实有两个。在maxwell架构中，scott重排主要是为了尽可能地解决对角线那些元素的寄存器bank冲突。在这里插一嘴，因为部分读者对于这个重排可能理解不是很到位。举个例子吧，要计算C矩阵中1，2，3，4，5的元素的值，正常的顺序是调用FFMA指令先算1，再算2，再算3，等等。重排的话，就是可能先算2，再算1，再算3。从指令角度的话，就是FFMA指令的排列顺序有所不同，所以叫指令重排，这个是我的个人理解。</p>
<p>重排的目的是为了更好地使用reuse标识，这个地方可以看看旷视写的<a href="https://zhuanlan.zhihu.com/p/410278370" target="_blank" rel="noopener">矩阵乘终极优化指南</a>，当然，基本上也就是scott的sgemm介绍内容。读取指令的操作数的时候，有一个寄存器的reuse cache。在指令中使用这个标识就代表这个数被hold住了，下一条指令可以直接使用。这个地方，大家都是这么说的，NV也没有官方的说明，那就这么理解吧。具体示意代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">FFMA R2, R64.reuse, R73, R2; # R64 进入 Reuse Cache</span><br><span class="line">FFMA R3, R64.reuse, R72, R3; # R64 从 Reuse Cache 中获取，避免与 R72 冲突</span><br></pre></td></tr></table></figure>
<p>为了更好地利用这个reuse特性，scott给了一种非常奇怪的指令排列顺序，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> 1,  0,  2,  3,  5,  4,  6,  7, 33, 32, 34, 35, 37, 36, 38, 39, </span><br><span class="line">45, 44, 46, 47, 41, 40, 42, 43, 13, 12, 14, 15,  9,  8, 10, 11,</span><br><span class="line">17, 16, 18, 19, 21, 20, 22, 23, 49, 48, 50, 51, 53, 52, 54, 55, </span><br><span class="line">61, 60, 62, 63, 57, 56, 58, 59, 29, 28, 30, 31, 25, 24, 26, 27</span><br></pre></td></tr></table></figure>
<p>通过CUDA C说的一系列优化手段，以及寄存器的remapping和指令重排，scott的sgemm在Maxwell架构的一些卡上能够达到98%的浮点计算效率，达到了优化的天花板。</p>
<p>扯远了，再说说指令重排，keplerAs的作者张秀霞针对kepler的双发射特性对FFMA指令进行了指令重排来提高性能。这个跟scott的工作又有一些不一样的地方，大家可以对比一下。</p>
<h2 id="实验与总结"><a href="#实验与总结" class="headerlink" title="实验与总结"></a>实验与总结</h2><p>最后，我们来做一下实验。实验分成两个部分，第一个部分是CUDA C层面的再次优化，第二个部分是针对SASS代码的调优工作以及中间经历的一些波折。</p>
<h3 id="CUDA-C-调优"><a href="#CUDA-C-调优" class="headerlink" title="CUDA C 调优"></a>CUDA C 调优</h3><p>这个部分的内容主要是介绍一下怎么解决GEMM（二）所存在的shared memory bank冲突。其实scott的文章已经说了这一点，但是吧，实在是太费解了。首先，再来回顾一下这个思路。我们一个block有256个线程，8个warp，8个warp要去取shared memory中的半行元素，也就是128/2=64个元素。warp0和warp4取得是同样的16个元素。而warp里面，线程0、2、4、6、8、10、12、14是取得同样的4个元素。由于取得是同样的元素，同一个bank触发多播的机制，没有冲突。取多少元素说清楚了，就得说一下shared memory的索引了。scott给出的256线程版本索引是：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">readAs = ((tid128 &gt;&gt; <span class="number">4</span>) | ((tid &gt;&gt; <span class="number">1</span>) &amp; <span class="number">7</span>)) &lt;&lt; <span class="number">4</span>;</span><br><span class="line">readBs  = (((tid &amp; <span class="number">0x70</span>) &gt;&gt; <span class="number">3</span>) | (tid &amp; <span class="number">1</span>)) &lt;&lt; <span class="number">4</span> + <span class="number">4096</span>;</span><br></pre></td></tr></table></figure>
<p><img src="/img/image-20220910222717437.png" alt="image-20220910222717437"></p>
<p>总之，这个索引给我整不会了。作为一个正常的人类，我实在是不太能直观地去理解这个位运算。思量许久，我决定用一种最简单粗暴的索引计算方式。我们本质上是要知道，每一个线程，对应到128个元素中的哪一个元素？这个是我们的核心问题。</p>
<p>我来说一下我的计算方法，以B矩阵对应的shared memory为例，首先，计算warp_id，也就是当前线程属于哪个warp，由tid/32即可得。随后计算lane_id，即当前线程属于这个warp上得哪个线程，由tid%32即可得。随后就是通过warp_id和lane_id来算出，对应128个元素得哪一个元素。先算(warp_id%4)×16，假设是warp2，就是上图左侧的第2个（从0算）warp。前面有2个warp，跳过了2*16=32个元素。然后再看看当前lane_id。0-15在左半边，16-31在右半边。所以lane_id/16，先看是左半边还是右半边。右半边的话，先跳过8个元素。最后再看lane_id的奇偶数，如果奇数的话，就再跳一个四个元素。代码实现如下，这个就是正常人可以看懂的方式了。对A矩阵的映射关系同理。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//load index of the tile</span></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> warp_id = tid / <span class="number">32</span>;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> lane_id = tid % <span class="number">32</span>;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> tile_index_b = (warp_id%<span class="number">4</span>)*<span class="number">16</span> + (lane_id/<span class="number">16</span>)*<span class="number">8</span> + (lane_id%<span class="number">2</span>)*<span class="number">4</span>;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> tile_index_a = (warp_id/<span class="number">4</span>)*<span class="number">32</span> + ((lane_id%<span class="number">16</span>)/<span class="number">2</span>)*<span class="number">4</span>;</span><br></pre></td></tr></table></figure>
<p>然后shared memory取数的代码更改就是下面这样，以B矩阵块为例：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 改变前</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> unroll</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> thread_y = <span class="number">0</span>; thread_y &lt; THREAD_SIZE_Y; thread_y += <span class="number">4</span>) &#123;</span><br><span class="line">     FETCH_FLOAT4(frag_b[(j+<span class="number">1</span>)%<span class="number">2</span>][thread_y]) = FETCH_FLOAT4(Bs[next_stage_flag][(j+<span class="number">1</span>)%BLOCK_SIZE_K][THREAD_SIZE_Y * ty + thread_y]);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 改变后</span></span><br><span class="line">FETCH_FLOAT4(frag_b[(j+<span class="number">1</span>)%<span class="number">2</span>][<span class="number">0</span>]) = FETCH_FLOAT4(Bs[next_stage_flag][(j+<span class="number">1</span>)%BLOCK_SIZE_K][tile_index]);</span><br><span class="line">FETCH_FLOAT4(frag_b[(j+<span class="number">1</span>)%<span class="number">2</span>][<span class="number">4</span>]) = FETCH_FLOAT4(Bs[next_stage_flag][(j+<span class="number">1</span>)%BLOCK_SIZE_K][tile_index + <span class="number">64</span>]);</span><br></pre></td></tr></table></figure>
<p>当然，因为用来寄存C的64个元素对应的位置变化，所以最后的store C的过程也有代码变动。</p>
<p>在进行了这个修改之后，4096（M=N=K）的矩阵大概可以达到96-97%的cublas的性能。单精度峰值浮点效率达93%左右。再往下想要持平或者超越cublas的话，就只能动汇编了。</p>
<h3 id="汇编代码调优"><a href="#汇编代码调优" class="headerlink" title="汇编代码调优"></a>汇编代码调优</h3><p>在做寄存器remapping的时候，发现NVCC编译出来的代码是这个样子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">FFMA R125, R52, R44, R72 ;</span><br><span class="line">FFMA R122, R53, R44.reuse, R73 ;</span><br><span class="line">FFMA R74, R54, R44.reuse, R74 ;</span><br><span class="line">FFMA R75, R55, R44.reuse, R75 ;</span><br></pre></td></tr></table></figure>
<p>看看第一条指令，做R125=R52×R44+R72，R72的值被拿出来，然后存到了R125上。编译出来的代码有一大堆这样的指令。而我希望所有的指令都满足第3条的样子，R74=R54×R44+R74，从R74取就放回R74才最好。如果不能保证这个形式的话，就意味着，我们不能让固定的寄存器来存储矩阵C中的固定的值。这玩意做remapping的话，就不能简简单单地改寄存器号。毕竟我也不能确定不同的寄存器对应到哪个具体的值了。</p>
<p>当时想了各种方式，调整CUDA C代码来让nvcc编译出我想要的FFMA格式，但是，这个尝试并不能实现。所以接下来，有两个方式，一个是头铁，搞清楚这个100多个寄存器在512条FFMA指令中对应的物理元素，然后做remapping，这个路线中间会遇到可以预想的无数的bug和计算问题。另一个是参考Maxas，把这玩意整合到汇编器上，定义好每个寄存器的对应元素和排列顺序。然后汇编器顺带着处理，如下：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">&lt;REGISTER_MAPPING&gt;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Temporary registers to calculate the state registers. Reuse the C output registers.</span></span><br><span class="line">    <span class="comment">// These can be dynamically allocated (~) in the available registger space to elimiate any register bank conflicts.</span></span><br><span class="line">    <span class="number">0</span><span class="number">-63</span>    ~ blk, ldx, ldx2, ldx4, k, tid1, tid4, tid7, tid31_4, xmad_t0, xmad_end, bxOrig, byOrig, loy</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Aliases for the C registers we use for initializing C (used as vectors)</span></span><br><span class="line">    <span class="number">0</span><span class="number">-63</span>    : cz&lt;<span class="number">00</span><span class="number">-63</span>&gt;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// The offset we store our zero value for initializing C. Reuse a register from the second blocking registers</span></span><br><span class="line">    <span class="number">80</span>      : zOffset</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 64 C maxtrix output registers.</span></span><br><span class="line">    <span class="comment">// Use special mapping to avoid register bank conflicts between these registers and the blocking registers.</span></span><br><span class="line">     <span class="number">3</span>, <span class="number">2</span>,<span class="number">11</span>,<span class="number">10</span>,<span class="number">19</span>,<span class="number">18</span>,<span class="number">27</span>,<span class="number">26</span> : cx00y&lt;<span class="number">00</span><span class="number">-03</span>|<span class="number">64</span><span class="number">-67</span>&gt;</span><br><span class="line">     <span class="number">7</span>, <span class="number">6</span>,<span class="number">15</span>,<span class="number">14</span>,<span class="number">23</span>,<span class="number">22</span>,<span class="number">31</span>,<span class="number">30</span> : cx01y&lt;<span class="number">00</span><span class="number">-03</span>|<span class="number">64</span><span class="number">-67</span>&gt;</span><br><span class="line">     <span class="number">1</span>, <span class="number">0</span>, <span class="number">9</span>, <span class="number">8</span>,<span class="number">17</span>,<span class="number">16</span>,<span class="number">25</span>,<span class="number">24</span> : cx02y&lt;<span class="number">00</span><span class="number">-03</span>|<span class="number">64</span><span class="number">-67</span>&gt;</span><br><span class="line">     <span class="number">5</span>, <span class="number">4</span>,<span class="number">13</span>,<span class="number">12</span>,<span class="number">21</span>,<span class="number">20</span>,<span class="number">29</span>,<span class="number">28</span> : cx03y&lt;<span class="number">00</span><span class="number">-03</span>|<span class="number">64</span><span class="number">-67</span>&gt;</span><br><span class="line">    <span class="number">35</span>,<span class="number">34</span>,<span class="number">43</span>,<span class="number">42</span>,<span class="number">51</span>,<span class="number">50</span>,<span class="number">59</span>,<span class="number">58</span> : cx64y&lt;<span class="number">00</span><span class="number">-03</span>|<span class="number">64</span><span class="number">-67</span>&gt;</span><br><span class="line">    <span class="number">39</span>,<span class="number">38</span>,<span class="number">47</span>,<span class="number">46</span>,<span class="number">55</span>,<span class="number">54</span>,<span class="number">63</span>,<span class="number">62</span> : cx65y&lt;<span class="number">00</span><span class="number">-03</span>|<span class="number">64</span><span class="number">-67</span>&gt;</span><br><span class="line">    <span class="number">33</span>,<span class="number">32</span>,<span class="number">41</span>,<span class="number">40</span>,<span class="number">49</span>,<span class="number">48</span>,<span class="number">57</span>,<span class="number">56</span> : cx66y&lt;<span class="number">00</span><span class="number">-03</span>|<span class="number">64</span><span class="number">-67</span>&gt;</span><br><span class="line">    <span class="number">37</span>,<span class="number">36</span>,<span class="number">45</span>,<span class="number">44</span>,<span class="number">53</span>,<span class="number">52</span>,<span class="number">61</span>,<span class="number">60</span> : cx67y&lt;<span class="number">00</span><span class="number">-03</span>|<span class="number">64</span><span class="number">-67</span>&gt;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Double buffered register blocking used in vector loads.</span></span><br><span class="line">    <span class="comment">// Any bank conflicts that we can't avoid in these registers we can hide with .reuse flags</span></span><br><span class="line">    <span class="number">64</span><span class="number">-79</span>   : j0Ax&lt;<span class="number">00</span><span class="number">-03</span>|<span class="number">64</span><span class="number">-67</span>&gt;, j0By&lt;<span class="number">00</span><span class="number">-03</span>|<span class="number">64</span><span class="number">-67</span>&gt;</span><br><span class="line">    <span class="number">80</span><span class="number">-95</span>   : j1Ax&lt;<span class="number">00</span><span class="number">-03</span>|<span class="number">64</span><span class="number">-67</span>&gt;, j1By&lt;<span class="number">00</span><span class="number">-03</span>|<span class="number">64</span><span class="number">-67</span>&gt;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Registers to load A or B</span></span><br><span class="line">    <span class="number">96</span><span class="number">-103</span>  : loadX&lt;<span class="number">0</span><span class="number">-7</span>&gt;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Key global state registers for main loop and some we reuse for outputing C.</span></span><br><span class="line">    <span class="comment">// Note, tweaking the register banks of track&lt;0|4&gt;, tex, writeS, readBs, readAs impacts performance because of</span></span><br><span class="line">    <span class="comment">// delayed bank conflicts between memory operations and ffmas.</span></span><br><span class="line">    <span class="comment">// The array index bracket notation can be used to request a bank in a dynamically allocated range.</span></span><br><span class="line">    <span class="number">104</span><span class="number">-127</span> ~ track&lt;<span class="number">0</span>|<span class="number">4</span>&gt;[<span class="number">0</span>], tex[<span class="number">2</span>], readAs[<span class="number">2</span>], readBs[<span class="number">3</span>], writeS[<span class="number">3</span>], end, ldx8, tid, bx, by, tid31, tid96, tid128 <span class="comment">//, clock, smId, nSMs</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Registers to store the results back to global memory. Reuse any register not needed after the main loop.</span></span><br><span class="line">    <span class="comment">// Statically allocate cs0-7 because they're vector registers.</span></span><br><span class="line">    <span class="number">64</span><span class="number">-71</span>   : cs&lt;<span class="number">0</span><span class="number">-7</span>&gt;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// dynamically allocated C output registers(~)</span></span><br><span class="line">    <span class="number">72</span><span class="number">-103</span>  ~ cy&lt;<span class="number">00</span>|<span class="number">04</span>|<span class="number">08</span>|<span class="number">12</span>&gt;, Cy&lt;<span class="number">00</span>|<span class="number">04</span>|<span class="number">08</span>|<span class="number">12</span>&gt;, ldc, ldc1, ldc4, ldc8, ldc60, writeCs, readCs, cx, ci, alpha, xmad_ci <span class="comment">//, xmad_D, D, blckDimX, gridDimX</span></span><br><span class="line"></span><br><span class="line">&lt;/REGISTER_MAPPING&gt;</span><br></pre></td></tr></table></figure>
<p>然而，我只是想简简单单写个sgemm，我并不想把我有限的周末时间全部投进去，毕竟读者也没给我钱。然后想想指令重排，通过reuse标识也能解决一部分reg的bank冲突，那就整这个吧。</p>
<p>遇到的另一个问题就是指令重排。我把里面所有存在寄存器bank冲突的指令列了出来。再来看看volta架构中的bank冲突，volta架构的寄存器有2路bank，奇数寄存器号代表bank0，偶数寄存器号代表bank1。如果FFMA指令的三个源寄存器的寄存器号都属于奇数或者偶数，那么就发生了bank冲突。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">//0    FFMA R74, R36, R62.reuse, R74 ;    </span><br><span class="line">//1    FFMA R78, R34, R62.reuse, R78 ;</span><br><span class="line">//2    FFMA R16, R35, R62, R54 ;</span><br></pre></td></tr></table></figure>
<p>比如上面的代码，0号指令和1号三个源寄存器都是偶数，不考虑reuse标识的话，都有bank冲突，而2号指令就没有bank冲突。调整这3个的位置，变成：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">//2    FFMA R16, R35, R62.reuse, R54 ;   </span><br><span class="line">//1    FFMA R78, R34, R62.reuse, R78 ; </span><br><span class="line">/ 0    FFMA R74, R36, R62.reuse, R74 ;</span><br></pre></td></tr></table></figure>
<p>让指令2的R62放入reuse cache中，指令1和指令0继续使用这个数，从而减少bank冲突。更改前后的代码在我的<a href="https://link.zhihu.com/?target=https%3A//github.com/Liu-xiandong/How_to_optimize_in_GPU/tree/master/sgemm/asm" target="_blank" rel="noopener">github repo</a>中。但是改完之后，我发现性能提升并不是很明显，大概就是1%左右的性能提升。这可能是在sgemm_v2的基础上改的原因，当时4.1所说的shared memory bank冲突还比较明显。总之，实验大概就是这样子。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// optimize sgemm</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"assert.h"</span> </span></span><br><span class="line"></span><br><span class="line"><span class="comment">// CUDA runtime</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cublas_v2.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// cal offset from row col and ld , in row-major matrix, ld is the width of the matrix</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> OFFSET(row, col, ld) ((row) * (ld) + (col))</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// transfer float4</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> FETCH_FLOAT4(pointer) (reinterpret_cast<span class="meta-string">&lt;float4*&gt;(&amp;(pointer))[0])</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> checkCudaErrors(func)				\</span></span><br><span class="line">&#123;									\</span><br><span class="line">    cudaError_t e = (func);			\</span><br><span class="line">    <span class="keyword">if</span>(e != cudaSuccess)						                \</span><br><span class="line">        <span class="built_in">printf</span> (<span class="string">"%s %d CUDA: %s\n"</span>, __FILE__,  __LINE__, cudaGetErrorString(e));		\</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// K: ldA</span></span><br><span class="line"><span class="comment">// N: ldB</span></span><br><span class="line"><span class="keyword">template</span> &lt;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> BLOCK_SIZE_M,  <span class="comment">// height of block of C that each thread block calculate</span></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> BLOCK_SIZE_K,  <span class="comment">// width of block of A that each thread block load into shared memory</span></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> BLOCK_SIZE_N,  <span class="comment">// width of block of C that each thread block calculate</span></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> THREAD_SIZE_Y, <span class="comment">// height of block of C that each thread calculate</span></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> THREAD_SIZE_X,  <span class="comment">// width of block of C that each thread calculate</span></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">bool</span> ENABLE_DOUBLE_BUFFER <span class="comment">// whether enable double buffering or not</span></span><br><span class="line">    &gt; </span><br><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">Sgemm</span><span class="params">( </span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">float</span> * __restrict__ A,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">float</span> * __restrict__ B,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">float</span> * __restrict__ C, </span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> <span class="keyword">int</span> M,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> <span class="keyword">int</span> N,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> <span class="keyword">int</span> K)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// Block index</span></span><br><span class="line">    <span class="keyword">int</span> bx = blockIdx.x;</span><br><span class="line">    <span class="keyword">int</span> by = blockIdx.y;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Thread index</span></span><br><span class="line">    <span class="keyword">int</span> tx = threadIdx.x;</span><br><span class="line">    <span class="keyword">int</span> ty = threadIdx.y;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// the threads number in Block of X,Y</span></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> THREAD_X_PER_BLOCK = BLOCK_SIZE_N / THREAD_SIZE_X;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> THREAD_Y_PER_BLOCK = BLOCK_SIZE_M / THREAD_SIZE_Y;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> THREAD_NUM_PER_BLOCK = THREAD_X_PER_BLOCK * THREAD_Y_PER_BLOCK;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// thread id in cur Block</span></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> tid = ty * THREAD_X_PER_BLOCK + tx;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// shared memory</span></span><br><span class="line">    __shared__ <span class="keyword">float</span> As[<span class="number">2</span>][BLOCK_SIZE_K][BLOCK_SIZE_M];</span><br><span class="line">    __shared__ <span class="keyword">float</span> Bs[<span class="number">2</span>][BLOCK_SIZE_K][BLOCK_SIZE_N];</span><br><span class="line">    <span class="comment">// registers for C</span></span><br><span class="line">    <span class="keyword">float</span> accum[THREAD_SIZE_Y][THREAD_SIZE_X];</span><br><span class="line">    <span class="meta">#<span class="meta-keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;THREAD_SIZE_Y; i++)&#123;</span><br><span class="line">        <span class="meta">#<span class="meta-keyword">pragma</span> unroll</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>; j&lt;THREAD_SIZE_X; j++)&#123;</span><br><span class="line">            accum[i][j]=<span class="number">0.0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// registers for A and B</span></span><br><span class="line">    <span class="keyword">float</span> frag_a[<span class="number">2</span>][THREAD_SIZE_Y];</span><br><span class="line">    <span class="keyword">float</span> frag_b[<span class="number">2</span>][THREAD_SIZE_X];</span><br><span class="line">    <span class="comment">// registers load global memory</span></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> ldg_num_a = BLOCK_SIZE_M * BLOCK_SIZE_K / (THREAD_NUM_PER_BLOCK * <span class="number">4</span>);</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> ldg_num_b = BLOCK_SIZE_K * BLOCK_SIZE_N / (THREAD_NUM_PER_BLOCK * <span class="number">4</span>);</span><br><span class="line">    <span class="keyword">float</span> ldg_a_reg[<span class="number">4</span>*ldg_num_a];</span><br><span class="line">    <span class="keyword">float</span> ldg_b_reg[<span class="number">4</span>*ldg_num_b];</span><br><span class="line"></span><br><span class="line">    <span class="comment">// threads number in one row</span></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> A_TILE_THREAD_PER_ROW = BLOCK_SIZE_K / <span class="number">4</span>;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> B_TILE_THREAD_PER_ROW = BLOCK_SIZE_N / <span class="number">4</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// row number and col number that needs to be loaded by this thread</span></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> A_TILE_ROW_START = tid / A_TILE_THREAD_PER_ROW;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> B_TILE_ROW_START = tid / B_TILE_THREAD_PER_ROW;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> A_TILE_COL = tid % A_TILE_THREAD_PER_ROW * <span class="number">4</span>; </span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> B_TILE_COL = tid % B_TILE_THREAD_PER_ROW * <span class="number">4</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// row stride that thread uses to load multiple rows of a tile</span></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> A_TILE_ROW_STRIDE = THREAD_NUM_PER_BLOCK / A_TILE_THREAD_PER_ROW;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> B_TILE_ROW_STRIDE = THREAD_NUM_PER_BLOCK / B_TILE_THREAD_PER_ROW;</span><br><span class="line"></span><br><span class="line">    A = &amp;A[(BLOCK_SIZE_M * by)* K];</span><br><span class="line">    B = &amp;B[BLOCK_SIZE_N * bx];</span><br><span class="line"></span><br><span class="line">    <span class="comment">//load index of the tile</span></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> warp_id = tid / <span class="number">32</span>;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> lane_id = tid % <span class="number">32</span>;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> a_tile_index =  warp_id/<span class="number">2</span>*<span class="number">16</span> + lane_id/<span class="number">8</span>*<span class="number">4</span>; <span class="comment">//warp_id * 8 + (lane_id / 16)*4; // (warp_id/4)*32 + ((lane_id%16)/2)*4;</span></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> b_tile_index =  warp_id%<span class="number">2</span>*<span class="number">32</span> + lane_id%<span class="number">8</span>*<span class="number">4</span>; <span class="comment">//(lane_id % 16) * 4; // (warp_id%4)*16 + (lane_id/16)*8 + (lane_id%2)*4;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">//transfer first tile from global mem to shared mem</span></span><br><span class="line">    <span class="comment">// load A from global memory to shared memory</span></span><br><span class="line">    <span class="meta">#<span class="meta-keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> ( <span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; BLOCK_SIZE_M ; i += A_TILE_ROW_STRIDE) &#123;</span><br><span class="line">        <span class="keyword">int</span> ldg_index = i / A_TILE_ROW_STRIDE * <span class="number">4</span>;</span><br><span class="line">        FETCH_FLOAT4(ldg_a_reg[ldg_index]) = FETCH_FLOAT4(A[OFFSET(</span><br><span class="line">            A_TILE_ROW_START + i, <span class="comment">// row</span></span><br><span class="line">            A_TILE_COL, <span class="comment">// col</span></span><br><span class="line">            K )]);</span><br><span class="line">        As[<span class="number">0</span>][A_TILE_COL][A_TILE_ROW_START + i]=ldg_a_reg[ldg_index];</span><br><span class="line">        As[<span class="number">0</span>][A_TILE_COL+<span class="number">1</span>][A_TILE_ROW_START + i]=ldg_a_reg[ldg_index+<span class="number">1</span>];</span><br><span class="line">        As[<span class="number">0</span>][A_TILE_COL+<span class="number">2</span>][A_TILE_ROW_START + i]=ldg_a_reg[ldg_index+<span class="number">2</span>];</span><br><span class="line">        As[<span class="number">0</span>][A_TILE_COL+<span class="number">3</span>][A_TILE_ROW_START + i]=ldg_a_reg[ldg_index+<span class="number">3</span>];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// load B from global memory to shared memory</span></span><br><span class="line">    <span class="meta">#<span class="meta-keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> ( <span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; BLOCK_SIZE_K; i += B_TILE_ROW_STRIDE) &#123;</span><br><span class="line">        FETCH_FLOAT4(Bs[<span class="number">0</span>][B_TILE_ROW_START + i][B_TILE_COL]) = FETCH_FLOAT4(B[OFFSET(</span><br><span class="line">                B_TILE_ROW_START + i, <span class="comment">// row</span></span><br><span class="line">                B_TILE_COL, <span class="comment">// col</span></span><br><span class="line">                N )]);</span><br><span class="line">    &#125;</span><br><span class="line">    __syncthreads();</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// load A from shared memory to register</span></span><br><span class="line">    FETCH_FLOAT4(frag_a[<span class="number">0</span>][<span class="number">0</span>]) = FETCH_FLOAT4(As[<span class="number">0</span>][<span class="number">0</span>][a_tile_index]);</span><br><span class="line">    FETCH_FLOAT4(frag_a[<span class="number">0</span>][<span class="number">4</span>]) = FETCH_FLOAT4(As[<span class="number">0</span>][<span class="number">0</span>][a_tile_index + <span class="number">64</span>]);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// load B from shared memory to register</span></span><br><span class="line">    FETCH_FLOAT4(frag_b[<span class="number">0</span>][<span class="number">0</span>]) = FETCH_FLOAT4(Bs[<span class="number">0</span>][<span class="number">0</span>][b_tile_index]);</span><br><span class="line">    FETCH_FLOAT4(frag_b[<span class="number">0</span>][<span class="number">4</span>]) = FETCH_FLOAT4(Bs[<span class="number">0</span>][<span class="number">0</span>][b_tile_index + <span class="number">64</span>]);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">int</span> write_stage_idx = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> tile_idx = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">do</span>&#123;</span><br><span class="line">        <span class="comment">// next tile index</span></span><br><span class="line">        tile_idx += BLOCK_SIZE_K;</span><br><span class="line">        <span class="comment">// load next tile from global mem</span></span><br><span class="line">        <span class="keyword">if</span>(tile_idx&lt; K)&#123;</span><br><span class="line">            <span class="meta">#<span class="meta-keyword">pragma</span> unroll</span></span><br><span class="line">            <span class="keyword">for</span> ( <span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; BLOCK_SIZE_M ; i += A_TILE_ROW_STRIDE) &#123;</span><br><span class="line">                <span class="keyword">int</span> ldg_index = i / A_TILE_ROW_STRIDE * <span class="number">4</span>;</span><br><span class="line">                FETCH_FLOAT4(ldg_a_reg[ldg_index]) = FETCH_FLOAT4(A[OFFSET(</span><br><span class="line">                    A_TILE_ROW_START + i, <span class="comment">// row</span></span><br><span class="line">                    A_TILE_COL + tile_idx, <span class="comment">// col</span></span><br><span class="line">                    K )]);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="meta">#<span class="meta-keyword">pragma</span> unroll</span></span><br><span class="line">            <span class="keyword">for</span> ( <span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; BLOCK_SIZE_K; i += B_TILE_ROW_STRIDE) &#123;</span><br><span class="line">                <span class="keyword">int</span> ldg_index = i / B_TILE_ROW_STRIDE * <span class="number">4</span>;</span><br><span class="line">                FETCH_FLOAT4(ldg_b_reg[ldg_index]) = FETCH_FLOAT4(B[OFFSET(</span><br><span class="line">                    tile_idx + B_TILE_ROW_START + i, <span class="comment">// row</span></span><br><span class="line">                    B_TILE_COL, <span class="comment">// col</span></span><br><span class="line">                    N )]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> load_stage_idx = write_stage_idx ^ <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="meta">#<span class="meta-keyword">pragma</span> unroll</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>; j&lt;BLOCK_SIZE_K - <span class="number">1</span>; ++j)&#123;</span><br><span class="line">            <span class="comment">// load next tile from shared mem to register </span></span><br><span class="line">            <span class="comment">// load A from shared memory to register</span></span><br><span class="line">            FETCH_FLOAT4(frag_a[(j+<span class="number">1</span>)%<span class="number">2</span>][<span class="number">0</span>]) = FETCH_FLOAT4(As[load_stage_idx][(j+<span class="number">1</span>)][a_tile_index]);</span><br><span class="line">            FETCH_FLOAT4(frag_a[(j+<span class="number">1</span>)%<span class="number">2</span>][<span class="number">4</span>]) = FETCH_FLOAT4(As[load_stage_idx][(j+<span class="number">1</span>)][a_tile_index + <span class="number">64</span>]);</span><br><span class="line">            <span class="comment">// load B from shared memory to register</span></span><br><span class="line">            FETCH_FLOAT4(frag_b[(j+<span class="number">1</span>)%<span class="number">2</span>][<span class="number">0</span>]) = FETCH_FLOAT4(Bs[load_stage_idx][(j+<span class="number">1</span>)][b_tile_index]);</span><br><span class="line">            FETCH_FLOAT4(frag_b[(j+<span class="number">1</span>)%<span class="number">2</span>][<span class="number">4</span>]) = FETCH_FLOAT4(Bs[load_stage_idx][(j+<span class="number">1</span>)][b_tile_index + <span class="number">64</span>]);</span><br><span class="line">            <span class="comment">// compute C THREAD_SIZE_X x THREAD_SIZE_Y</span></span><br><span class="line">            <span class="meta">#<span class="meta-keyword">pragma</span> unroll</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> thread_y = <span class="number">0</span>; thread_y &lt; THREAD_SIZE_Y; ++thread_y) &#123;</span><br><span class="line">                <span class="meta">#<span class="meta-keyword">pragma</span> unroll</span></span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">int</span> thread_x = <span class="number">0</span>; thread_x &lt; THREAD_SIZE_X; ++thread_x) &#123;</span><br><span class="line">                    accum[thread_y][thread_x] += frag_a[j%<span class="number">2</span>][thread_y] * frag_b[j%<span class="number">2</span>][thread_x];</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(tile_idx &lt; K)&#123;</span><br><span class="line">            <span class="comment">// load A from global memory to shared memory</span></span><br><span class="line">            <span class="meta">#<span class="meta-keyword">pragma</span> unroll</span></span><br><span class="line">            <span class="keyword">for</span> ( <span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; BLOCK_SIZE_M ; i += A_TILE_ROW_STRIDE) &#123;</span><br><span class="line">                <span class="keyword">int</span> ldg_index = i / A_TILE_ROW_STRIDE * <span class="number">4</span>;</span><br><span class="line">                As[write_stage_idx][A_TILE_COL][A_TILE_ROW_START + i]=ldg_a_reg[ldg_index];</span><br><span class="line">                As[write_stage_idx][A_TILE_COL+<span class="number">1</span>][A_TILE_ROW_START + i]=ldg_a_reg[ldg_index+<span class="number">1</span>];</span><br><span class="line">                As[write_stage_idx][A_TILE_COL+<span class="number">2</span>][A_TILE_ROW_START + i]=ldg_a_reg[ldg_index+<span class="number">2</span>];</span><br><span class="line">                As[write_stage_idx][A_TILE_COL+<span class="number">3</span>][A_TILE_ROW_START + i]=ldg_a_reg[ldg_index+<span class="number">3</span>];</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// load B from global memory to shared memory</span></span><br><span class="line">            <span class="meta">#<span class="meta-keyword">pragma</span> unroll</span></span><br><span class="line">            <span class="keyword">for</span> ( <span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; BLOCK_SIZE_K; i += B_TILE_ROW_STRIDE) &#123;</span><br><span class="line">                <span class="keyword">int</span> ldg_index = i / B_TILE_ROW_STRIDE * <span class="number">4</span>;</span><br><span class="line">                FETCH_FLOAT4(Bs[write_stage_idx][B_TILE_ROW_START + i][B_TILE_COL]) = FETCH_FLOAT4(ldg_b_reg[ldg_index]);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// use double buffer, only need one sync</span></span><br><span class="line">            __syncthreads();</span><br><span class="line">            <span class="comment">// switch</span></span><br><span class="line">            write_stage_idx ^= <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// load first tile from shared mem to register of next iter</span></span><br><span class="line">        <span class="comment">// load A from shared memory to register</span></span><br><span class="line">        FETCH_FLOAT4(frag_a[<span class="number">0</span>][<span class="number">0</span>]) = FETCH_FLOAT4(As[load_stage_idx^<span class="number">1</span>][<span class="number">0</span>][a_tile_index]);</span><br><span class="line">        FETCH_FLOAT4(frag_a[<span class="number">0</span>][<span class="number">4</span>]) = FETCH_FLOAT4(As[load_stage_idx^<span class="number">1</span>][<span class="number">0</span>][a_tile_index + <span class="number">64</span>]);</span><br><span class="line">        <span class="comment">// load B from shared memory to register</span></span><br><span class="line">        FETCH_FLOAT4(frag_b[<span class="number">0</span>][<span class="number">0</span>]) = FETCH_FLOAT4(Bs[load_stage_idx^<span class="number">1</span>][<span class="number">0</span>][b_tile_index]);</span><br><span class="line">        FETCH_FLOAT4(frag_b[<span class="number">0</span>][<span class="number">4</span>]) = FETCH_FLOAT4(Bs[load_stage_idx^<span class="number">1</span>][<span class="number">0</span>][b_tile_index + <span class="number">64</span>]);</span><br><span class="line">        <span class="comment">// compute C THREAD_SIZE_X x THREAD_SIZE_Y</span></span><br><span class="line">        <span class="meta">#<span class="meta-keyword">pragma</span> unroll</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> thread_y = <span class="number">0</span>; thread_y &lt; THREAD_SIZE_Y; ++thread_y) &#123;</span><br><span class="line">            <span class="meta">#<span class="meta-keyword">pragma</span> unroll</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> thread_x = <span class="number">0</span>; thread_x &lt; THREAD_SIZE_X; ++thread_x) &#123;</span><br><span class="line">                accum[thread_y][thread_x] += frag_a[<span class="number">1</span>][thread_y] * frag_b[<span class="number">1</span>][thread_x];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;<span class="keyword">while</span>(tile_idx&lt; K);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> c_block_row = a_tile_index;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> c_block_col = b_tile_index;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//store C00 block</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;<span class="number">4</span>; i++)&#123;</span><br><span class="line">      FETCH_FLOAT4(C[OFFSET(</span><br><span class="line">        BLOCK_SIZE_M * by + c_block_row + i,</span><br><span class="line">        BLOCK_SIZE_N * bx + c_block_col,</span><br><span class="line">        N)]) = FETCH_FLOAT4(accum[i][<span class="number">0</span>]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//store C01 block</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;<span class="number">4</span>; i++)&#123;</span><br><span class="line">      FETCH_FLOAT4(C[OFFSET(</span><br><span class="line">        BLOCK_SIZE_M * by + c_block_row + i,</span><br><span class="line">        BLOCK_SIZE_N * bx + c_block_col + <span class="number">64</span>,</span><br><span class="line">        N)]) = FETCH_FLOAT4(accum[i][<span class="number">4</span>]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//store C10 block</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;<span class="number">4</span>; i++)&#123;</span><br><span class="line">      FETCH_FLOAT4(C[OFFSET(</span><br><span class="line">        BLOCK_SIZE_M * by + c_block_row + <span class="number">64</span> + i,</span><br><span class="line">        BLOCK_SIZE_N * bx + c_block_col,</span><br><span class="line">        N)]) = FETCH_FLOAT4(accum[i+<span class="number">4</span>][<span class="number">0</span>]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//store C11 block</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;<span class="number">4</span>; i++)&#123;</span><br><span class="line">      FETCH_FLOAT4(C[OFFSET(</span><br><span class="line">        BLOCK_SIZE_M * by + c_block_row + <span class="number">64</span> + i,</span><br><span class="line">        BLOCK_SIZE_N * bx + c_block_col + <span class="number">64</span>,</span><br><span class="line">        N)]) = FETCH_FLOAT4(accum[i+<span class="number">4</span>][<span class="number">4</span>]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (argc != <span class="number">4</span>) &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"usage: ./main [M] [K] [N]\n"</span>);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">size_t</span> M = atoi(argv[<span class="number">1</span>]);</span><br><span class="line">    <span class="keyword">size_t</span> K = atoi(argv[<span class="number">2</span>]);</span><br><span class="line">    <span class="keyword">size_t</span> N = atoi(argv[<span class="number">3</span>]);</span><br><span class="line"></span><br><span class="line">    assert( M%<span class="number">8</span> == <span class="number">0</span>); </span><br><span class="line">    assert( N%<span class="number">8</span> == <span class="number">0</span>); </span><br><span class="line">    assert( K%<span class="number">8</span> == <span class="number">0</span>); </span><br><span class="line"></span><br><span class="line">    <span class="keyword">size_t</span> bytes_A = <span class="keyword">sizeof</span>(<span class="keyword">float</span>) * M * K;</span><br><span class="line">    <span class="keyword">size_t</span> bytes_B = <span class="keyword">sizeof</span>(<span class="keyword">float</span>) * K * N;</span><br><span class="line">    <span class="keyword">size_t</span> bytes_C = <span class="keyword">sizeof</span>(<span class="keyword">float</span>) * M * N;</span><br><span class="line">    <span class="keyword">float</span>* h_A = (<span class="keyword">float</span>*)<span class="built_in">malloc</span>(bytes_A);</span><br><span class="line">    <span class="keyword">float</span>* h_B = (<span class="keyword">float</span>*)<span class="built_in">malloc</span>(bytes_B);</span><br><span class="line">    <span class="keyword">float</span>* h_C = (<span class="keyword">float</span>*)<span class="built_in">malloc</span>(bytes_C);</span><br><span class="line">    <span class="keyword">float</span>* h_C1 = (<span class="keyword">float</span>*)<span class="built_in">malloc</span>(bytes_C);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">float</span>* d_A;</span><br><span class="line">    <span class="keyword">float</span>* d_B;</span><br><span class="line">    <span class="keyword">float</span>* d_C;</span><br><span class="line"></span><br><span class="line">    checkCudaErrors(cudaMalloc(&amp;d_A, bytes_A));</span><br><span class="line">    checkCudaErrors(cudaMalloc(&amp;d_B, bytes_B));</span><br><span class="line">    checkCudaErrors(cudaMalloc(&amp;d_C, bytes_C));</span><br><span class="line">    <span class="keyword">double</span> msecPerMatrixMul[<span class="number">2</span>] = &#123;<span class="number">0</span>, <span class="number">0</span>&#125;;</span><br><span class="line">    <span class="keyword">double</span> gigaFlops[<span class="number">2</span>] = &#123;<span class="number">0</span>, <span class="number">0</span>&#125;;</span><br><span class="line">    <span class="keyword">double</span> flopsPerMatrixMul = <span class="number">2.0</span> * M * N * K;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// don't edit it</span></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> BLOCK_SIZE_M = <span class="number">128</span>;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> BLOCK_SIZE_K = <span class="number">8</span>;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> BLOCK_SIZE_N = <span class="number">128</span>;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> THREAD_SIZE_X = <span class="number">8</span>;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> THREAD_SIZE_Y = <span class="number">8</span>;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">bool</span> ENABLE_DOUBLE_BUFFER = <span class="literal">false</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 生成A的数据</span></span><br><span class="line">    <span class="keyword">for</span>( <span class="keyword">int</span> i = <span class="number">0</span>; i &lt; M * K; i++ ) &#123;</span><br><span class="line">        h_A[i] = i / <span class="number">13</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 生成B的数据</span></span><br><span class="line">    <span class="keyword">for</span>( <span class="keyword">int</span> i = <span class="number">0</span>; i &lt; K * N; i++ ) &#123;</span><br><span class="line">        h_B[i] = i % <span class="number">13</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    checkCudaErrors(cudaMemcpy( d_A, h_A, bytes_A, cudaMemcpyHostToDevice));</span><br><span class="line">    checkCudaErrors(cudaMemcpy( d_B, h_B, bytes_B, cudaMemcpyHostToDevice));</span><br><span class="line">    </span><br><span class="line">    cudaEvent_t start, stop;</span><br><span class="line">    checkCudaErrors(cudaEventCreate(&amp;start));</span><br><span class="line">    checkCudaErrors(cudaEventCreate(&amp;stop));</span><br><span class="line">    <span class="keyword">float</span> msecTotal = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> nIter = <span class="number">1000</span>;</span><br><span class="line"></span><br><span class="line">    checkCudaErrors(cudaMemcpy( d_C, h_C, bytes_C, cudaMemcpyHostToDevice));</span><br><span class="line">    checkCudaErrors(cudaEventRecord(start));</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> run = <span class="number">0</span> ; run &lt; nIter; run ++ ) &#123;</span><br><span class="line">        <span class="function">dim3 <span class="title">dimBlock</span><span class="params">(BLOCK_SIZE_N / THREAD_SIZE_X, BLOCK_SIZE_M / THREAD_SIZE_Y)</span></span>;</span><br><span class="line">        <span class="function">dim3 <span class="title">dimGrid</span><span class="params">(N / BLOCK_SIZE_N, M / BLOCK_SIZE_M)</span></span>;</span><br><span class="line">        Sgemm&lt;BLOCK_SIZE_M, BLOCK_SIZE_K, BLOCK_SIZE_N, THREAD_SIZE_Y, THREAD_SIZE_X, ENABLE_DOUBLE_BUFFER&gt; </span><br><span class="line">        &lt;&lt;&lt; dimGrid, dimBlock &gt;&gt;&gt;(d_A, d_B, d_C, M, N, K);</span><br><span class="line">    &#125;</span><br><span class="line">    checkCudaErrors(cudaEventRecord(stop));</span><br><span class="line">    checkCudaErrors(cudaEventSynchronize(stop));</span><br><span class="line">    checkCudaErrors(cudaEventElapsedTime(&amp;msecTotal, start, stop));</span><br><span class="line">    checkCudaErrors(cudaMemcpy( h_C, d_C, bytes_C, cudaMemcpyDeviceToHost));</span><br><span class="line"></span><br><span class="line">    msecPerMatrixMul[<span class="number">0</span>] = msecTotal / nIter;</span><br><span class="line">    gigaFlops[<span class="number">0</span>] = (flopsPerMatrixMul * <span class="number">1.0e-9</span>f) / (msecPerMatrixMul[<span class="number">0</span>] / <span class="number">1000.0f</span>);</span><br><span class="line">    <span class="built_in">printf</span>( <span class="string">"My gemm Performance= %.2f GFlop/s, Time= %.3f msec, Size= %.0f Ops,\n"</span>,</span><br><span class="line">        gigaFlops[<span class="number">0</span>],</span><br><span class="line">        msecPerMatrixMul[<span class="number">0</span>],</span><br><span class="line">        flopsPerMatrixMul);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// cublas</span></span><br><span class="line">    </span><br><span class="line">    cublasHandle_t blas_handle;  </span><br><span class="line">    cublasCreate(&amp;blas_handle);</span><br><span class="line">    <span class="keyword">float</span> alpha = <span class="number">1.0</span>;</span><br><span class="line">    <span class="keyword">float</span> beta = <span class="number">0</span>;</span><br><span class="line">    checkCudaErrors(cudaMemcpy( d_C, h_C, bytes_C, cudaMemcpyHostToDevice));</span><br><span class="line">    checkCudaErrors(cudaEventRecord(start));</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> run = <span class="number">0</span> ; run &lt; nIter; run ++ ) &#123;</span><br><span class="line">        cublasSgemm (blas_handle, CUBLAS_OP_T, CUBLAS_OP_T, </span><br><span class="line">            M, N, K, &amp;alpha, </span><br><span class="line">            d_A, K, d_B, N, &amp;beta, d_C, N</span><br><span class="line">        );</span><br><span class="line">    &#125;</span><br><span class="line">    checkCudaErrors(cudaEventRecord(stop));</span><br><span class="line">    checkCudaErrors(cudaEventSynchronize(stop));</span><br><span class="line">    checkCudaErrors(cudaEventElapsedTime(&amp;msecTotal, start, stop));</span><br><span class="line"></span><br><span class="line">    checkCudaErrors(cudaMemcpy( h_C1, d_C, bytes_C, cudaMemcpyDeviceToHost));</span><br><span class="line"></span><br><span class="line">    msecPerMatrixMul[<span class="number">1</span>] = msecTotal / nIter;</span><br><span class="line">    gigaFlops[<span class="number">1</span>] = (flopsPerMatrixMul * <span class="number">1.0e-9</span>f) / (msecPerMatrixMul[<span class="number">1</span>] / <span class="number">1000.0f</span>);</span><br><span class="line">    <span class="built_in">printf</span>( <span class="string">"CuBlas Performance= %.2f GFlop/s, Time= %.3f msec, Size= %.0f Ops,\n"</span>,</span><br><span class="line">        gigaFlops[<span class="number">1</span>],</span><br><span class="line">        msecPerMatrixMul[<span class="number">1</span>],</span><br><span class="line">        flopsPerMatrixMul);</span><br><span class="line"></span><br><span class="line">    cublasDestroy(blas_handle); </span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">double</span> eps = <span class="number">1.e-6</span>;  <span class="comment">// machine zero</span></span><br><span class="line">    <span class="keyword">bool</span> correct = <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; M * N; i++) &#123;</span><br><span class="line">        <span class="keyword">int</span> row = i / N;</span><br><span class="line">        <span class="keyword">int</span> col = i % N;</span><br><span class="line">        <span class="keyword">double</span> abs_err = <span class="built_in">fabs</span>(h_C[i] - h_C1[col * M + row]);</span><br><span class="line">        <span class="keyword">double</span> dot_length = M;</span><br><span class="line">        <span class="keyword">double</span> abs_val = <span class="built_in">fabs</span>(h_C[i]);</span><br><span class="line">        <span class="keyword">double</span> rel_err = abs_err / abs_val / dot_length;</span><br><span class="line">        <span class="keyword">if</span> (rel_err &gt; eps) &#123;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"Error! Matrix[%d][%d]=%.8f, ref=%.8f error term is &gt; %E\n"</span>,</span><br><span class="line">                    row, col, h_C[i], h_C1[col * M + row], eps);</span><br><span class="line">            correct = <span class="literal">false</span>;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%s\n"</span>, correct ? <span class="string">"Result= PASS"</span> : <span class="string">"Result= FAIL"</span>);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"ratio= %f\n"</span>, gigaFlops[<span class="number">0</span>] / gigaFlops[<span class="number">1</span>]);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Free Memory</span></span><br><span class="line">    cudaFree(d_A);</span><br><span class="line">    cudaFree(d_B);</span><br><span class="line">    cudaFree(d_C);</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">free</span>(h_A);</span><br><span class="line">    <span class="built_in">free</span>(h_B);</span><br><span class="line">    <span class="built_in">free</span>(h_C);</span><br><span class="line">    <span class="built_in">free</span>(h_C1);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/C/" rel="tag"># C++</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2022/09/07/colab上使用cuda/" rel="next" title="Colab上使用cuda">
                <i class="fa fa-chevron-left"></i> Colab上使用cuda
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2022/09/27/买房要点/" rel="prev" title="买房要点">
                买房要点 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/header.jpg" alt="Hao Yu">
            
              <p class="site-author-name" itemprop="name">Hao Yu</p>
              <p class="site-description motion-element" itemprop="description">Introduce something interesting and recode learning process, some articles are written by others, the original link has been given as much as possible, thanks to the original author</p>
          </div>
          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">247</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">13</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>
        	<audio controls="controls" loop="loop" preload="auto" src="/resource/xiaomeihao.mp3">
	        </audio>
	

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/yuhao0102" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:yuh18@mails.tsinghua.edu.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#前言"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#并行算法设计"><span class="nav-number">2.</span> <span class="nav-text">并行算法设计</span></a></li></ol><li class="nav-item nav-level-1"><a class="nav-link" href="#reduce优化"><span class="nav-number"></span> <span class="nav-text">reduce优化</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#reduce-baseline算法介绍"><span class="nav-number">1.</span> <span class="nav-text">reduce baseline算法介绍</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#优化技巧1：解决warp-divergence"><span class="nav-number">2.</span> <span class="nav-text">优化技巧1：解决warp divergence</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#现有问题"><span class="nav-number">2.1.</span> <span class="nav-text">现有问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#解决方式"><span class="nav-number">2.2.</span> <span class="nav-text">解决方式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#优化技巧2：解决bank冲突"><span class="nav-number">3.</span> <span class="nav-text">优化技巧2：解决bank冲突</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#现有问题-1"><span class="nav-number">3.1.</span> <span class="nav-text">现有问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#解决方式-1"><span class="nav-number">3.2.</span> <span class="nav-text">解决方式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#优化技巧3：解决idle线程"><span class="nav-number">4.</span> <span class="nav-text">优化技巧3：解决idle线程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#现有问题-2"><span class="nav-number">4.1.</span> <span class="nav-text">现有问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#解决方式-2"><span class="nav-number">4.2.</span> <span class="nav-text">解决方式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#优化技巧4：展开最后一维减少同步"><span class="nav-number">5.</span> <span class="nav-text">优化技巧4：展开最后一维减少同步</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#现有问题-3"><span class="nav-number">5.1.</span> <span class="nav-text">现有问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#解决方式-3"><span class="nav-number">5.2.</span> <span class="nav-text">解决方式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#优化技巧5：完全展开减少计算"><span class="nav-number">6.</span> <span class="nav-text">优化技巧5：完全展开减少计算</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#现有问题-4"><span class="nav-number">6.1.</span> <span class="nav-text">现有问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#解决方法"><span class="nav-number">6.2.</span> <span class="nav-text">解决方法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#优化技巧6：合理设置block数量"><span class="nav-number">7.</span> <span class="nav-text">优化技巧6：合理设置block数量</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#现有问题-5"><span class="nav-number">7.1.</span> <span class="nav-text">现有问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#解决方式-4"><span class="nav-number">7.2.</span> <span class="nav-text">解决方式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#优化技巧7：使用shuffle指令"><span class="nav-number">8.</span> <span class="nav-text">优化技巧7：使用shuffle指令</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#现有问题-6"><span class="nav-number">8.1.</span> <span class="nav-text">现有问题</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#GEMM优化"><span class="nav-number"></span> <span class="nav-text">GEMM优化</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#前言-1"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#从global-memory到shared-memory"><span class="nav-number">2.</span> <span class="nav-text">从global memory到shared memory</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#从shared-memory到register"><span class="nav-number">3.</span> <span class="nav-text">从shared memory到register</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#register分块"><span class="nav-number">4.</span> <span class="nav-text">register分块</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#数据的prefetch"><span class="nav-number">5.</span> <span class="nav-text">数据的prefetch</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#GEMM算法概述"><span class="nav-number"></span> <span class="nav-text">GEMM算法概述</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#不采用数据预取"><span class="nav-number">1.</span> <span class="nav-text">不采用数据预取</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#采用数据预取"><span class="nav-number">2.</span> <span class="nav-text">采用数据预取</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GEMM代码解析"><span class="nav-number">3.</span> <span class="nav-text">GEMM代码解析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#参数说明"><span class="nav-number">3.1.</span> <span class="nav-text">参数说明</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#大迭代前预取数据"><span class="nav-number">3.2.</span> <span class="nav-text">大迭代前预取数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#大迭代逻辑"><span class="nav-number">3.3.</span> <span class="nav-text">大迭代逻辑</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#大迭代详细解析"><span class="nav-number">3.4.</span> <span class="nav-text">大迭代详细解析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#计算结果写回"><span class="nav-number">3.5.</span> <span class="nav-text">计算结果写回</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#实验"><span class="nav-number">4.</span> <span class="nav-text">实验</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#从汇编代码分析程序性能"><span class="nav-number">5.</span> <span class="nav-text">从汇编代码分析程序性能</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#对于现有sgemm的代码分析及观察"><span class="nav-number">6.</span> <span class="nav-text">对于现有sgemm的代码分析及观察</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#汇编级别代码调整"><span class="nav-number">7.</span> <span class="nav-text">汇编级别代码调整</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#寄存器的重映射"><span class="nav-number">7.1.</span> <span class="nav-text">寄存器的重映射</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#指令重排"><span class="nav-number">7.2.</span> <span class="nav-text">指令重排</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#实验与总结"><span class="nav-number">8.</span> <span class="nav-text">实验与总结</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#CUDA-C-调优"><span class="nav-number">8.1.</span> <span class="nav-text">CUDA C 调优</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#汇编代码调优"><span class="nav-number">8.2.</span> <span class="nav-text">汇编代码调优</span></a></li></ol></li></ol></li></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>
  <div id="DvelopmentTarget">     
  </div>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart" aria-hidden="false"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hao Yu</span>

  
</div>

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<span id="busuanzi_container_site_uv"> 
  本站访客数<span id="busuanzi_value_site_uv"></span>人次
</span>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  


  

  


  <script type="text/javascript" src="/js/src/love.js"></script>

</body>
</html>
