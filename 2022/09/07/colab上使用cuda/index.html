<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zn-ch">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-center-simple.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="C++,">










<meta name="description" content="https://colab.research.google.com/drive/1-dkuFrfver70j-UCpAp0XQZszsJexHkH#scrollTo=GJHxLzmGfb3G CUDA编程模型为应用和硬件设备之间的桥梁，所以CUDA C是编译型语言，不是解释型语言，OpenCL就有点类似于解释型语言，通过编译器和链接，给操作系统执行（操作系统包括GPU在内的系统） 首先安装插件并加">
<meta name="keywords" content="C++">
<meta property="og:type" content="article">
<meta property="og:title" content="Colab上使用cuda">
<meta property="og:url" content="http://yoursite.com/2022/09/07/colab上使用cuda/index.html">
<meta property="og:site_name" content="Hao Yu&#39;s blog">
<meta property="og:description" content="https://colab.research.google.com/drive/1-dkuFrfver70j-UCpAp0XQZszsJexHkH#scrollTo=GJHxLzmGfb3G CUDA编程模型为应用和硬件设备之间的桥梁，所以CUDA C是编译型语言，不是解释型语言，OpenCL就有点类似于解释型语言，通过编译器和链接，给操作系统执行（操作系统包括GPU在内的系统） 首先安装插件并加">
<meta property="og:locale" content="zn-ch">
<meta property="og:image" content="http://yoursite.com/img/20220907141600.png">
<meta property="og:image" content="http://yoursite.com/img/20220907141602.png">
<meta property="og:image" content="http://yoursite.com/img/20220907141612.png">
<meta property="og:image" content="http://yoursite.com/img/20220907141613.png">
<meta property="og:image" content="http://yoursite.com/img/20220907141614.png">
<meta property="og:image" content="http://yoursite.com/img/20220907141615.png">
<meta property="og:image" content="http://yoursite.com/img/20220907141616.png">
<meta property="og:image" content="http://yoursite.com/img/20220907141617.png">
<meta property="og:image" content="http://yoursite.com/img/20220907141618.png">
<meta property="og:image" content="c:/Users/69033/Desktop/image-20220910105700518.png">
<meta property="og:image" content="c:/Users/69033/Desktop/image-20220910105840559.png">
<meta property="og:updated_time" content="2022-09-10T03:08:01.624Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Colab上使用cuda">
<meta name="twitter:description" content="https://colab.research.google.com/drive/1-dkuFrfver70j-UCpAp0XQZszsJexHkH#scrollTo=GJHxLzmGfb3G CUDA编程模型为应用和硬件设备之间的桥梁，所以CUDA C是编译型语言，不是解释型语言，OpenCL就有点类似于解释型语言，通过编译器和链接，给操作系统执行（操作系统包括GPU在内的系统） 首先安装插件并加">
<meta name="twitter:image" content="http://yoursite.com/img/20220907141600.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":true,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2022/09/07/colab上使用cuda/">





  <title>Colab上使用cuda | Hao Yu's blog</title>
  








 
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zn-ch">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hao Yu's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">The program monkey was eaten by the siege lion.</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/resume.pdf" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-mybetterhalf">
          <a href="/mybetterhalf/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br>
            
            mybetterhalf
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2022/09/07/colab上使用cuda/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Hao Yu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hao Yu's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Colab上使用cuda</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2022-09-07T14:10:00+08:00">
                2022-09-07
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><a href="https://colab.research.google.com/drive/1-dkuFrfver70j-UCpAp0XQZszsJexHkH#scrollTo=GJHxLzmGfb3G" target="_blank" rel="noopener">https://colab.research.google.com/drive/1-dkuFrfver70j-UCpAp0XQZszsJexHkH#scrollTo=GJHxLzmGfb3G</a></p>
<p>CUDA编程模型为应用和硬件设备之间的桥梁，所以CUDA C是编译型语言，不是解释型语言，OpenCL就有点类似于解释型语言，通过编译器和链接，给操作系统执行（操作系统包括GPU在内的系统）</p>
<p>首先安装插件并加载：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">!git config --global http.sslVerify&quot;False&quot;</span><br><span class="line">!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git</span><br><span class="line">%load_ext nvcc_plugin</span><br></pre></td></tr></table></figure></p>
<p>从hello world开始：要在笔记本中运行代码，请在代码的开头添加%%cu扩展名。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">%%cu</span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"> </span><br><span class="line"><span class="comment">//kernel function</span></span><br><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">helloFromGPU</span> <span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Hello World from GPU!\n"</span>);</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  helloFromGPU &lt;&lt;&lt;<span class="number">1</span>, <span class="number">1</span>&gt;&gt;&gt;();</span><br><span class="line">  cudaDeviceSynchronize();</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><code>__global__</code>：声明一个函数作为一个存在的kernel。这样的一个函数是：</p>
<ol>
<li>在设备上执行的，</li>
<li>仅可从主机调用。</li>
<li>其调用形式为：<code>helloFromGPU&lt;&lt;&lt;1,10&gt;&gt;&gt;();</code></li>
</ol>
<p>一个kernel是由一组线程执行，所有线程执行相同的代码。上面一行三对尖括号中的1和10 ，表示启动一个 grid 为 螺纹块 的内核。执行配置中的第一个参数1指定网格中线程块的数量，第二个参数10指定线程块中的线程数。</p>
<p>有一点需要注意的是，printf的输出是在GPU内部执行的，你若想在控制台（网页上）收到该输出，你必须添加<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaDeviceSynchronize();</span><br></pre></td></tr></table></figure></p>
<p>矩阵加法：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">%%cu</span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"> </span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> VECTOR_LENGTH 10000 </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MAX_ERR 1e-4</span></span><br><span class="line"> </span><br><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">vector_add</span><span class="params">(<span class="keyword">float</span> *out, <span class="keyword">float</span> *a, <span class="keyword">float</span> *b, <span class="keyword">int</span> n)</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        out[i] = a[i] + b[i];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">float</span> *a, *b, *out;</span><br><span class="line">    <span class="keyword">float</span> *d_a, *d_b, *d_out; </span><br><span class="line"> </span><br><span class="line">    a = (<span class="keyword">float</span>*)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="keyword">float</span>) * VECTOR_LENGTH);</span><br><span class="line">    b = (<span class="keyword">float</span>*)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="keyword">float</span>) * VECTOR_LENGTH);</span><br><span class="line">    out = (<span class="keyword">float</span>*)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="keyword">float</span>) * VECTOR_LENGTH);</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; VECTOR_LENGTH; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        a[i] = <span class="number">3.0f</span>;</span><br><span class="line">        b[i] = <span class="number">0.14f</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    cudaMalloc((<span class="keyword">void</span>**)&amp;d_a, <span class="keyword">sizeof</span>(<span class="keyword">float</span>) * VECTOR_LENGTH);</span><br><span class="line">    cudaMalloc((<span class="keyword">void</span>**)&amp;d_b, <span class="keyword">sizeof</span>(<span class="keyword">float</span>) * VECTOR_LENGTH);</span><br><span class="line">    cudaMalloc((<span class="keyword">void</span>**)&amp;d_out, <span class="keyword">sizeof</span>(<span class="keyword">float</span>) * VECTOR_LENGTH);</span><br><span class="line"> </span><br><span class="line">    cudaMemcpy(d_a, a, <span class="keyword">sizeof</span>(<span class="keyword">float</span>) * VECTOR_LENGTH, cudaMemcpyHostToDevice);</span><br><span class="line">    cudaMemcpy(d_b, b, <span class="keyword">sizeof</span>(<span class="keyword">float</span>) * VECTOR_LENGTH, cudaMemcpyHostToDevice);</span><br><span class="line">    </span><br><span class="line">    vector_add&lt;&lt;&lt;<span class="number">1</span>,<span class="number">1</span>&gt;&gt;&gt;(d_out, d_a, d_b, VECTOR_LENGTH);</span><br><span class="line"> </span><br><span class="line">    cudaMemcpy(out, d_out, <span class="keyword">sizeof</span>(<span class="keyword">float</span>) * VECTOR_LENGTH, cudaMemcpyDeviceToHost);</span><br><span class="line">    <span class="comment">// Test the result</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; VECTOR_LENGTH; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%f\n"</span>, out[i]);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    cudaFree(d_a);</span><br><span class="line">    cudaFree(d_b);</span><br><span class="line">    cudaFree(d_out);</span><br><span class="line">    <span class="built_in">free</span>(a);</span><br><span class="line">    <span class="built_in">free</span>(b);</span><br><span class="line">    <span class="built_in">free</span>(out);</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>一个异构环境，通常有多个CPU多个GPU，他们都通过PCIe总线相互通信，也是通过PCIe总线分隔开的。所以我们要区分一下两种设备的内存：</p>
<ul>
<li>主机：CPU及其内存</li>
<li>设备：GPU及其内存</li>
</ul>
<p>注意这两个内存从硬件到软件都是隔离的（CUDA6.0 以后支持统一寻址），我们目前先不研究统一寻址，我们现在还是用内存来回拷贝的方法来编写调试程序，以巩固大家对两个内存隔离这个事实的理解。</p>
<p>一个完整的CUDA应用可能的执行顺序如下图：<br><img src="/img/20220907141600.png" alt></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>标准C函数</th>
<th>CUDA C 函数</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>malloc</td>
<td>cudaMalloc</td>
<td>内存分配</td>
</tr>
<tr>
<td>memcpy</td>
<td>cudaMemcpy</td>
<td>内存复制</td>
</tr>
<tr>
<td>memset</td>
<td>cudaMemset</td>
<td>内存设置</td>
</tr>
<tr>
<td>free</td>
<td>cudaFree</td>
<td>释放内存</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaMemcpy</span><span class="params">(<span class="keyword">void</span> * dst,<span class="keyword">const</span> <span class="keyword">void</span> * src,<span class="keyword">size_t</span> count,</span></span></span><br><span class="line"><span class="function"><span class="params">  cudaMemcpyKind kind)</span></span></span><br></pre></td></tr></table></figure>
<p>这个函数是内存拷贝过程，可以完成以下几种过程（cudaMemcpyKind kind）</p>
<ul>
<li>cudaMemcpyHostToHost</li>
<li>cudaMemcpyHostToDevice</li>
<li>cudaMemcpyDeviceToHost</li>
<li>cudaMemcpyDeviceToDevice</li>
</ul>
<p>这四个过程的方向可以清楚的从字面上看出来，这里就不废话了，如果函数执行成功，则会返回 cudaSuccess 否则返回 cudaErrorMemoryAllocation</p>
<p>使用下面这个指令可以吧上面的错误代码翻译成详细信息：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">char</span>* <span class="title">cudaGetErrorString</span><span class="params">(cudaError_t error)</span></span></span><br></pre></td></tr></table></figure></p>
<p>内存是分层次的，下图可以简单地描述，但是不够准确，后面我们会详细介绍每一个具体的环节：<br><img src="/img/20220907141602.png" alt></p>
<p>共享内存（shared Memory）和全局内存（global Memory）后面我们会特别详细深入的研究，这里我们来个例子，两个向量的加法：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line">%%cu</span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CHECK(a) a</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">checkResult</span><span class="params">(<span class="keyword">float</span> *a, <span class="keyword">float</span> *b, <span class="keyword">int</span> size)</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;size;i++) &#123;</span><br><span class="line">      <span class="keyword">if</span> (a[i] != b[i]) &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"the %d is different, %f, %f\n"</span>, i, a[i], b[i]);</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">else</span> </span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"the %d is same, %f, %f\n"</span>, i, a[i], b[i]);</span><br><span class="line">    &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">initialData</span><span class="params">(<span class="keyword">float</span> *a, <span class="keyword">int</span> size)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;size;i++)</span><br><span class="line">      a[i] = <span class="number">1.0</span> * i;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">sumArrays</span><span class="params">(<span class="keyword">float</span> * a,<span class="keyword">float</span> * b,<span class="keyword">float</span> * res,<span class="keyword">const</span> <span class="keyword">int</span> size)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;size;i+=<span class="number">4</span>)</span><br><span class="line">  &#123;</span><br><span class="line">    res[i]=a[i]+b[i];</span><br><span class="line">    res[i+<span class="number">1</span>]=a[i+<span class="number">1</span>]+b[i+<span class="number">1</span>];</span><br><span class="line">    res[i+<span class="number">2</span>]=a[i+<span class="number">2</span>]+b[i+<span class="number">2</span>];</span><br><span class="line">    res[i+<span class="number">3</span>]=a[i+<span class="number">3</span>]+b[i+<span class="number">3</span>];</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">sumArraysGPU</span><span class="params">(<span class="keyword">float</span>*a,<span class="keyword">float</span>*b,<span class="keyword">float</span>*res)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">int</span> i=threadIdx.x;</span><br><span class="line">  res[i]=a[i]+b[i];</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc,<span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">int</span> dev = <span class="number">0</span>;</span><br><span class="line">  cudaSetDevice(dev);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span> nElem=<span class="number">32</span>;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">"Vector size:%d\n"</span>,nElem);</span><br><span class="line">  <span class="keyword">int</span> nByte=<span class="keyword">sizeof</span>(<span class="keyword">float</span>)*nElem;</span><br><span class="line">  <span class="keyword">float</span> *a_h=(<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="keyword">float</span> *b_h=(<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="keyword">float</span> *res_h=(<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="keyword">float</span> *res_from_gpu_h=(<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="built_in">memset</span>(res_h,<span class="number">0</span>,nByte);</span><br><span class="line">  <span class="built_in">memset</span>(res_from_gpu_h,<span class="number">0</span>,nByte);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">float</span> *a_d,*b_d,*res_d;</span><br><span class="line">  CHECK(cudaMalloc((<span class="keyword">float</span>**)&amp;a_d,nByte));</span><br><span class="line">  CHECK(cudaMalloc((<span class="keyword">float</span>**)&amp;b_d,nByte));</span><br><span class="line">  CHECK(cudaMalloc((<span class="keyword">float</span>**)&amp;res_d,nByte));</span><br><span class="line"></span><br><span class="line">  initialData(a_h,nElem);</span><br><span class="line">  initialData(b_h,nElem);</span><br><span class="line"></span><br><span class="line">  CHECK(cudaMemcpy(a_d,a_h,nByte,cudaMemcpyHostToDevice));</span><br><span class="line">  CHECK(cudaMemcpy(b_d,b_h,nByte,cudaMemcpyHostToDevice));</span><br><span class="line"></span><br><span class="line">  <span class="function">dim3 <span class="title">block</span><span class="params">(nElem)</span></span>;</span><br><span class="line">  <span class="function">dim3 <span class="title">grid</span><span class="params">(nElem/block.x)</span></span>;</span><br><span class="line">  sumArraysGPU&lt;&lt;&lt;grid,block&gt;&gt;&gt;(a_d,b_d,res_d);</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">"Execution configuration&lt;&lt;&lt;%d,%d&gt;&gt;&gt;\n"</span>,block.x,grid.x);</span><br><span class="line"></span><br><span class="line">  CHECK(cudaMemcpy(res_from_gpu_h,res_d,nByte,cudaMemcpyDeviceToHost));</span><br><span class="line">  sumArrays(a_h,b_h,res_h,nElem);</span><br><span class="line"></span><br><span class="line">  checkResult(res_h,res_from_gpu_h,nElem);</span><br><span class="line">  cudaFree(a_d);</span><br><span class="line">  cudaFree(b_d);</span><br><span class="line">  cudaFree(res_d);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">free</span>(a_h);</span><br><span class="line">  <span class="built_in">free</span>(b_h);</span><br><span class="line">  <span class="built_in">free</span>(res_h);</span><br><span class="line">  <span class="built_in">free</span>(res_from_gpu_h);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="线程管理"><a href="#线程管理" class="headerlink" title="线程管理"></a>线程管理</h2><p>当内核函数开始执行，如何组织GPU的线程就变成了最主要的问题了，我们必须明确，一个核函数只能有一个grid，一个grid可以有很多个块，每个块可以有很多的线程，这种分层的组织结构使得我们的并行过程更加自如灵活：</p>
<p><img src="/img/20220907141612.png" alt></p>
<p>一个线程块block中的线程可以完成下述协作：</p>
<ul>
<li>同步</li>
<li>共享内存</li>
</ul>
<p>不同块内线程不能相互影响！他们是物理隔离的！</p>
<p>接下来就是给每个线程一个编号了，我们知道每个线程都执行同样的一段串行代码，那么怎么让这段相同的代码对应不同的数据呢？首先第一步就是让这些线程彼此区分开，才能对应到相应从线程，使得这些线程也能区分自己的数据。如果线程本身没有任何标记，那么没办法确认其行为。依靠下面两个内置结构体确定线程标号：</p>
<ul>
<li>blockIdx（线程块在线程网格内的位置索引）</li>
<li>threadIdx（线程在线程块内的位置索引）</li>
</ul>
<p>注意这里的Idx是index的缩写（我之前一直以为是identity x的缩写），这两个内置结构体基于 uint3 定义，包含三个无符号整数的结构，通过三个字段来指定：</p>
<ul>
<li><code>blockIdx.x</code></li>
<li><code>blockIdx.y</code></li>
<li><code>blockIdx.z</code></li>
<li><code>threadIdx.x</code></li>
<li><code>threadIdx.y</code></li>
<li><code>threadIdx.z</code></li>
</ul>
<p>上面这两个是坐标，当然我们要有同样对应的两个结构体来保存其范围，也就是blockIdx中三个字段的范围threadIdx中三个字段的范围：</p>
<ul>
<li>blockDim</li>
<li>gridDim</li>
</ul>
<p>他们是dim3类型(基于uint3定义的数据结构)的变量，也包含三个字段x,y,z.</p>
<ul>
<li><code>blockDim.x</code></li>
<li><code>blockDim.y</code></li>
<li><code>blockDim.z</code></li>
</ul>
<p>网格和块的维度一般是二维和三维的，也就是说一个网格通常被分成二维的块，而每个块常被分成三维的线程。</p>
<p>注意：dim3是手工定义的，主机端可见。uint3是设备端在执行的时候可见的，不可以在核函数运行时修改，初始化完成后uint3值就不变了。他们是有区别的！这一点必须要注意。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">%%cu</span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">checkIndex</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">"threadIdx:(%d,%d,%d) </span></span><br><span class="line"><span class="string">          blockIdx:(%d,%d,%d) </span></span><br><span class="line"><span class="string">          blockDim:(%d,%d,%d)</span></span><br><span class="line"><span class="string">          gridDim(%d,%d,%d)\n"</span>,</span><br><span class="line">                       threadIdx.x,threadIdx.y,threadIdx.z,</span><br><span class="line">                       blockIdx.x,blockIdx.y,blockIdx.z,</span><br><span class="line">                       blockDim.x,blockDim.y,blockDim.z,</span><br><span class="line">                       gridDim.x,gridDim.y,gridDim.z);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc,<span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">int</span> nElem=<span class="number">6</span>;</span><br><span class="line">  <span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">3</span>)</span></span>;</span><br><span class="line">  <span class="function">dim3 <span class="title">grid</span><span class="params">((nElem+block.x<span class="number">-1</span>)/block.x)</span></span>;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">"grid.x %d grid.y %d grid.z %d\n"</span>,grid.x,grid.y,grid.z);</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">"block.x %d block.y %d block.z %d\n"</span>,block.x,block.y,block.z);</span><br><span class="line">  checkIndex&lt;&lt;&lt;grid,block&gt;&gt;&gt;();</span><br><span class="line">  cudaDeviceReset();</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>输出：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">grid.x 2 grid.y 1 grid.z 1</span><br><span class="line">block.x 3 block.y 1 block.z 1</span><br><span class="line">threadIdx:(0,0,0) blockIdx:(0,0,0) blockDim:(3,1,1)  gridDim(2,1,1)</span><br><span class="line">threadIdx:(1,0,0) blockIdx:(0,0,0) blockDim:(3,1,1)  gridDim(2,1,1)</span><br><span class="line">threadIdx:(2,0,0) blockIdx:(0,0,0) blockDim:(3,1,1)  gridDim(2,1,1)</span><br><span class="line">threadIdx:(0,0,0) blockIdx:(1,0,0) blockDim:(3,1,1)  gridDim(2,1,1)</span><br><span class="line">threadIdx:(1,0,0) blockIdx:(1,0,0) blockDim:(3,1,1)  gridDim(2,1,1)</span><br><span class="line">threadIdx:(2,0,0) blockIdx:(1,0,0) blockDim:(3,1,1)  gridDim(2,1,1)</span><br></pre></td></tr></table></figure></p>
<h2 id="核函数概述"><a href="#核函数概述" class="headerlink" title="核函数概述"></a>核函数概述</h2><p>核函数就是在CUDA模型上诸多线程中运行的那段串行代码，这段代码在设备上运行，用NVCC编译，产生的机器码是GPU的机器码，所以我们写CUDA程序就是写核函数。</p>
<p>启动核函数，通过的以下的ANSI C 扩展出的CUDA C指令：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kernel_name&lt;&lt;&lt;grid,block&gt;&gt;&gt;(argument <span class="built_in">list</span>);</span><br></pre></td></tr></table></figure></p>
<p>其标准C的原型就是C语言函数调用<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">function_name(argument <span class="built_in">list</span>);</span><br></pre></td></tr></table></figure></p>
<p>这个三个尖括号<code>&lt;&lt;&lt;grid,block&gt;&gt;&gt;</code>内是对设备代码执行的线程结构的配置（或者简称为对内核进行配置），也就是我们上一篇中提到的线程结构中的网格，块。回忆一下上文，我们通过CUDA C内置的数据类型dim3类型的变量来配置grid和block。通过指定grid和block的维度，我们可以配置：</p>
<ul>
<li>内核中线程的数目</li>
<li>内核中使用的线程布局</li>
</ul>
<p>我们可以使用dim3类型的grid维度和block维度配置内核，也可以使用int类型的变量，或者常量直接初始化：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kernel_name&lt;&lt;&lt;4,8&gt;&gt;&gt;(argument list);</span><br></pre></td></tr></table></figure></p>
<p><img src="/img/20220907141613.png" alt></p>
<p>我们的核函数是同时复制到多个线程执行的，上文我们说过一个对应问题，多个计算执行在一个数据，肯定是浪费时间，所以为了让多线程按照我们的意愿对应到不同的数据，就要给线程一个唯一的标识，由于设备内存是线性的（基本市面上的内存硬件都是线性形式存储数据的）我们观察上图，可以用threadIdx.x 和blockIdx.x 来组合获得对应的线程的唯一标识</p>
<p>接下来我们就是修改代码的时间了，改变核函数的配置，产生运行出结果一样，但效率不同的代码：</p>
<p>一个块：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kernel_name&lt;&lt;&lt;<span class="number">1</span>,<span class="number">32</span>&gt;&gt;&gt;(argument <span class="built_in">list</span>);</span><br></pre></td></tr></table></figure></p>
<p>32个块<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kernel_name&lt;&lt;&lt;<span class="number">32</span>,<span class="number">1</span>&gt;&gt;&gt;(argument <span class="built_in">list</span>);</span><br></pre></td></tr></table></figure></p>
<p>上述代码如果没有特殊结构在核函数中，执行结果应该一致，但是有些效率会一直比较低。</p>
<p>上面这些是启动部分，当主机启动了核函数，控制权马上回到主机，而不是主机等待设备完成核函数的运行，这一点我们上一篇文章也有提到过（就是等待hello world输出的那段代码后面要加一句）</p>
<p>想要主机等待设备端执行可以用下面这个指令：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaDeviceSynchronize</span><span class="params">(<span class="keyword">void</span>)</span></span>;</span><br></pre></td></tr></table></figure></p>
<p>这是一个显示的方法，对应的也有隐式方法，隐式方法就是不明确说明主机要等待设备端，而是设备端不执行完，主机没办法进行，比如内存拷贝函数：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaMemcpy</span><span class="params">(<span class="keyword">void</span>* dst,<span class="keyword">const</span> <span class="keyword">void</span> * src,</span></span></span><br><span class="line"><span class="function"><span class="params">  <span class="keyword">size_t</span> count,cudaMemcpyKind kind)</span></span>;</span><br></pre></td></tr></table></figure></p>
<p>这个函数上文已经介绍过了，当核函数启动后的下一条指令就是从设备复制数据回主机端，那么主机端必须要等待设备端计算完成。</p>
<p><strong>所有CUDA核函数的启动都是异步的</strong>，这点与C语言是完全不同的</p>
<h2 id="编写核函数"><a href="#编写核函数" class="headerlink" title="编写核函数"></a>编写核函数</h2><p>我们会启动核函数了，但是核函数哪里来的？当然我们写的，核函数也是一个函数，但是声明核函数有一个比较模板化的方法：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">kernel_name</span><span class="params">(argument <span class="built_in">list</span>)</span></span>;</span><br></pre></td></tr></table></figure></p>
<p>注意：声明和定义是不同的，这点CUDA与C语言是一致的</p>
<p>在C语言函数前没有的限定符global，CUDA C中还有一些其他我们在C中没有的限定符，如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>限定符</th>
<th>执行</th>
<th>调用</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>global</strong></td>
<td>设备端执行</td>
<td>可以从主机调用也可以从计算能力3以上的设备调用</td>
<td>必须有一个void的返回类型</td>
</tr>
<tr>
<td><strong>device</strong></td>
<td>设备端执行</td>
<td>设备端调用</td>
</tr>
<tr>
<td><strong>host</strong></td>
<td>主机端执行</td>
<td>主机调用</td>
<td>可以省略</td>
</tr>
</tbody>
</table>
</div>
<p>而且这里有个特殊的情况就是有些函数可以同时定义为 device 和 host ，这种函数可以同时被设备和主机端的代码调用，主机端代码调用函数很正常，设备端调用函数与C语言一致，但是要声明成设备端代码，告诉nvcc编译成设备机器码，同时声明主机端设备端函数，那么就要告诉编译器，生成两份不同设备的机器码。</p>
<p>Kernel核函数编写有以下限制</p>
<ul>
<li>只能访问设备内存</li>
<li>必须有void返回类型</li>
<li>不支持可变数量的参数</li>
<li>不支持静态变量</li>
<li>显示异步行为</li>
</ul>
<p>并行程序中经常的一种现象：把串行代码并行化时对串行代码块for的操作，也就是把for并行化。例如：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">sumArraysOnGPU</span><span class="params">(<span class="keyword">float</span> *A, <span class="keyword">float</span> *B, <span class="keyword">float</span> *C)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> i = threadIdx.x;</span><br><span class="line">  C[i] = A[i] + B[i];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这两个简单的段不能执行，但是我们可以大致的看一下for展开并行化的样子。</p>
<h2 id="验证核函数"><a href="#验证核函数" class="headerlink" title="验证核函数"></a>验证核函数</h2><p>验证核函数就是验证其正确性，下面这段代码上文出现过，但是同样包含验证核函数的方法：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 3_sum_arrays</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">sumArrays</span><span class="params">(<span class="keyword">float</span> * a,<span class="keyword">float</span> * b,<span class="keyword">float</span> * res,<span class="keyword">const</span> <span class="keyword">int</span> size)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;size;i+=<span class="number">4</span>)</span><br><span class="line">  &#123;</span><br><span class="line">    res[i]=a[i]+b[i];</span><br><span class="line">    res[i+<span class="number">1</span>]=a[i+<span class="number">1</span>]+b[i+<span class="number">1</span>];</span><br><span class="line">    res[i+<span class="number">2</span>]=a[i+<span class="number">2</span>]+b[i+<span class="number">2</span>];</span><br><span class="line">    res[i+<span class="number">3</span>]=a[i+<span class="number">3</span>]+b[i+<span class="number">3</span>];</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">sumArraysGPU</span><span class="params">(<span class="keyword">float</span>*a,<span class="keyword">float</span>*b,<span class="keyword">float</span>*res)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">int</span> i=threadIdx.x;</span><br><span class="line">  res[i]=a[i]+b[i];</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc,<span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">int</span> dev = <span class="number">0</span>;</span><br><span class="line">  cudaSetDevice(dev);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span> nElem=<span class="number">32</span>;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">"Vector size:%d\n"</span>,nElem);</span><br><span class="line">  <span class="keyword">int</span> nByte=<span class="keyword">sizeof</span>(<span class="keyword">float</span>)*nElem;</span><br><span class="line">  <span class="keyword">float</span> *a_h=(<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="keyword">float</span> *b_h=(<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="keyword">float</span> *res_h=(<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="keyword">float</span> *res_from_gpu_h=(<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="built_in">memset</span>(res_h,<span class="number">0</span>,nByte);</span><br><span class="line">  <span class="built_in">memset</span>(res_from_gpu_h,<span class="number">0</span>,nByte);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">float</span> *a_d,*b_d,*res_d;</span><br><span class="line">  CHECK(cudaMalloc((<span class="keyword">float</span>**)&amp;a_d,nByte));</span><br><span class="line">  CHECK(cudaMalloc((<span class="keyword">float</span>**)&amp;b_d,nByte));</span><br><span class="line">  CHECK(cudaMalloc((<span class="keyword">float</span>**)&amp;res_d,nByte));</span><br><span class="line"></span><br><span class="line">  initialData(a_h,nElem);</span><br><span class="line">  initialData(b_h,nElem);</span><br><span class="line"></span><br><span class="line">  CHECK(cudaMemcpy(a_d,a_h,nByte,cudaMemcpyHostToDevice));</span><br><span class="line">  CHECK(cudaMemcpy(b_d,b_h,nByte,cudaMemcpyHostToDevice));</span><br><span class="line"></span><br><span class="line">  <span class="function">dim3 <span class="title">block</span><span class="params">(nElem)</span></span>;</span><br><span class="line">  <span class="function">dim3 <span class="title">grid</span><span class="params">(nElem/block.x)</span></span>;</span><br><span class="line">  sumArraysGPU&lt;&lt;&lt;grid,block&gt;&gt;&gt;(a_d,b_d,res_d);</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">"Execution configuration&lt;&lt;&lt;%d,%d&gt;&gt;&gt;\n"</span>,block.x,grid.x);</span><br><span class="line"></span><br><span class="line">  CHECK(cudaMemcpy(res_from_gpu_h,res_d,nByte,cudaMemcpyDeviceToHost));</span><br><span class="line">  sumArrays(a_h,b_h,res_h,nElem);</span><br><span class="line"></span><br><span class="line">  checkResult(res_h,res_from_gpu_h,nElem);</span><br><span class="line">  cudaFree(a_d);</span><br><span class="line">  cudaFree(b_d);</span><br><span class="line">  cudaFree(res_d);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">free</span>(a_h);</span><br><span class="line">  <span class="built_in">free</span>(b_h);</span><br><span class="line">  <span class="built_in">free</span>(res_h);</span><br><span class="line">  <span class="built_in">free</span>(res_from_gpu_h);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>CUDA小技巧，当我们进行调试的时候可以把核函数配置成单线程的：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kernel_name&lt;&lt;&lt;<span class="number">1</span>,<span class="number">1</span>&gt;&gt;&gt;(argument <span class="built_in">list</span>)</span><br></pre></td></tr></table></figure></p>
<p>当错误出现的时候，不一定是哪一条指令触发的，这一点非常头疼；这时候我们就需要对错误进行防御性处理了，例如我们代码库头文件里面的这个宏：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CHECK(call)\</span></span><br><span class="line">&#123;\</span><br><span class="line">  <span class="keyword">const</span> cudaError_t error=call;\</span><br><span class="line">  <span class="keyword">if</span>(error!=cudaSuccess)\</span><br><span class="line">  &#123;\</span><br><span class="line">      <span class="built_in">printf</span>(<span class="string">"ERROR: %s:%d,"</span>,__FILE__,__LINE__);\</span><br><span class="line">      <span class="built_in">printf</span>(<span class="string">"code:%d,reason:%s\n"</span>,error,cudaGetErrorString(error));\</span><br><span class="line">      <span class="built_in">exit</span>(<span class="number">1</span>);\</span><br><span class="line">  &#125;\</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>就是获得每个函数执行后的返回结果，然后对不成功的信息加以处理，CUDA C 的API每个调用都会返回一个错误代码，这个代码我们就可以好好利用了，当然在release版本中可以去除这部分，但是开发的时候一定要有的。</p>
<h1 id="给核函数计时"><a href="#给核函数计时" class="headerlink" title="给核函数计时"></a>给核函数计时</h1><p>gettimeofday是linux下的一个库函数，创建一个cpu计时器，从1970年1月1日0点以来到现在的秒数，需要头文件<code>sys/time.h</code>。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"freshman.h"</span></span></span><br><span class="line"></span><br><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">sumArraysGPU</span><span class="params">(<span class="keyword">float</span>*a,<span class="keyword">float</span>*b,<span class="keyword">float</span>*res,<span class="keyword">int</span> N)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">int</span> i=blockIdx.x*blockDim.x+threadIdx.x;</span><br><span class="line">  <span class="keyword">if</span>(i &lt; N)</span><br><span class="line">    res[i]=a[i]+b[i];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc,<span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="comment">// set up device.....</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// init data ......</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">//timer</span></span><br><span class="line">  <span class="keyword">double</span> iStart,iElaps;</span><br><span class="line">  iStart=cpuSecond();</span><br><span class="line">  sumArraysGPU&lt;&lt;&lt;grid,block&gt;&gt;&gt;(a_d,b_d,res_d,nElem);</span><br><span class="line">  cudaDeviceSynchronize();</span><br><span class="line">  iElaps=cpuSecond()-iStart;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// ......</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>主要分析计时这段，首先iStart是cpuSecond返回一个秒数，接着执行核函数，核函数开始执行后马上返回主机线程，所以我们必须要加一个同步函数等待核函数执行完毕，如果不加这个同步函数，那么测试的时间是从调用核函数，到核函数返回给主机线程的时间段，而不是核函数的执行时间，加上了<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaDeviceSynchronize();</span><br></pre></td></tr></table></figure></p>
<p>函数后，计时是从调用核函数开始，到核函数执行完并返回给主机的时间段，下面图大致描述了执行过程的不同时间节点：<br><img src="/img/20220907141614.png" alt></p>
<p>我们可以大概分析下核函数启动到结束的过程：</p>
<ul>
<li>主机线程启动核函数</li>
<li>核函数启动成功</li>
<li>控制返回主机线程</li>
<li>核函数执行完成</li>
<li>主机同步函数侦测到核函数执行完</li>
</ul>
<p>我们要测试的是2~4的时间，但是用CPU计时方法，只能测试1~5的时间，所以测试得到的时间偏长。</p>
<h2 id="用nvprof计时"><a href="#用nvprof计时" class="headerlink" title="用nvprof计时"></a>用nvprof计时</h2><p>CUDA 5.0后有一个工具叫做nvprof的命令行分析工具，后面还要介绍一个图形化的工具，现在我们来学习一下nvprof，学习工具主要技巧是学习工具的功能，当你掌握了一个工具的全部功能，那就是学习成功了。<br>nvprof的用法如下：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ nvprof [nvprof_args] &lt;application&gt;[application_args]</span><br></pre></td></tr></table></figure></p>
<p>工具不仅给出了kernel执行的时间，比例，还有其他cuda函数的执行时间，可以看出核函数执行时间只有4%左右，其他内存分配，内存拷贝占了大部分事件。</p>
<h1 id="组织并行线程"><a href="#组织并行线程" class="headerlink" title="组织并行线程"></a>组织并行线程</h1><h2 id="使用块和线程建立矩阵索引"><a href="#使用块和线程建立矩阵索引" class="headerlink" title="使用块和线程建立矩阵索引"></a>使用块和线程建立矩阵索引</h2><p>多线程的优点就是每个线程处理不同的数据计算，那么怎么分配好每个线程处理不同的数据，而不至于多个不同的线程处理同一个数据。下图可以非常形象的反应线程模型：<br><img src="/img/20220907141615.png" alt></p>
<p>这里(ix,iy)就是整个线程模型中任意一个线程的索引，或者叫做全局地址，局部地址当然就是(threadIdx.x,threadIdx.y)了，当然这个局部地址目前还没有什么用处，他只能索引线程块内的线程，不同线程块中有相同的局部索引值，比如同一个小区，A栋有16楼，B栋也有16楼，A栋和B栋就是blockIdx，而16就是threadIdx啦。图中的横坐标就是：<code>ix=threadIdx.x+blockIdx.x×blockDim.x</code>，纵坐标是：<code>iy=threadIdx.y+blockIdx.y×blockDim.y</code></p>
<p>这样我们就得到了每个线程的唯一标号，并且在运行时kernel是可以访问这个标号的。前面讲过CUDA每一个线程执行相同的代码，也就是异构计算中说的多线程单指令，如果每个不同的线程执行同样的代码，又处理同一组数据，将会得到多个相同的结果，显然这是没意义的，为了让不同线程处理不同的数据，CUDA常用的做法是让不同的线程对应不同的数据，也就是用线程的全局标号对应不同组的数据。</p>
<p>设备内存或者主机内存都是线性存在的，我们要做管理的就是：</p>
<ul>
<li>线程和块索引（来计算线程的全局索引）</li>
<li>矩阵中给定点的坐标（ix,iy）</li>
<li>(ix,iy)对应的线性内存的位置</li>
</ul>
<p>线性位置的计算方法是：<code>idx=ix+iy∗nx</code></p>
<p>我们上面已经计算出了线程的全局坐标，用线程的全局坐标对应矩阵的坐标，也就是说，线程的坐标(ix,iy)对应矩阵中(ix,iy)的元素，这样就形成了一一对应，不同的线程处理矩阵中不同的数据，举个具体的例子，ix=10,iy=10的线程去处理矩阵中(10,10)的数据，当然你也可以设计别的对应模式，但是这种方法是最简单出错可能最低的。我们接下来的代码来输出每个线程的标号信息：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">printThreadIndex</span><span class="params">(<span class="keyword">float</span> *A,<span class="keyword">const</span> <span class="keyword">int</span> nx,<span class="keyword">const</span> <span class="keyword">int</span> ny)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">int</span> ix=threadIdx.x+blockIdx.x*blockDim.x;</span><br><span class="line">  <span class="keyword">int</span> iy=threadIdx.y+blockIdx.y*blockDim.y;</span><br><span class="line">  <span class="keyword">unsigned</span> <span class="keyword">int</span> idx=iy*nx+ix;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">"thread_id(%d,%d) block_id(%d,%d) coordinate(%d,%d)"</span></span><br><span class="line">          <span class="string">"global index %2d ival %2d\n"</span>,threadIdx.x,threadIdx.y,</span><br><span class="line">          blockIdx.x,blockIdx.y,ix,iy,idx,A[idx]);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc,<span class="keyword">char</span>** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  initDevice(<span class="number">0</span>);</span><br><span class="line">  <span class="keyword">int</span> nx=<span class="number">8</span>,ny=<span class="number">6</span>;</span><br><span class="line">  <span class="keyword">int</span> nxy=nx*ny;</span><br><span class="line">  <span class="keyword">int</span> nBytes=nxy*<span class="keyword">sizeof</span>(<span class="keyword">float</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">//Malloc</span></span><br><span class="line">  <span class="keyword">float</span>* A_host=(<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line">  initialData(A_host,nxy);</span><br><span class="line">  printMatrix(A_host,nx,ny);</span><br><span class="line"></span><br><span class="line">  <span class="comment">//cudaMalloc</span></span><br><span class="line">  <span class="keyword">float</span> *A_dev=<span class="literal">NULL</span>;</span><br><span class="line">  CHECK(cudaMalloc((<span class="keyword">void</span>**)&amp;A_dev,nBytes));</span><br><span class="line"></span><br><span class="line">  cudaMemcpy(A_dev,A_host,nBytes,cudaMemcpyHostToDevice);</span><br><span class="line"></span><br><span class="line">  <span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">4</span>,<span class="number">2</span>)</span></span>;</span><br><span class="line">  <span class="function">dim3 <span class="title">grid</span><span class="params">((nx<span class="number">-1</span>)/block.x+<span class="number">1</span>,(ny<span class="number">-1</span>)/block.y+<span class="number">1</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">  printThreadIndex&lt;&lt;&lt;grid,block&gt;&gt;&gt;(A_dev,nx,ny);</span><br><span class="line"></span><br><span class="line">  CHECK(cudaDeviceSynchronize());</span><br><span class="line">  cudaFree(A_dev);</span><br><span class="line">  <span class="built_in">free</span>(A_host);</span><br><span class="line"></span><br><span class="line">  cudaDeviceReset();</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="二维矩阵加法"><a href="#二维矩阵加法" class="headerlink" title="二维矩阵加法"></a>二维矩阵加法</h2><p>我们利用上面的线程与数据的对应完成了下面的核函数：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">sumMatrix</span><span class="params">(<span class="keyword">float</span> * MatA,<span class="keyword">float</span> * MatB,<span class="keyword">float</span> * MatC,<span class="keyword">int</span> nx,<span class="keyword">int</span> ny)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> ix=threadIdx.x+blockDim.x*blockIdx.x;</span><br><span class="line">    <span class="keyword">int</span> iy=threadIdx.y+blockDim.y*blockIdx.y;</span><br><span class="line">    <span class="keyword">int</span> idx=ix+iy*ny;</span><br><span class="line">    <span class="keyword">if</span> (ix&lt;nx &amp;&amp; iy&lt;ny)</span><br><span class="line">    &#123;</span><br><span class="line">      MatC[idx]=MatA[idx]+MatB[idx];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="二维网格和二维块"><a href="#二维网格和二维块" class="headerlink" title="二维网格和二维块"></a>二维网格和二维块</h2><p>首先来看二维网格二维模块的代码：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 2d block and 2d grid</span></span><br><span class="line"><span class="function">dim3 <span class="title">block_0</span><span class="params">(dimx,dimy)</span></span>;</span><br><span class="line"><span class="function">dim3 <span class="title">grid_0</span><span class="params">((nx<span class="number">-1</span>)/block_0.x+<span class="number">1</span>,(ny<span class="number">-1</span>)/block_0.y+<span class="number">1</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">iStart=cpuSecond();</span><br><span class="line"></span><br><span class="line">sumMatrix&lt;&lt;&lt;grid_0,block_0&gt;&gt;&gt;(A_dev,B_dev,C_dev,nx,ny);</span><br><span class="line">CHECK(cudaDeviceSynchronize());</span><br><span class="line"></span><br><span class="line">iElaps=cpuSecond()-iStart;</span><br><span class="line"></span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"GPU Execution configuration&lt;&lt;&lt;(%d,%d),(%d,%d)&gt;&gt;&gt; Time elapsed %f sec\n"</span>,</span><br><span class="line">      grid_0.x,grid_0.y,block_0.x,block_0.y,iElaps);</span><br><span class="line"></span><br><span class="line">CHECK(cudaMemcpy(C_from_gpu,C_dev,nBytes,cudaMemcpyDeviceToHost));</span><br><span class="line">checkResult(C_host,C_from_gpu,nxy);</span><br></pre></td></tr></table></figure></p>
<h2 id="一维网格和一维块"><a href="#一维网格和一维块" class="headerlink" title="一维网格和一维块"></a>一维网格和一维块</h2><p>接着我们使用一维网格一维块：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1d block and 1d grid</span></span><br><span class="line">dimx=<span class="number">32</span>;</span><br><span class="line"><span class="function">dim3 <span class="title">block_1</span><span class="params">(dimx)</span></span>;</span><br><span class="line"><span class="function">dim3 <span class="title">grid_1</span><span class="params">((nxy<span class="number">-1</span>)/block_1.x+<span class="number">1</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">iStart=cpuSecond();</span><br><span class="line">sumMatrix&lt;&lt;&lt;grid_1,block_1&gt;&gt;&gt;(A_dev,B_dev,C_dev,nx*ny ,<span class="number">1</span>);</span><br><span class="line">CHECK(cudaDeviceSynchronize());</span><br><span class="line">iElaps=cpuSecond()-iStart;</span><br><span class="line"></span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"GPU Execution configuration&lt;&lt;&lt;(%d,%d),(%d,%d)&gt;&gt;&gt; Time elapsed %f sec\n"</span>,</span><br><span class="line">      grid_1.x,grid_1.y,block_1.x,block_1.y,iElaps);</span><br><span class="line"></span><br><span class="line">CHECK(cudaMemcpy(C_from_gpu,C_dev,nBytes,cudaMemcpyDeviceToHost));</span><br><span class="line">checkResult(C_host,C_from_gpu,nxy);</span><br></pre></td></tr></table></figure></p>
<h1 id="GPU设备信息"><a href="#GPU设备信息" class="headerlink" title="GPU设备信息"></a>GPU设备信息</h1><p>在软件内查询信息，用到如下代码：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc,<span class="keyword">char</span>** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%s Starting ...\n"</span>,argv[<span class="number">0</span>]);</span><br><span class="line">    <span class="keyword">int</span> deviceCount = <span class="number">0</span>;</span><br><span class="line">    cudaError_t error_id = cudaGetDeviceCount(&amp;deviceCount);</span><br><span class="line">    <span class="keyword">if</span>(error_id!=cudaSuccess)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"cudaGetDeviceCount returned %d\n -&gt;%s\n"</span>,</span><br><span class="line">              (<span class="keyword">int</span>)error_id,cudaGetErrorString(error_id));</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Result = FAIL\n"</span>);</span><br><span class="line">        <span class="built_in">exit</span>(EXIT_FAILURE);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(deviceCount==<span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"There are no available device(s) that support CUDA\n"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Detected %d CUDA Capable device(s)\n"</span>,deviceCount);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">int</span> dev=<span class="number">0</span>,driverVersion=<span class="number">0</span>,runtimeVersion=<span class="number">0</span>;</span><br><span class="line">    cudaSetDevice(dev);</span><br><span class="line">    cudaDeviceProp deviceProp;</span><br><span class="line">    cudaGetDeviceProperties(&amp;deviceProp,dev);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Device %d:\"%s\"\n"</span>,dev,deviceProp.name);</span><br><span class="line">    cudaDriverGetVersion(&amp;driverVersion);</span><br><span class="line">    cudaRuntimeGetVersion(&amp;runtimeVersion);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"  CUDA Driver Version / Runtime Version         %d.%d  /  %d.%d\n"</span>,</span><br><span class="line">        driverVersion/<span class="number">1000</span>,(driverVersion%<span class="number">100</span>)/<span class="number">10</span>,</span><br><span class="line">        runtimeVersion/<span class="number">1000</span>,(runtimeVersion%<span class="number">100</span>)/<span class="number">10</span>);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"  CUDA Capability Major/Minor version number:   %d.%d\n"</span>,</span><br><span class="line">        deviceProp.major,deviceProp.minor);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"  Total amount of global memory:                %.2f MBytes (%llu bytes)\n"</span>,</span><br><span class="line">            (<span class="keyword">float</span>)deviceProp.totalGlobalMem/<span class="built_in">pow</span>(<span class="number">1024.0</span>,<span class="number">3</span>));</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"  GPU Clock rate:                               %.0f MHz (%0.2f GHz)\n"</span>,</span><br><span class="line">            deviceProp.clockRate*<span class="number">1e-3</span>f,deviceProp.clockRate*<span class="number">1e-6</span>f);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"  Memory Bus width:                             %d-bits\n"</span>,</span><br><span class="line">            deviceProp.memoryBusWidth);</span><br><span class="line">    <span class="keyword">if</span> (deviceProp.l2CacheSize)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"  L2 Cache Size:                            	%d bytes\n"</span>,</span><br><span class="line">                deviceProp.l2CacheSize);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"  Max Texture Dimension Size (x,y,z)            1D=(%d),2D=(%d,%d),3D=(%d,%d,%d)\n"</span>,</span><br><span class="line">            deviceProp.maxTexture1D,deviceProp.maxTexture2D[<span class="number">0</span>],deviceProp.maxTexture2D[<span class="number">1</span>]</span><br><span class="line">            ,deviceProp.maxTexture3D[<span class="number">0</span>],deviceProp.maxTexture3D[<span class="number">1</span>],deviceProp.maxTexture3D[<span class="number">2</span>]);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"  Max Layered Texture Size (dim) x layers       1D=(%d) x %d,2D=(%d,%d) x %d\n"</span>,</span><br><span class="line">            deviceProp.maxTexture1DLayered[<span class="number">0</span>],deviceProp.maxTexture1DLayered[<span class="number">1</span>],</span><br><span class="line">            deviceProp.maxTexture2DLayered[<span class="number">0</span>],deviceProp.maxTexture2DLayered[<span class="number">1</span>],</span><br><span class="line">            deviceProp.maxTexture2DLayered[<span class="number">2</span>]);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"  Total amount of constant memory               %lu bytes\n"</span>,</span><br><span class="line">            deviceProp.totalConstMem);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"  Total amount of shared memory per block:      %lu bytes\n"</span>,</span><br><span class="line">            deviceProp.sharedMemPerBlock);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"  Total number of registers available per block:%d\n"</span>,</span><br><span class="line">            deviceProp.regsPerBlock);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"  Wrap size:                                    %d\n"</span>,deviceProp.warpSize);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"  Maximun number of thread per multiprocesser:  %d\n"</span>,</span><br><span class="line">            deviceProp.maxThreadsPerMultiProcessor);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"  Maximun number of thread per block:           %d\n"</span>,</span><br><span class="line">            deviceProp.maxThreadsPerBlock);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"  Maximun size of each dimension of a block:    %d x %d x %d\n"</span>,</span><br><span class="line">            deviceProp.maxThreadsDim[<span class="number">0</span>],deviceProp.maxThreadsDim[<span class="number">1</span>],deviceProp.maxThreadsDim[<span class="number">2</span>]);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"  Maximun size of each dimension of a grid:     %d x %d x %d\n"</span>,</span><br><span class="line">            deviceProp.maxGridSize[<span class="number">0</span>],</span><br><span class="line">            deviceProp.maxGridSize[<span class="number">1</span>],</span><br><span class="line">            deviceProp.maxGridSize[<span class="number">2</span>]);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"  Maximu memory pitch                           %lu bytes\n"</span>,deviceProp.memPitch);</span><br><span class="line">    <span class="built_in">exit</span>(EXIT_SUCCESS);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>输出：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">Detected 1 CUDA Capable device(s)</span><br><span class="line">Device 0:&quot;Tesla T4&quot;</span><br><span class="line">  CUDA Driver Version / Runtime Version         11.2  /  11.1</span><br><span class="line">  CUDA Capability Major/Minor version number:   7.5</span><br><span class="line">  Total amount of global memory:                14.76 MBytes (140518271855200 bytes)</span><br><span class="line">  GPU Clock rate:                               1590 MHz (1.59 GHz)</span><br><span class="line">  Memory Bus width:                             256-bits</span><br><span class="line">  L2 Cache Size:                                4194304 bytes</span><br><span class="line">  Max Texture Dimension Size (x,y,z)            1D=(131072),2D=(131072,65536),3D=(16384,16384,16384)</span><br><span class="line">  Max Layered Texture Size (dim) x layers       1D=(32768) x 2048,2D=(32768,32768) x 2048</span><br><span class="line">  Total amount of constant memory               65536 bytes</span><br><span class="line">  Total amount of shared memory per block:      49152 bytes</span><br><span class="line">  Total number of registers available per block:65536</span><br><span class="line">  Wrap size:                                    32</span><br><span class="line">  Maximun number of thread per multiprocesser:  1024</span><br><span class="line">  Maximun number of thread per block:           1024</span><br><span class="line">  Maximun size of each dimension of a block:    1024 x 1024 x 64</span><br><span class="line">  Maximun size of each dimension of a grid:     2147483647 x 65535 x 65535</span><br><span class="line">  Maximu memory pitch                           2147483647 bytes</span><br></pre></td></tr></table></figure></p>
<p>这里面很多参数是我们后面要介绍的，而且每一个都对性能有影响：</p>
<ul>
<li>CUDA驱动版本</li>
<li>设备计算能力编号</li>
<li>全局内存大小（1.95G,原文有错误，写成MBytes了）</li>
<li>GPU主频</li>
<li>GPU带宽</li>
<li>L2缓存大小</li>
<li>纹理维度最大值，不同维度下的</li>
<li>层叠纹理维度最大值</li>
<li>常量内存大小</li>
<li>块内共享内存大小</li>
<li>块内寄存器大小</li>
<li>线程束大小</li>
<li>每个处理器硬件处理的最大线程数</li>
<li>每个块处理的最大线程数</li>
<li>块的最大尺寸</li>
<li>网格的最大尺寸</li>
<li>最大连续线性内存</li>
</ul>
<h1 id="CUDA执行模型概述"><a href="#CUDA执行模型概述" class="headerlink" title="CUDA执行模型概述"></a>CUDA执行模型概述</h1><p>CUDA执行模型揭示了GPU并行架构的抽象视图，再设计硬件的时候，其功能和特性都已经被设计好了，然后去开发硬件，如果这个过程模型特性或功能与硬件设计有冲突，双方就会进行商讨妥协，知道最后产品定型量产，功能和特性算是全部定型，而这些功能和特性就是变成模型的设计基础，而编程模型又直接反应了硬件设计，从而反映了设备的硬件特性。</p>
<p>比如最直观的一个就是内存，线程的层次结构帮助我们控制大规模并行，这个特性就是硬件设计最初设计好，然后集成电路工程师拿去设计，定型后程序员开发驱动，然后在上层可以直接使用这种执行模型来控制硬件。<br>所以了解CUDA的执行模型，可以帮助我们优化指令吞吐量，和内存使用来获得极限速度。</p>
<h2 id="GPU架构概述"><a href="#GPU架构概述" class="headerlink" title="GPU架构概述"></a>GPU架构概述</h2><p>GPU架构是围绕一个流式多处理器（SM）的扩展阵列搭建的。通过复制这种结构来实现GPU的硬件并行。</p>
<p><img src="/img/20220907141616.png" alt></p>
<p>上图包括关键组件：</p>
<ul>
<li>CUDA核心</li>
<li>共享内存/一级缓存</li>
<li>寄存器文件</li>
<li>加载/存储单元</li>
<li>特殊功能单元</li>
<li>线程束调度器</li>
</ul>
<p>GPU中每个SM都能支持数百个线程并发执行，每个GPU通常有多个SM，当一个核函数的网格被启动的时候，多个block会被同时分配给可用的SM上执行。</p>
<blockquote>
<p>注意: 当一个blcok被分配给一个SM后，他就只能在这个SM上执行了，不可能重新分配到其他SM上了，多个线程块可以被分配到同一个SM上。</p>
</blockquote>
<p>在SM上同一个块内的多个线程进行线程级别并行，而同一线程内，指令利用指令级并行将单个线程处理成流水线。</p>
<h3 id="线程束"><a href="#线程束" class="headerlink" title="线程束"></a>线程束</h3><p>CUDA 采用单指令多线程SIMT架构管理执行线程，不同设备有不同的线程束大小，但是到目前为止基本所有设备都是维持在32，也就是说每个SM上有多个block，一个block有多个线程（可以是几百个，但不会超过某个最大值），但是从机器的角度，在某时刻T，SM上只执行一个线程束，也就是32个线程在同时同步执行，线程束中的每个线程执行同一条指令，包括有分支的部分，这个我们后面会讲到，</p>
<h3 id="SIMD-vs-SIMT"><a href="#SIMD-vs-SIMT" class="headerlink" title="SIMD vs SIMT"></a>SIMD vs SIMT</h3><p>单指令多数据的执行属于向量机，比如我们有四个数字要加上四个数字，那么我们可以用这种单指令多数据的指令来一次完成本来要做四次的运算。这种机制的问题就是过于死板，不允许每个分支有不同的操作，所有分支必须同时执行相同的指令，必须执行没有例外。</p>
<p>相比之下单指令多线程SIMT就更加灵活了，虽然两者都是将相同指令广播给多个执行单元，但是SIMT的某些线程可以选择不执行，也就是说同一时刻所有线程被分配给相同的指令，SIMD规定所有人必须执行，而SIMT则规定有些人可以根据需要不执行，这样SIMT就保证了线程级别的并行，而SIMD更像是指令级别的并行。</p>
<p>SIMT包括以下SIMD不具有的关键特性：</p>
<ul>
<li>每个线程都有自己的指令地址计数器</li>
<li>每个线程都有自己的寄存器状态</li>
<li>每个线程可以有一个独立的执行路径</li>
</ul>
<p>而上面这三个特性在编程模型可用的方式就是给每个线程一个唯一的标号（blckIdx,threadIdx），并且这三个特性保证了各线程之间的独立</p>
<h3 id="32"><a href="#32" class="headerlink" title="32"></a>32</h3><p>32是个神奇数字，他的产生是硬件系统设计的结果，也就是集成电路工程师搞出来的，所以软件工程师只能接受。</p>
<p>从概念上讲，32是SM以SIMD方式同时处理的工作粒度，这句话这么理解，可能学过后面的会更深刻的明白，一个SM上在某一个时刻，有32个线程在执行同一条指令，这32个线程可以选择性执行，虽然有些可以不执行，但是他也不能执行别的指令，需要另外需要执行这条指令的线程执行完</p>
<h2 id="CUDA编程的组件与逻辑"><a href="#CUDA编程的组件与逻辑" class="headerlink" title="CUDA编程的组件与逻辑"></a>CUDA编程的组件与逻辑</h2><p>下图从逻辑角度和硬件角度描述了CUDA编程模型对应的组件。</p>
<p><img src="/img/20220907141617.png" alt></p>
<p>SM中共享内存，和寄存器是关键的资源，线程块中线程通过共享内存和寄存器相互通信协调。寄存器和共享内存的分配可以严重影响性能！</p>
<p>因为SM有限，虽然我们的编程模型层面看所有线程都是并行执行的，但是在微观上看，所有线程块也是分批次的在物理层面的机器上执行，线程块里不同的线程可能进度都不一样，但是同一个线程束内的线程拥有相同的进度。</p>
<p>并行就会引起竞争，多线程以未定义的顺序访问同一个数据，就导致了不可预测的行为，CUDA只提供了一种块内同步的方式，块之间没办法同步！同一个SM上可以有不止一个常驻的线程束，有些在执行，有些在等待，他们之间状态的转换是不需要开销的。</p>
<h2 id="理解线程束执行的本质"><a href="#理解线程束执行的本质" class="headerlink" title="理解线程束执行的本质"></a>理解线程束执行的本质</h2><p>从外表来看，CUDA执行所有的线程，并行的，没有先后次序的，但实际上硬件资源是有限的，不可能同时执行百万个线程，所以从硬件角度来看，物理层面上执行的也只是线程的一部分，而每次执行的这一部分，就是我们前面提到的线程束。</p>
<p>线程束是SM中基本的执行单元，当一个网格被启动（网格被启动，等价于一个内核被启动，每个内核对应于自己的网格），网格中包含线程块，线程块被分配到某一个SM上以后，将分为多个线程束，每个线程束一般是32个线程（目前的GPU都是32个线程，但不保证未来还是32个）在一个线程束中，所有线程按照单指令多线程SIMT的方式执行，每一步执行相同的指令，但是处理的数据为私有的数据。</p>
<p>在块中，每个线程有唯一的编号（可能是个三维的编号），threadIdx。网格中，每个线程块也有唯一的编号(可能是个三维的编号)，blockIdx。那么每个线程就有在网格中的唯一编号。当一个线程块中有128个线程的时候，其分配到SM上执行时，会分成4个块：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">warp0: thread  0,........thread31</span><br><span class="line">warp1: thread 32,........thread63</span><br><span class="line">warp2: thread 64,........thread95</span><br><span class="line">warp3: thread 96,........thread127</span><br></pre></td></tr></table></figure></p>
<p>当编号使用三维编号时，x位于最内层，y位于中层，z位于最外层，想象下c语言的数组，如果把上面这句话写成c语言，假设三维数组t保存了所有的线程，那么(threadIdx.x,threadIdx.y,threadIdx.z)表示为<code>t[z][y][x];</code></p>
<p>计算出三维对应的线性地址是：<code>tid=threadIdx.x+threadIdx.y×blockDim.x+threadIdx.z×blockDim.x×blockDim.y</code>。上面的公式可以借助c语言的三维数组计算相对地址的方法</p>
<p>因为线程束分化导致的性能下降就应该用线程束的方法解决，根本思路是避免同一个线程束内的线程分化，而让我们能控制线程束内线程行为的原因是线程块中线程分配到线程束是有规律的而不是随机的。这就使得我们根据线程编号来设计分支是可以的，补充说明下，当一个线程束中所有的线程都执行if或者，都执行else时，不存在性能下降；只有当线程束内有分歧产生分支的时候，性能才会急剧下降。</p>
<p>线程束内的线程是可以被我们控制的，那么我们就把都执行if的线程塞到一个线程束中，或者让一个线程束中的线程都执行if，另外线程都执行else的这种方式可以将效率提高很多。下面这个kernel可以产生一个比较低效的分支：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">mathKernel1</span><span class="params">(<span class="keyword">float</span> *c)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> tid = blockIdx.x* blockDim.x + threadIdx.x;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">float</span> a = <span class="number">0.0</span>;</span><br><span class="line">	<span class="keyword">float</span> b = <span class="number">0.0</span>;</span><br><span class="line">	<span class="keyword">if</span> (tid % <span class="number">2</span> == <span class="number">0</span>)</span><br><span class="line">	&#123;</span><br><span class="line">		a = <span class="number">100.0f</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">else</span></span><br><span class="line">	&#123;</span><br><span class="line">		b = <span class="number">200.0f</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	c[tid] = a + b;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这种情况下我们假设只配置一个x=64的一维线程块，那么只有两个个线程束，线程束内奇数线程（threadIdx.x为奇数）会执行else，偶数线程执行if，分化很严重。</p>
<p>但是如果我们换一种方法，得到相同但是错乱的结果C，这个顺序其实是无所谓的，因为我们可以后期调整。那么下面代码就会很高效<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">mathKernel2</span><span class="params">(<span class="keyword">float</span> *c)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> tid = blockIdx.x* blockDim.x + threadIdx.x;</span><br><span class="line">	<span class="keyword">float</span> a = <span class="number">0.0</span>;</span><br><span class="line">	<span class="keyword">float</span> b = <span class="number">0.0</span>;</span><br><span class="line">	<span class="keyword">if</span> ((tid/warpSize) % <span class="number">2</span> == <span class="number">0</span>)</span><br><span class="line">	&#123;</span><br><span class="line">		a = <span class="number">100.0f</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">else</span></span><br><span class="line">	&#123;</span><br><span class="line">		b = <span class="number">200.0f</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	c[tid] = a + b;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>第一个线程束内的线程编号tid从0到31，tid/warpSize都等于0，那么就都执行if语句。第二个线程束内的线程编号tid从32到63，tid/warpSize都等于1，执行else。线程束内没有分支，效率较高。</p>
<h2 id="延迟隐藏"><a href="#延迟隐藏" class="headerlink" title="延迟隐藏"></a>延迟隐藏</h2><p>与其他类型的编程相比，GPU的延迟隐藏及其重要。对于指令的延迟，通常分为两种：</p>
<ul>
<li>算术指令</li>
<li>内存指令</li>
</ul>
<p>算数指令延迟是一个算术操作从开始，到产生结果之间的时间，这个时间段内只有某些计算单元处于工作状态，而其他逻辑计算单元处于空闲。内存指令延迟很好理解，当产生内存访问的时候，计算单元要等数据从内存拿到寄存器，这个周期是非常长的。</p>
<p>延迟：</p>
<ul>
<li>算术延迟 10~20 个时钟周期</li>
<li>内存延迟 400~800 个时钟周期</li>
</ul>
<h2 id="同步"><a href="#同步" class="headerlink" title="同步"></a>同步</h2><p>并发程序对同步非常有用，比如pthread中的锁，openmp中的同步机制，主要目的是避免内存竞争。CUDA同步这里只讲两种：</p>
<ul>
<li>线程块内同步</li>
<li>系统级别</li>
</ul>
<p>块级别的就是同一个块内的线程会同时停止在某个设定的位置，用<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__syncthread();</span><br></pre></td></tr></table></figure></p>
<p>这个函数完成，这个函数只能同步同一个块内的线程，不能同步不同块内的线程，想要同步不同块内的线程，就只能让核函数执行完成，控制程序交换主机，这种方式来同步所有线程。</p>
<p>内存竞争是非常危险的，一定要非常小心，这里经常出错。</p>
<h2 id="并行性表现"><a href="#并行性表现" class="headerlink" title="并行性表现"></a>并行性表现</h2><p>本文的主要内容就是进一步理解线程束在硬件上执行的本质过程，结合上几篇关于执行模型的学习，本文相对简单，通过修改核函数的配置，来观察核函数的执行速度，以及分析硬件利用数据，分析性能，调整核函数配置是CUDA开发人员必须掌握的技能，本篇只研究对核函数的配置是如何影响效率的（也就是通过网格，块的配置来获得不同的执行效率。）本文全文只用到下面的核函数<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">sumMatrix</span><span class="params">(<span class="keyword">float</span> * MatA,<span class="keyword">float</span> * MatB,<span class="keyword">float</span> * MatC,<span class="keyword">int</span> nx,<span class="keyword">int</span> ny)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> ix=threadIdx.x+blockDim.x*blockIdx.x;</span><br><span class="line">    <span class="keyword">int</span> iy=threadIdx.y+blockDim.y*blockIdx.y;</span><br><span class="line">    <span class="keyword">int</span> idx=ix+iy*ny;</span><br><span class="line">    <span class="keyword">if</span> (ix&lt;nx &amp;&amp; iy&lt;ny)</span><br><span class="line">    &#123;</span><br><span class="line">      MatC[idx]=MatA[idx]+MatB[idx];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>没有任何优化的最简单的二维矩阵加法。</p>
<p>全部代码：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc,<span class="keyword">char</span>** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="comment">//printf("strating...\n");</span></span><br><span class="line">  <span class="comment">//initDevice(0);</span></span><br><span class="line">  <span class="keyword">int</span> nx=<span class="number">1</span>&lt;&lt;<span class="number">13</span>;</span><br><span class="line">  <span class="keyword">int</span> ny=<span class="number">1</span>&lt;&lt;<span class="number">13</span>;</span><br><span class="line">  <span class="keyword">int</span> nxy=nx*ny;</span><br><span class="line">  <span class="keyword">int</span> nBytes=nxy*<span class="keyword">sizeof</span>(<span class="keyword">float</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">//Malloc</span></span><br><span class="line">  <span class="keyword">float</span>* A_host=(<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line">  <span class="keyword">float</span>* B_host=(<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line">  <span class="keyword">float</span>* C_host=(<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line">  <span class="keyword">float</span>* C_from_gpu=(<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line">  initialData(A_host,nxy);</span><br><span class="line">  initialData(B_host,nxy);</span><br><span class="line"></span><br><span class="line">  <span class="comment">//cudaMalloc</span></span><br><span class="line">  <span class="keyword">float</span> *A_dev=<span class="literal">NULL</span>;</span><br><span class="line">  <span class="keyword">float</span> *B_dev=<span class="literal">NULL</span>;</span><br><span class="line">  <span class="keyword">float</span> *C_dev=<span class="literal">NULL</span>;</span><br><span class="line">  CHECK(cudaMalloc((<span class="keyword">void</span>**)&amp;A_dev,nBytes));</span><br><span class="line">  CHECK(cudaMalloc((<span class="keyword">void</span>**)&amp;B_dev,nBytes));</span><br><span class="line">  CHECK(cudaMalloc((<span class="keyword">void</span>**)&amp;C_dev,nBytes));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  CHECK(cudaMemcpy(A_dev,A_host,nBytes,cudaMemcpyHostToDevice));</span><br><span class="line">  CHECK(cudaMemcpy(B_dev,B_host,nBytes,cudaMemcpyHostToDevice));</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span> dimx=argc&gt;<span class="number">2</span>?atoi(argv[<span class="number">1</span>]):<span class="number">32</span>;</span><br><span class="line">  <span class="keyword">int</span> dimy=argc&gt;<span class="number">2</span>?atoi(argv[<span class="number">2</span>]):<span class="number">32</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">double</span> iStart,iElaps;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 2d block and 2d grid</span></span><br><span class="line">  <span class="function">dim3 <span class="title">block</span><span class="params">(dimx,dimy)</span></span>;</span><br><span class="line">  <span class="function">dim3 <span class="title">grid</span><span class="params">((nx<span class="number">-1</span>)/block.x+<span class="number">1</span>,(ny<span class="number">-1</span>)/block.y+<span class="number">1</span>)</span></span>;</span><br><span class="line">  iStart=cpuSecond();</span><br><span class="line">  sumMatrix&lt;&lt;&lt;grid,block&gt;&gt;&gt;(A_dev,B_dev,C_dev,nx,ny);</span><br><span class="line">  CHECK(cudaDeviceSynchronize());</span><br><span class="line">  iElaps=cpuSecond()-iStart;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">"GPU Execution configuration&lt;&lt;&lt;(%d,%d),(%d,%d)|%f sec\n"</span>,</span><br><span class="line">        grid.x,grid.y,block.x,block.y,iElaps);</span><br><span class="line">  CHECK(cudaMemcpy(C_from_gpu,C_dev,nBytes,cudaMemcpyDeviceToHost));</span><br><span class="line"></span><br><span class="line">  cudaFree(A_dev);</span><br><span class="line">  cudaFree(B_dev);</span><br><span class="line">  cudaFree(C_dev);</span><br><span class="line">  <span class="built_in">free</span>(A_host);</span><br><span class="line">  <span class="built_in">free</span>(B_host);</span><br><span class="line">  <span class="built_in">free</span>(C_host);</span><br><span class="line">  <span class="built_in">free</span>(C_from_gpu);</span><br><span class="line">  cudaDeviceReset();</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>可见我们用两个 8192×8192 的矩阵相加来测试我们效率。</p>
<h2 id="避免分支分化"><a href="#避免分支分化" class="headerlink" title="避免分支分化"></a>避免分支分化</h2><h3 id="并行规约问题"><a href="#并行规约问题" class="headerlink" title="并行规约问题"></a>并行规约问题</h3><p>在串行编程中，我们最最最常见的一个问题就是一组特别多数字通过计算变成一个数字，比如加法，也就是求这一组数据的和，或者乘法，对应的加法或者乘法就是交换律和结合律。归约的方式基本包括如下几个步骤：</p>
<ul>
<li>将输入向量划分到更小的数据块中</li>
<li>用一个线程计算一个数据块的部分和</li>
<li>对每个数据块的部分和再求和得到最终的结果。</li>
<li>数据分块保证我们可以用一个线程块来处理一个数据块。</li>
<li>一个线程处理更小的块，所以一个线程块可以处理一个较大的块，然后多个块完成整个数据集的处理。</li>
<li>最后将所有线程块得到的结果相加，就是结果，这一步一般在cpu上完成。</li>
</ul>
<p>归约问题最常见的加法计算是把向量的数据分成对，然后用不同线程计算每一对元素，得到的结果作为输入继续分成对，迭代的进行，直到最后一个元素。成对的划分常见的方法有以下两种：</p>
<ol>
<li>相邻配对：元素与他们相邻的元素配对</li>
<li>交错配对：元素与一定距离的元素配对</li>
</ol>
<p>首先是cpu版本实现交错配对归约计算的代码：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">recursiveReduce</span><span class="params">(<span class="keyword">int</span> *data, <span class="keyword">int</span> <span class="keyword">const</span> size)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="comment">// terminate check</span></span><br><span class="line">	<span class="keyword">if</span> (size == <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> data[<span class="number">0</span>];</span><br><span class="line">	<span class="comment">// renew the stride</span></span><br><span class="line">	<span class="keyword">int</span> <span class="keyword">const</span> stride = size / <span class="number">2</span>;</span><br><span class="line">	<span class="keyword">if</span> (size % <span class="number">2</span> == <span class="number">1</span>)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; stride; i++)</span><br><span class="line">		&#123;</span><br><span class="line">			data[i] += data[i + stride];</span><br><span class="line">		&#125;</span><br><span class="line">		data[<span class="number">0</span>] += data[size - <span class="number">1</span>];</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">else</span></span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; stride; i++)</span><br><span class="line">		&#123;</span><br><span class="line">			data[i] += data[i + stride];</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="comment">// call</span></span><br><span class="line">	<span class="keyword">return</span> recursiveReduce(data, stride);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="并行规约中的分化"><a href="#并行规约中的分化" class="headerlink" title="并行规约中的分化"></a>并行规约中的分化</h3><p>线程束分化已经明确说明了，有判断条件的地方就会产生分支，比如if 和 for这类关键词。</p>
<p>第一步：是把这个一个数组分块，每一块只包含部分数据，如上图那样（图中数据较少，但是我们假设一块上只有这么多。），我们假定这是线程块的全部数据</p>
<p>第二步：就是每个线程要做的事，橙色圆圈就是每个线程做的操作，可见线程threadIdx.x=0 的线程进行了三次计算，奇数线程一致在陪跑，没做过任何计算，但是根据3.2中介绍，这些线程虽然什么都不干，但是不可以执行别的指令，4号线程做了两步计算，2号和6号只做了一次计算。</p>
<p>第三步：将所有块得到的结果相加，就是最终结果</p>
<p>这个计算划分就是最简单的并行规约算法，完全符合上面我们提到的三步走的套路</p>
<p>值得注意的是，我们每次进行一轮计算（黄色框，这些操作同时并行）的时候，部分全局内存要进行一次修改，但只有部分被替换，而不被替换的，也不会在后面被使用到，如蓝色框里标注的内存，就被读了一次，后面就完全没有人管了。</p>
<p>我们现在把我们的内核代码贴出来<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">reduceNeighbored</span><span class="params">(<span class="keyword">int</span> * g_idata,<span class="keyword">int</span> * g_odata,<span class="keyword">unsigned</span> <span class="keyword">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="comment">//set thread ID</span></span><br><span class="line">	<span class="keyword">unsigned</span> <span class="keyword">int</span> tid = threadIdx.x;</span><br><span class="line">	<span class="comment">//boundary check</span></span><br><span class="line">	<span class="keyword">if</span> (tid &gt;= n) <span class="keyword">return</span>;</span><br><span class="line">	<span class="comment">//convert global data pointer to the</span></span><br><span class="line">	<span class="keyword">int</span> *idata = g_idata + blockIdx.x*blockDim.x;</span><br><span class="line">	<span class="comment">//in-place reduction in global memory</span></span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> stride = <span class="number">1</span>; stride &lt; blockDim.x; stride *= <span class="number">2</span>)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">if</span> ((tid % (<span class="number">2</span> * stride)) == <span class="number">0</span>)</span><br><span class="line">		&#123;</span><br><span class="line">			idata[tid] += idata[tid + stride];</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="comment">//synchronize within block</span></span><br><span class="line">		__syncthreads();</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="comment">//write result for this block to global mem</span></span><br><span class="line">	<span class="keyword">if</span> (tid == <span class="number">0</span>)</span><br><span class="line">		g_odata[blockIdx.x] = idata[<span class="number">0</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>这里面唯一要注意的地方就是同步指令<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__syncthreads();</span><br></pre></td></tr></table></figure></p>
<p>原因还是能从图上找到，我们的每一轮操作都是并行的，但是不保证所有线程能同时执行完毕，所以需要等待，执行的快的等待慢的，这样就能避免块内的线程竞争内存了。</p>
<p>被操作的两个对象之间的距离叫做跨度，也就是变量stride，</p>
<h2 id="展开循环"><a href="#展开循环" class="headerlink" title="展开循环"></a>展开循环</h2><p>目前CUDA的编译器还不能帮我们做这种优化，人为的展开核函数内的循环，能够非常大的提升内核性能。在CUDA中展开循环的目的还是那两个：</p>
<ul>
<li>减少指令消耗</li>
<li>增加更多的独立调度指令来提高性能</li>
</ul>
<p>如果这种指令<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a[i+<span class="number">0</span>]=b[i+<span class="number">0</span>]+c[i+<span class="number">0</span>];</span><br><span class="line">a[i+<span class="number">1</span>]=b[i+<span class="number">1</span>]+c[i+<span class="number">1</span>];</span><br><span class="line">a[i+<span class="number">2</span>]=b[i+<span class="number">2</span>]+c[i+<span class="number">2</span>];</span><br><span class="line">a[i+<span class="number">3</span>]=b[i+<span class="number">3</span>]+c[i+<span class="number">3</span>];</span><br></pre></td></tr></table></figure></p>
<p>被添加到CUDA流水线上，是非常受欢迎的，因为其能最大限度的提高指令和内存带宽。下面我们就在前面归约的例子上继续挖掘性能，看看是否能得到更高的效率。</p>
<h1 id="cuda内存模型"><a href="#cuda内存模型" class="headerlink" title="cuda内存模型"></a>cuda内存模型</h1><p>CUDA内存模型相对于CPU来说那是相当丰富了，GPU上的内存设备有：</p>
<ul>
<li>寄存器</li>
<li>共享内存</li>
<li>本地内存</li>
<li>常量内存</li>
<li>纹理内存</li>
<li>全局内存</li>
</ul>
<p>上述各种都有自己的作用域，生命周期和缓存行为。CUDA中每个线程都有自己的私有的本地内存；线程块有自己的共享内存，对线程块内所有线程可见；所有线程都能访问读取常量内存和纹理内存，但是不能写，因为他们是只读的；全局内存，常量内存和纹理内存空间有不同的用途。对于一个应用来说，全局内存，常量内存和纹理内存有相同的生命周期。下图总结了上面这段话，后面的大篇幅文章就是挨个介绍这些内存的性质和使用的。</p>
<p><img src="/img/20220907141618.png" alt></p>
<h2 id="寄存器"><a href="#寄存器" class="headerlink" title="寄存器"></a>寄存器</h2><p>寄存器无论是在CPU还是在GPU都是速度最快的内存空间，但是和CPU不同的是GPU的寄存器储量要多一些，而且当我们在核函数内不加修饰的声明一个变量，此变量就存储在寄存器中，但是CPU运行的程序有些不同，只有当前在计算的变量存储在寄存器中，其余在主存中，使用时传输至寄存器。在核函数中定义的有常数长度的数组也是在寄存器中分配地址的。</p>
<p>寄存器对于每个线程是私有的，寄存器通常保存被频繁使用的私有变量，注意这里的变量也一定不能使共有的，不然的话彼此之间不可见，就会导致大家同时改变一个变量而互相不知道，寄存器变量的声明周期和核函数一致，从开始运行到运行结束，执行完毕后，寄存器就不能访问了。</p>
<p>寄存器是SM中的稀缺资源，Fermi架构中每个线程最多63个寄存器。Kepler结构扩展到255个寄存器，一个线程如果使用更少的寄存器，那么就会有更多的常驻线程块，SM上并发的线程块越多，效率越高，性能和使用率也就越高。</p>
<p>那么问题就来了，如果一个线程里面的变量太多，以至于寄存器完全不够呢？这时候寄存器发生溢出，本地内存就会过来帮忙存储多出来的变量，这种情况会对效率产生非常负面的影响，所以，不到万不得已，一定要避免此种情况发生。</p>
<p>为了避免寄存器溢出，可以在核函数的代码中配置额外的信息来辅助编译器优化，比如：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">__global__ <span class="keyword">void</span></span><br><span class="line">__lauch_bounds__(maxThreadaPerBlock,minBlocksPerMultiprocessor)</span><br><span class="line">kernel(...) &#123;</span><br><span class="line">    <span class="comment">/* kernel code */</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>这里面在核函数定义前加了一个 关键字 lauch_bounds，然后他后面对应了两个变量：</p>
<ol>
<li>maxThreadaPerBlock：线程块内包含的最大线程数，线程块由核函数来启动</li>
<li>minBlocksPerMultiprocessor：可选参数，每个SM中预期的最小的常驻内存块参数。</li>
</ol>
<p>注意，对于一定的核函数，优化的启动边界会因为不同的结构而不同。也可以在编译选项中加入<code>-maxrregcount=32</code>来控制一个编译单元里所有核函数使用的最大数量。</p>
<h2 id="本地内存"><a href="#本地内存" class="headerlink" title="本地内存"></a>本地内存</h2><p>核函数中符合存储在寄存器中但不能进入被核函数分配的寄存器空间中的变量将存储在本地内存中，编译器可能存放在本地内存中的变量有以下几种：</p>
<ul>
<li>使用未知索引引用的本地数组</li>
<li>可能会占用大量寄存器空间的较大本地数组或者结构体</li>
<li>任何不满足核函数寄存器限定条件的变量</li>
</ul>
<p>本地内存实质上是和全局内存一样在同一块存储区域当中的，其访问特点——高延迟，低带宽。对于2.0以上的设备，本地内存存储在每个SM的一级缓存，或者设备的二级缓存上。</p>
<h2 id="共享内存"><a href="#共享内存" class="headerlink" title="共享内存"></a>共享内存</h2><p>在核函数中使用如下修饰符的内存，称为共享内存：<code>__share__</code>。</p>
<p>每个SM都有一定数量的由线程块分配的共享内存，共享内存是片上内存，跟主存相比，速度要快很多，也即是延迟低，带宽高。其类似于一级缓存，但是可以被编程。使用共享内存的时候一定要注意，不要因为过度使用共享内存，而导致SM上活跃的线程束减少，也就是说，一个线程块使用的共享内存过多，导致更过的线程块没办法被SM启动，这样影响活跃的线程束数量。</p>
<p>共享内存在核函数内声明，生命周期和线程块一致，线程块运行开始，此块的共享内存被分配，当此块结束，则共享内存被释放。因为共享内存是块内线程可见的，所以就有竞争问题的存在，也可以通过共享内存进行通信，当然，为了避免内存竞争，可以使用同步语句：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">void</span> __syncthreads();</span><br></pre></td></tr></table></figure></p>
<p>此语句相当于在线程块执行时各个线程的一个障碍点，当块内所有线程都执行到本障碍点的时候才能进行下一步的计算，这样可以设计出避免内存竞争的共享内存使用程序。</p>
<p>注意，<code>__syncthreads();</code>频繁使用会影响内核执行效率。</p>
<p>SM中的一级缓存，和共享内存共享一个64k的片上内存（不知道现在的设备有没有提高），他们通过静态划分，划分彼此的容量，运行时可以通过下面语句进行设置：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaFuncSetCacheConfig</span><span class="params">(<span class="keyword">const</span> <span class="keyword">void</span> * func,<span class="keyword">enum</span> cudaFuncCache)</span></span>;</span><br></pre></td></tr></table></figure></p>
<p>这个函数可以设置内核的共享内存和一级缓存之间的比例。cudaFuncCache参数可选如下配置：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cudaFuncCachePreferNone<span class="comment">//无参考值，默认设置</span></span><br><span class="line">cudaFuncCachePreferShared<span class="comment">//48k共享内存，16k一级缓存</span></span><br><span class="line">cudaFuncCachePreferL1<span class="comment">// 48k一级缓存，16k共享内存</span></span><br><span class="line">cudaFuncCachePreferEqual<span class="comment">// 32k一级缓存，32k共享内存</span></span><br></pre></td></tr></table></figure></p>
<h2 id="常量内存"><a href="#常量内存" class="headerlink" title="常量内存"></a>常量内存</h2><p>常量内存驻留在设备内存中，每个SM都有专用的常量内存缓存，常量内存使用：<code>__constant__</code>修饰，常量内存在核函数外，全局范围内声明，对于所有设备，只可以声明64k的常量内存，常量内存静态声明，并对同一编译单元中的所有核函数可见。</p>
<p>常量内存，显然是不能被修改的，这里不能被修改指的是被核函数修改，主机端代码是可以初始化常量内存的，不然这个内存谁都不能改就没有什么使用意义了，常量内存，被主机端初始化后不能被核函数修改，初始化函数如下：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaMemcpyToSymbol</span><span class="params">(<span class="keyword">const</span> <span class="keyword">void</span>* symbol,<span class="keyword">const</span> <span class="keyword">void</span> *src,<span class="keyword">size_t</span> count)</span></span>;</span><br></pre></td></tr></table></figure></p>
<p>同 cudaMemcpy的参数列表相似，从src复制count个字节的内存到symbol里面，也就是设备端的常量内存。多数情况下此函数是同步的，也就是会马上被执行。</p>
<p>当线程束中所有线程都从相同的地址取数据时，常量内存表现较好，比如执行某一个多项式计算，系数都存在常量内存里效率会非常高，但是如果不同的线程取不同地址的数据，常量内存就不那么好了，因为常量内存的读取机制是：一次读取会广播给所有线程束内的线程。</p>
<h2 id="纹理内存"><a href="#纹理内存" class="headerlink" title="纹理内存"></a>纹理内存</h2><p>纹理内存驻留在设备内存中，在每个SM的只读缓存中缓存，纹理内存是通过指定的缓存访问的全局内存，只读缓存包括硬件滤波的支持，它可以将浮点插入作为读取过程中的一部分来执行，纹理内存是对二维空间局部性的优化。总的来说纹理内存设计目的应该是为了GPU本职工作显示设计的，但是对于某些特定的程序可能效果更好，比如需要滤波的程序，可以直接通过硬件完成。</p>
<h2 id="全局内存"><a href="#全局内存" class="headerlink" title="全局内存"></a>全局内存</h2><p>GPU上最大的内存空间，延迟最高，使用最常见的内存，global指的是作用域和生命周期，一般在主机端代码里定义，也可以在设备端定义，不过需要加修饰符，只要不销毁，是和应用程序同生命周期的。全局内存对应于设备内存，一个是逻辑表示，一个是硬件表示。</p>
<p>全局内存可以动态声明，或者静态声明，可以用下面的修饰符在设备代码中静态的声明一个变量：<code>__device__</code>。我们前面声明的所有的在GPU上访问的内存都是全局内存，或者说到目前为止我们还没对内存进行任何优化。因为全局内存的性质，当有多个核函数同时执行的时候，如果使用到了同一全局变量，应注意内存竞争。</p>
<p>全局内存访问是对齐，也就是一次要读取指定大小（32，64，128）整数倍字节的内存，所以当线程束执行内存加载/存储时，需要满足的传输数量通常取决与以下两个因素：</p>
<ul>
<li>跨线程的内存地址分布</li>
<li>内存事务的对齐方式。</li>
</ul>
<p>一般情况下满足内存请求的事务越多，未使用的字节被传输的可能性越大，数据吞吐量就会降低，换句话说，对齐的读写模式使得不需要的数据也被传输，所以，利用率低到时吞吐量下降。1.1以下的设备对内存访问要求非常严格（为了达到高效，访问受到限制）因为当时还没有缓存，现在的设备都有缓存了，所以宽松了一些。</p>
<h2 id="GPU缓存"><a href="#GPU缓存" class="headerlink" title="GPU缓存"></a>GPU缓存</h2><p>与CPU缓存类似，GPU缓存不可编程，其行为出厂是时已经设定好了。GPU上有4种缓存：</p>
<ul>
<li>一级缓存</li>
<li>二级缓存</li>
<li>只读常量缓存</li>
<li>只读纹理缓存</li>
</ul>
<p>每个SM都有一个一级缓存，所有SM公用一个二级缓存。一级二级缓存的作用都是被用来存储本地内存和全局内存中的数据，也包括寄存器溢出的部分。Fermi，Kepler以及以后的设备，CUDA允许我们配置读操作的数据是使用一级缓存和二级缓存，还是只使用二级缓存。</p>
<p>与CPU不同的是，CPU读写过程都有可能被缓存，但是GPU写的过程不被缓存，只有加载会被缓存！</p>
<p>每个SM有一个只读常量缓存，只读纹理缓存，它们用于设备内存中提高来自于各自内存空间内的读取性能。</p>
<p>CUDA变量声明总结<br>用表格进行总结：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>修饰符</th>
<th>变量名称</th>
<th>存储器</th>
<th>作用域</th>
<th>生命周期</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>float var</td>
<td>寄存器</td>
<td>线程</td>
<td>线程</td>
</tr>
<tr>
<td></td>
<td>float var[100]</td>
<td>本地</td>
<td>线程</td>
<td>线程</td>
</tr>
<tr>
<td><strong>share</strong></td>
<td>float var*</td>
<td>共享</td>
<td>块</td>
<td>块</td>
</tr>
<tr>
<td><strong>device</strong></td>
<td>float var*</td>
<td>全局</td>
<td>全局</td>
<td>应用程序</td>
</tr>
<tr>
<td>__constant</td>
<td>float var*</td>
<td>常量</td>
<td>全局</td>
<td>应用程序</td>
</tr>
</tbody>
</table>
</div>
<p>设备存储器的重要特征：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>存储器</th>
<th>片上/片外</th>
<th>缓存</th>
<th>存取</th>
<th>范围</th>
<th>生命周期</th>
</tr>
</thead>
<tbody>
<tr>
<td>寄存器</td>
<td>片上</td>
<td>n/a</td>
<td>R/W</td>
<td>一个线程</td>
<td>线程</td>
</tr>
<tr>
<td>本地</td>
<td>片外</td>
<td>1.0以上有</td>
<td>R/W</td>
<td>一个线程</td>
<td>线程</td>
</tr>
<tr>
<td>共享</td>
<td>片上</td>
<td>n/a</td>
<td>R/W</td>
<td>块内所有线程</td>
<td>块</td>
</tr>
<tr>
<td>全局</td>
<td>片外</td>
<td>1.0以上有</td>
<td>R/W</td>
<td>所有线程+主机</td>
<td>主机配置</td>
</tr>
<tr>
<td>常量</td>
<td>片外</td>
<td>Yes</td>
<td>R</td>
<td>所有线程+主机</td>
<td>主机配置</td>
</tr>
<tr>
<td>纹理</td>
<td>片外</td>
<td>Yes</td>
<td>R</td>
<td>所有线程+主机</td>
<td>主机配置</td>
</tr>
</tbody>
</table>
</div>
<h2 id="静态全局内存"><a href="#静态全局内存" class="headerlink" title="静态全局内存"></a>静态全局内存</h2><p>CPU内存有动态分配和静态分配两种类型，从内存位置来说，动态分配在堆上进行，静态分配在栈上进行，在代码上的表现是一个需要new，malloc等类似的函数动态分配空间，并用delete和free来释放。在CUDA中也有类似的动态静态之分，我们前面用的都是要cudaMalloc的，所以对比来说就是动态分配，我们今天来个静态分配的，不过与动态分配相同是，也需要显式的将内存copy到设备端，我们用下面代码来看一下程序的运行结果:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line">__device__ <span class="keyword">float</span> devData;</span><br><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">checkGlobalVariable</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Device: The value of the global variable is %f\n"</span>,devData);</span><br><span class="line">    devData+=<span class="number">2.0</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">float</span> value=<span class="number">3.14f</span>;</span><br><span class="line">    cudaMemcpyToSymbol(devData,&amp;value,<span class="keyword">sizeof</span>(<span class="keyword">float</span>));</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Host: copy %f to the global variable\n"</span>,value);</span><br><span class="line">    checkGlobalVariable&lt;&lt;&lt;<span class="number">1</span>,<span class="number">1</span>&gt;&gt;&gt;();</span><br><span class="line">    cudaMemcpyFromSymbol(&amp;value,devData,<span class="keyword">sizeof</span>(<span class="keyword">float</span>));</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Host: the value changed by the kernel to %f \n"</span>,value);</span><br><span class="line">    cudaDeviceReset();</span><br><span class="line">    <span class="keyword">return</span> EXIT_SUCCESS;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个唯一要注意的就是，这一句<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaMemcpyToSymbol(devData,&amp;value,sizeof(float));</span><br></pre></td></tr></table></figure></p>
<p>函数原型说的是第一个应该是个void*，但是这里写了一个<code>device float devData;</code>变量，这个说到底还是设备上的变量定义和主机变量定义的不同，设备变量在代码中定义的时候其实就是一个指针，这个指针指向何处，主机端是不知道的，指向的内容也不知道，想知道指向的内容，唯一的办法还是通过显式的办法传输过来：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaMemcpyFromSymbol(&amp;value,devData,sizeof(float));</span><br></pre></td></tr></table></figure></p>
<p>这里需要注意的只有这点：在主机端，devData只是一个标识符，不是设备全局内存的变量地址<br>在核函数中，devData就是一个全局内存中的变量。主机代码不能直接访问设备变量，设备也不能访问主机变量，这就是CUDA编程与CPU多核最大的不同之处<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaMemcpy(&amp;value,devData,sizeof(float));</span><br></pre></td></tr></table></figure></p>
<p>是不可以的！这个函数是无效的！就是你不能用动态copy的方法给静态变量赋值！</p>
<p>如果你死活都要用cudaMemcpy，只能用下面的方式：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">float</span> *dptr=<span class="literal">NULL</span>;</span><br><span class="line">cudaGetSymbolAddress((<span class="keyword">void</span>**)&amp;dptr,devData);</span><br><span class="line">cudaMemcpy(dptr,&amp;value,<span class="keyword">sizeof</span>(<span class="keyword">float</span>),cudaMemcpyHostToDevice);</span><br></pre></td></tr></table></figure></p>
<p>主机端不可以对设备变量进行取地址操作！这是非法的！</p>
<p>想要得到devData的地址可以用下面方法：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">float</span> *dptr=<span class="literal">NULL</span>;</span><br><span class="line">cudaGetSymbolAddress((<span class="keyword">void</span>**)&amp;dptr,devData);</span><br></pre></td></tr></table></figure></p>
<p>当然也有一个例外，可以直接从主机引用GPU内存——CUDA固定内存。后面我们会研究这部分。</p>
<p>CUDA运行时API能访问主机和设备变量，但这取决于你给正确的函数是否提供了正确的参数，使用运行时API，如果参数填错，尤其是主机和设备上的指针，结果是无法预测的。</p>
<h1 id="内存管理"><a href="#内存管理" class="headerlink" title="内存管理"></a>内存管理</h1><p>CUDA是C语言的扩展，内存方面基本集成了C语言的方式，由程序员控制CUDA内存，当然，这些内存的物理设备是在GPU上的，而且与CPU内存分配不同，CPU内存分配完就完事了，GPU还涉及到数据传输，主机和设备之间的传输。接下来我们要了解的是：</p>
<ul>
<li>分配释放设备内存</li>
<li>在主机和设备间传输内存</li>
</ul>
<p>为达到最优性能，CUDA提供了在主机端准备设备内存的函数，并且显式地向设备传递数据，显式的从设备取回数据。</p>
<h2 id="内存分配和释放"><a href="#内存分配和释放" class="headerlink" title="内存分配和释放"></a>内存分配和释放</h2><p>内存的分配和释放我们在前面已经用过很多次了，前面所有的要计算的例子都包含这一步：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaMalloc</span><span class="params">(<span class="keyword">void</span> ** devPtr,<span class="keyword">size_t</span> count)</span></span></span><br></pre></td></tr></table></figure>
<p>这个函数用过很多次了，唯一要注意的是第一个参数，是指针的指针，一般的用法是首先我们生命一个指针变量，然后调用这个函数：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">float</span> * devMem=<span class="literal">NULL</span>;</span><br><span class="line"><span class="function">cudaError_t <span class="title">cudaMalloc</span><span class="params">((<span class="keyword">float</span>**) devMem, count)</span></span></span><br></pre></td></tr></table></figure>
<p>这里是这样的，devMem是一个指针，定义时初始化指向NULL，这样做是安全的，避免出现野指针，cudaMalloc函数要修改devMem的值，所以必须把他的指针传递给函数，如果把devMem当做参数传递，经过函数后，指针的内容还是NULL。</p>
<p>内存分配支持所有的数据类型，什么int，float。。。这些都无所谓，因为他是按照字节分配的，只要是正数字节的变量都能分配，当然我们根本没有半个字节的东西。函数执行失败返回：cudaErrorMemoryAllocation。</p>
<p>当分配完地址后，可以使用下面函数进行初始化：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaMemset</span><span class="params">(<span class="keyword">void</span> * devPtr,<span class="keyword">int</span> value,<span class="keyword">size_t</span> count)</span></span></span><br></pre></td></tr></table></figure>
<p>用法和Memset类似，但是注意，这些被我们操作的内存对应的物理内存都在GPU上。</p>
<p>当分配的内存不被使用时，使用下面语句释放程序。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaFree</span><span class="params">(<span class="keyword">void</span> * devPtr)</span></span></span><br></pre></td></tr></table></figure>
<p>注意这个参数一定是前面cudaMalloc类的函数（还有其他分配函数）分配到空间，如果输入非法指针参数，会返回 cudaErrorInvalidDevicePointer 错误，如果重复释放一个空间，也会报错。</p>
<h2 id="内存传输"><a href="#内存传输" class="headerlink" title="内存传输"></a>内存传输</h2><p>下面介绍点C语言没有的，C语言的内存分配完成后就可以直接读写了，但是对于异构计算，这样是不行的，因为主机线程不能访问设备内存，设备线程也不能访问主机内存，这时候我们要传送数据了：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t cudaMemcpy(void *dst,const void * src,size_t count,enum cudaMemcpyKind kind)</span><br></pre></td></tr></table></figure>
<p>这个函数我们前面也反复用到，注意这里的参数是指针，而不是指针的指针，第一个参数dst是目标地址，第二个参数src是原始地址，然后是拷贝的内存大小，最后是传输类型，传输类型包括以下几种：</p>
<ul>
<li>cudaMemcpyHostToHost</li>
<li>cudaMemcpyHostToDevice</li>
<li>cudaMemcpyDeviceToHost</li>
<li>cudaMemcpyDeviceToDevice</li>
</ul>
<p>这个例子也不用说了，前面随便找个有数据传输的都有这两步：从主机到设备，然后计算，最后从设备到主机。</p>
<p><img src="C:\Users\69033\Desktop\image-20220910105700518.png" alt="image-20220910105700518"></p>
<p>GPU的内存理论峰值带宽非常高，对于Fermi C2050 有144GB/s，这个值估计现在的GPU应该都超过了，CPU和GPU之间通信要经过PCIe总线，总线的理论峰值要低很多——8GB/s左右，也就是说所，管理不当，算到半路需要从主机读数据，那效率瞬间全挂在PCIe上了。</p>
<p><strong>CUDA编程需要大家减少主机和设备之间的内存传输</strong>。</p>
<h2 id="固定内存"><a href="#固定内存" class="headerlink" title="固定内存"></a>固定内存</h2><p>主机内存采用分页式管理，通俗的说法就是操作系统把物理内存分成一些“页”，然后给一个应用程序一大块内存，而操作系统可能随时更换物理地址的页，但是从主机传输到设备上的时候，如果此时发生了页面移动，对于传输操作来说是致命的，所以在数据传输之前，CUDA驱动会锁定页面，或者直接分配固定的主机内存，将主机源数据复制到固定内存上，然后从固定内存传输数据到设备上：</p>
<p><img src="C:\Users\69033\Desktop\image-20220910105840559.png" alt="image-20220910105840559"></p>
<p>上图左边是正常分配内存，传输过程是：锁页-复制到固定内存-复制到设备。右边时分配时就是固定内存，直接传输到设备上。</p>
<p>下面函数用来分配固定内存：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaMallocHost</span><span class="params">(<span class="keyword">void</span> ** devPtr,<span class="keyword">size_t</span> count)</span></span></span><br></pre></td></tr></table></figure>
<p>分配count字节的固定内存，这些内存是页面锁定的，可以直接传输到设备的。这样就是的传输带宽变得高很多。</p>
<p>固定的主机内存释放使用：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaFreeHost</span><span class="params">(<span class="keyword">void</span> *ptr)</span></span></span><br></pre></td></tr></table></figure>
<p>我们可以测试一下固定内存和分页内存的传输效率，代码如下</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"freshman.h"</span></span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">sumArrays</span><span class="params">(<span class="keyword">float</span> * a,<span class="keyword">float</span> * b,<span class="keyword">float</span> * res,<span class="keyword">const</span> <span class="keyword">int</span> size)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;size;i+=<span class="number">4</span>)</span><br><span class="line">  &#123;</span><br><span class="line">    res[i]=a[i]+b[i];</span><br><span class="line">    res[i+<span class="number">1</span>]=a[i+<span class="number">1</span>]+b[i+<span class="number">1</span>];</span><br><span class="line">    res[i+<span class="number">2</span>]=a[i+<span class="number">2</span>]+b[i+<span class="number">2</span>];</span><br><span class="line">    res[i+<span class="number">3</span>]=a[i+<span class="number">3</span>]+b[i+<span class="number">3</span>];</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">sumArraysGPU</span><span class="params">(<span class="keyword">float</span>*a,<span class="keyword">float</span>*b,<span class="keyword">float</span>*res)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">int</span> i=blockIdx.x*blockDim.x+threadIdx.x;</span><br><span class="line">  res[i]=a[i]+b[i];</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc,<span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">int</span> dev = <span class="number">0</span>;</span><br><span class="line">  cudaSetDevice(dev);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span> nElem=<span class="number">1</span>&lt;&lt;<span class="number">14</span>;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">"Vector size:%d\n"</span>,nElem);</span><br><span class="line">  <span class="keyword">int</span> nByte=<span class="keyword">sizeof</span>(<span class="keyword">float</span>)*nElem;</span><br><span class="line">  <span class="keyword">float</span> *a_h=(<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="keyword">float</span> *b_h=(<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="keyword">float</span> *res_h=(<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="keyword">float</span> *res_from_gpu_h=(<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="built_in">memset</span>(res_h,<span class="number">0</span>,nByte);</span><br><span class="line">  <span class="built_in">memset</span>(res_from_gpu_h,<span class="number">0</span>,nByte);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">float</span> *a_d,*b_d,*res_d;</span><br><span class="line">  <span class="comment">// pine memory malloc</span></span><br><span class="line">  CHECK(cudaMallocHost((<span class="keyword">float</span>**)&amp;a_d,nByte));</span><br><span class="line">  CHECK(cudaMallocHost((<span class="keyword">float</span>**)&amp;b_d,nByte));</span><br><span class="line">  CHECK(cudaMallocHost((<span class="keyword">float</span>**)&amp;res_d,nByte));</span><br><span class="line"></span><br><span class="line">  initialData(a_h,nElem);</span><br><span class="line">  initialData(b_h,nElem);</span><br><span class="line"></span><br><span class="line">  CHECK(cudaMemcpy(a_d,a_h,nByte,cudaMemcpyHostToDevice));</span><br><span class="line">  CHECK(cudaMemcpy(b_d,b_h,nByte,cudaMemcpyHostToDevice));</span><br><span class="line"></span><br><span class="line">  <span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">1024</span>)</span></span>;</span><br><span class="line">  <span class="function">dim3 <span class="title">grid</span><span class="params">(nElem/block.x)</span></span>;</span><br><span class="line">  sumArraysGPU&lt;&lt;&lt;grid,block&gt;&gt;&gt;(a_d,b_d,res_d);</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">"Execution configuration&lt;&lt;&lt;%d,%d&gt;&gt;&gt;\n"</span>,grid.x,block.x);</span><br><span class="line"></span><br><span class="line">  CHECK(cudaMemcpy(res_from_gpu_h,res_d,nByte,cudaMemcpyDeviceToHost));</span><br><span class="line">  sumArrays(a_h,b_h,res_h,nElem);</span><br><span class="line"></span><br><span class="line">  checkResult(res_h,res_from_gpu_h,nElem);</span><br><span class="line">  cudaFreeHost(a_d);</span><br><span class="line">  cudaFreeHost(b_d);</span><br><span class="line">  cudaFreeHost(res_d);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">free</span>(a_h);</span><br><span class="line">  <span class="built_in">free</span>(b_h);</span><br><span class="line">  <span class="built_in">free</span>(res_h);</span><br><span class="line">  <span class="built_in">free</span>(res_from_gpu_h);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>使用</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvprof ./pine_memory</span><br></pre></td></tr></table></figure>
<p>固定内存的释放和分配成本比可分页内存要高很多，但是传输速度更快，所以对于大规模数据，固定内存效率更高。</p>
<h2 id="零拷贝内存"><a href="#零拷贝内存" class="headerlink" title="零拷贝内存"></a>零拷贝内存</h2><p>截止到目前，我们所接触到的内存知识的基础都是：主机直接不能访问设备内存，设备不能直接访问主机内存。对于早期设备，这是肯定的，但是后来，一个例外出现了——零拷贝内存。GPU线程可以直接访问零拷贝内存，这部分内存在主机内存里面，CUDA核函数使用零拷贝内存有以下几种情况：</p>
<ul>
<li>当设备内存不足的时候可以利用主机内存</li>
<li>避免主机和设备之间的显式内存传输</li>
<li>提高PCIe传输率</li>
</ul>
<p>前面我们讲，注意线程之间的内存竞争，因为他们可以同时访问同一个内存地址，现在设备和主机可以同时访问同一个设备地址了，所以，我们要注意主机和设备的内存竞争——当使用零拷贝内存的时候。</p>
<p>零拷贝内存是固定内存，不可分页。可以通过以下函数创建零拷贝内存：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaHostAlloc</span><span class="params">(<span class="keyword">void</span> ** pHost,<span class="keyword">size_t</span> count,<span class="keyword">unsigned</span> <span class="keyword">int</span> flags)</span></span></span><br></pre></td></tr></table></figure>
<p>最后一个标志参数，可以选择以下值：</p>
<ul>
<li>cudaHostAllocDefalt</li>
<li>cudaHostAllocPortable</li>
<li>cudaHostAllocWriteCombined</li>
<li>cudaHostAllocMapped</li>
</ul>
<p><code>cudaHostAllocDefalt</code>和<code>cudaMallocHost</code>函数一致，<code>cudaHostAllocPortable</code>函数返回能被所有CUDA上下文使用的固定内存，<code>cudaHostAllocWriteCombined</code>返回写结合内存，在某些设备上这种内存传输效率更高。<code>cudaHostAllocMapped</code>产生零拷贝内存。</p>
<p>注意，零拷贝内存虽然不需要显式的传递到设备上，但是设备还不能通过pHost直接访问对应的内存地址，设备需要访问主机上的零拷贝内存，需要先获得另一个地址，这个地址帮助设备访问到主机对应的内存，方法是：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaHostGetDevicePointer</span><span class="params">(<span class="keyword">void</span> ** pDevice,<span class="keyword">void</span> * pHost,<span class="keyword">unsigned</span> flags)</span></span>;</span><br></pre></td></tr></table></figure>
<p><code>pDevice</code>就是设备上访问主机零拷贝内存的指针了！零拷贝内存可以当做比设备主存储器更慢的一个设备。</p>
<p>频繁的读写，零拷贝内存效率极低，这个非常容易理解，因为每次都要经过PCIe。</p>
<p>我们下面进行一个小实验，数组加法，改编自前面的代码，然后我们看看效果：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc,<span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">int</span> dev = <span class="number">0</span>;</span><br><span class="line">  cudaSetDevice(dev);</span><br><span class="line">  <span class="keyword">int</span> power=<span class="number">10</span>;</span><br><span class="line">  <span class="keyword">if</span>(argc&gt;=<span class="number">2</span>)</span><br><span class="line">    power=atoi(argv[<span class="number">1</span>]);</span><br><span class="line">  <span class="keyword">int</span> nElem=<span class="number">1</span>&lt;&lt;power;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">"Vector size:%d\n"</span>,nElem);</span><br><span class="line">  <span class="keyword">int</span> nByte=<span class="keyword">sizeof</span>(<span class="keyword">float</span>)*nElem;</span><br><span class="line">  <span class="keyword">float</span> *res_from_gpu_h=(<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="keyword">float</span> *res_h=(<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="built_in">memset</span>(res_h,<span class="number">0</span>,nByte);</span><br><span class="line">  <span class="built_in">memset</span>(res_from_gpu_h,<span class="number">0</span>,nByte);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">float</span> *a_host,*b_host,*res_d;</span><br><span class="line">  <span class="keyword">double</span> iStart,iElaps;</span><br><span class="line">  <span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">1024</span>)</span></span>;</span><br><span class="line">  <span class="function">dim3 <span class="title">grid</span><span class="params">(nElem/block.x)</span></span>;</span><br><span class="line">  res_from_gpu_h=(<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="keyword">float</span> *a_dev,*b_dev;</span><br><span class="line">  CHECK(cudaHostAlloc((<span class="keyword">float</span>**)&amp;a_host,nByte,cudaHostAllocMapped));</span><br><span class="line">  CHECK(cudaHostAlloc((<span class="keyword">float</span>**)&amp;b_host,nByte,cudaHostAllocMapped));</span><br><span class="line">  CHECK(cudaMalloc((<span class="keyword">float</span>**)&amp;res_d,nByte));</span><br><span class="line">  initialData(a_host,nElem);</span><br><span class="line">  initialData(b_host,nElem);</span><br><span class="line"></span><br><span class="line"> <span class="comment">//=============================================================//</span></span><br><span class="line">  iStart = cpuSecond();</span><br><span class="line">  CHECK(cudaHostGetDevicePointer((<span class="keyword">void</span>**)&amp;a_dev,(<span class="keyword">void</span>*) a_host,<span class="number">0</span>));</span><br><span class="line">  CHECK(cudaHostGetDevicePointer((<span class="keyword">void</span>**)&amp;b_dev,(<span class="keyword">void</span>*) b_host,<span class="number">0</span>));</span><br><span class="line">  sumArraysGPU&lt;&lt;&lt;grid,block&gt;&gt;&gt;(a_dev,b_dev,res_d);</span><br><span class="line">  CHECK(cudaMemcpy(res_from_gpu_h,res_d,nByte,cudaMemcpyDeviceToHost));</span><br><span class="line">  iElaps = cpuSecond() - iStart;</span><br><span class="line"> <span class="comment">//=============================================================//</span></span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">"zero copy memory elapsed %lf ms \n"</span>, iElaps);</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">"Execution configuration&lt;&lt;&lt;%d,%d&gt;&gt;&gt;\n"</span>,grid.x,block.x);</span><br><span class="line"><span class="comment">//-----------------------normal memory---------------------------</span></span><br><span class="line">  <span class="keyword">float</span> *a_h_n=(<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="keyword">float</span> *b_h_n=(<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="keyword">float</span> *res_h_n=(<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="keyword">float</span> *res_from_gpu_h_n=(<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="built_in">memset</span>(res_h_n,<span class="number">0</span>,nByte);</span><br><span class="line">  <span class="built_in">memset</span>(res_from_gpu_h_n,<span class="number">0</span>,nByte);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">float</span> *a_d_n,*b_d_n,*res_d_n;</span><br><span class="line">  CHECK(cudaMalloc((<span class="keyword">float</span>**)&amp;a_d_n,nByte));</span><br><span class="line">  CHECK(cudaMalloc((<span class="keyword">float</span>**)&amp;b_d_n,nByte));</span><br><span class="line">  CHECK(cudaMalloc((<span class="keyword">float</span>**)&amp;res_d_n,nByte));</span><br><span class="line"></span><br><span class="line">  initialData(a_h_n,nElem);</span><br><span class="line">  initialData(b_h_n,nElem);</span><br><span class="line"><span class="comment">//=============================================================//</span></span><br><span class="line">  iStart = cpuSecond();</span><br><span class="line">  CHECK(cudaMemcpy(a_d_n,a_h_n,nByte,cudaMemcpyHostToDevice));</span><br><span class="line">  CHECK(cudaMemcpy(b_d_n,b_h_n,nByte,cudaMemcpyHostToDevice));</span><br><span class="line">  sumArraysGPU&lt;&lt;&lt;grid,block&gt;&gt;&gt;(a_d_n,b_d_n,res_d_n);</span><br><span class="line">  CHECK(cudaMemcpy(res_from_gpu_h,res_d,nByte,cudaMemcpyDeviceToHost));</span><br><span class="line">  iElaps = cpuSecond() - iStart;</span><br><span class="line"><span class="comment">//=============================================================//</span></span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">"device memory elapsed %lf ms \n"</span>, iElaps);</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">"Execution configuration&lt;&lt;&lt;%d,%d&gt;&gt;&gt;\n"</span>,grid.x,block.x);</span><br><span class="line"><span class="comment">//--------------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line">  sumArrays(a_host,b_host,res_h,nElem);</span><br><span class="line">  checkResult(res_h,res_from_gpu_h,nElem);</span><br><span class="line"></span><br><span class="line">  cudaFreeHost(a_host);</span><br><span class="line">  cudaFreeHost(b_host);</span><br><span class="line">  cudaFree(res_d);</span><br><span class="line">  <span class="built_in">free</span>(res_h);</span><br><span class="line">  <span class="built_in">free</span>(res_from_gpu_h);</span><br><span class="line"></span><br><span class="line">  cudaFree(a_d_n);</span><br><span class="line">  cudaFree(b_d_n);</span><br><span class="line">  cudaFree(res_d_n);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">free</span>(a_h_n);</span><br><span class="line">  <span class="built_in">free</span>(b_h_n);</span><br><span class="line">  <span class="built_in">free</span>(res_h_n);</span><br><span class="line">  <span class="built_in">free</span>(res_from_gpu_h_n);</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我们把结果写在一个表里面：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">数据规模n( 2^n )</th>
<th style="text-align:center">常规内存（us）</th>
<th style="text-align:center">零拷贝内存（us）</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">10</td>
<td style="text-align:center">2.5</td>
<td style="text-align:center">3.0</td>
</tr>
<tr>
<td style="text-align:center">12</td>
<td style="text-align:center">3.0</td>
<td style="text-align:center">4.1</td>
</tr>
<tr>
<td style="text-align:center">14</td>
<td style="text-align:center">7.8</td>
<td style="text-align:center">8.6</td>
</tr>
<tr>
<td style="text-align:center">16</td>
<td style="text-align:center">23.1</td>
<td style="text-align:center">25.8</td>
</tr>
<tr>
<td style="text-align:center">18</td>
<td style="text-align:center">86.5</td>
<td style="text-align:center">98.2</td>
</tr>
<tr>
<td style="text-align:center">20</td>
<td style="text-align:center">290.9</td>
<td style="text-align:center">310.5</td>
</tr>
</tbody>
</table>
</div>
<p>这是通过观察运行时间得到的，当然也可以通过我们上面的nvprof得到内核执行时间：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">数据规模n( 2^n )</th>
<th style="text-align:center">常规内存（us）</th>
<th style="text-align:center">零拷贝内存（us）</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">10</td>
<td style="text-align:center">1.088</td>
<td style="text-align:center">4.257</td>
</tr>
<tr>
<td style="text-align:center">12</td>
<td style="text-align:center">1.056</td>
<td style="text-align:center">8.00</td>
</tr>
<tr>
<td style="text-align:center">14</td>
<td style="text-align:center">1.920</td>
<td style="text-align:center">24.578</td>
</tr>
<tr>
<td style="text-align:center">16</td>
<td style="text-align:center">4.544</td>
<td style="text-align:center">86.63</td>
</tr>
</tbody>
</table>
</div>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/C/" rel="tag"># C++</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2022/08/18/cpp并发编程/" rel="next" title="C++并发编程">
                <i class="fa fa-chevron-left"></i> C++并发编程
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/header.jpg" alt="Hao Yu">
            
              <p class="site-author-name" itemprop="name">Hao Yu</p>
              <p class="site-description motion-element" itemprop="description">Introduce something interesting and recode learning process, some articles are written by others, the original link has been given as much as possible, thanks to the original author</p>
          </div>
          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">291</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">13</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>
        	<audio controls="controls" loop="loop" preload="auto" src="/resource/xiaomeihao.mp3">
	        </audio>
	

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/yuhao0102" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:yuh18@mails.tsinghua.edu.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#概述"><span class="nav-number">1.</span> <span class="nav-text">概述</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#线程管理"><span class="nav-number">1.1.</span> <span class="nav-text">线程管理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#核函数概述"><span class="nav-number">1.2.</span> <span class="nav-text">核函数概述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#编写核函数"><span class="nav-number">1.3.</span> <span class="nav-text">编写核函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#验证核函数"><span class="nav-number">1.4.</span> <span class="nav-text">验证核函数</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#给核函数计时"><span class="nav-number">2.</span> <span class="nav-text">给核函数计时</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#用nvprof计时"><span class="nav-number">2.1.</span> <span class="nav-text">用nvprof计时</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#组织并行线程"><span class="nav-number">3.</span> <span class="nav-text">组织并行线程</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#使用块和线程建立矩阵索引"><span class="nav-number">3.1.</span> <span class="nav-text">使用块和线程建立矩阵索引</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#二维矩阵加法"><span class="nav-number">3.2.</span> <span class="nav-text">二维矩阵加法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#二维网格和二维块"><span class="nav-number">3.3.</span> <span class="nav-text">二维网格和二维块</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#一维网格和一维块"><span class="nav-number">3.4.</span> <span class="nav-text">一维网格和一维块</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#GPU设备信息"><span class="nav-number">4.</span> <span class="nav-text">GPU设备信息</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#CUDA执行模型概述"><span class="nav-number">5.</span> <span class="nav-text">CUDA执行模型概述</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#GPU架构概述"><span class="nav-number">5.1.</span> <span class="nav-text">GPU架构概述</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#线程束"><span class="nav-number">5.1.1.</span> <span class="nav-text">线程束</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SIMD-vs-SIMT"><span class="nav-number">5.1.2.</span> <span class="nav-text">SIMD vs SIMT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#32"><span class="nav-number">5.1.3.</span> <span class="nav-text">32</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CUDA编程的组件与逻辑"><span class="nav-number">5.2.</span> <span class="nav-text">CUDA编程的组件与逻辑</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#理解线程束执行的本质"><span class="nav-number">5.3.</span> <span class="nav-text">理解线程束执行的本质</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#延迟隐藏"><span class="nav-number">5.4.</span> <span class="nav-text">延迟隐藏</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#同步"><span class="nav-number">5.5.</span> <span class="nav-text">同步</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#并行性表现"><span class="nav-number">5.6.</span> <span class="nav-text">并行性表现</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#避免分支分化"><span class="nav-number">5.7.</span> <span class="nav-text">避免分支分化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#并行规约问题"><span class="nav-number">5.7.1.</span> <span class="nav-text">并行规约问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#并行规约中的分化"><span class="nav-number">5.7.2.</span> <span class="nav-text">并行规约中的分化</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#展开循环"><span class="nav-number">5.8.</span> <span class="nav-text">展开循环</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#cuda内存模型"><span class="nav-number">6.</span> <span class="nav-text">cuda内存模型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#寄存器"><span class="nav-number">6.1.</span> <span class="nav-text">寄存器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#本地内存"><span class="nav-number">6.2.</span> <span class="nav-text">本地内存</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#共享内存"><span class="nav-number">6.3.</span> <span class="nav-text">共享内存</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#常量内存"><span class="nav-number">6.4.</span> <span class="nav-text">常量内存</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#纹理内存"><span class="nav-number">6.5.</span> <span class="nav-text">纹理内存</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#全局内存"><span class="nav-number">6.6.</span> <span class="nav-text">全局内存</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GPU缓存"><span class="nav-number">6.7.</span> <span class="nav-text">GPU缓存</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#静态全局内存"><span class="nav-number">6.8.</span> <span class="nav-text">静态全局内存</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#内存管理"><span class="nav-number">7.</span> <span class="nav-text">内存管理</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#内存分配和释放"><span class="nav-number">7.1.</span> <span class="nav-text">内存分配和释放</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#内存传输"><span class="nav-number">7.2.</span> <span class="nav-text">内存传输</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#固定内存"><span class="nav-number">7.3.</span> <span class="nav-text">固定内存</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#零拷贝内存"><span class="nav-number">7.4.</span> <span class="nav-text">零拷贝内存</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>
  <div id="DvelopmentTarget">     
  </div>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart" aria-hidden="false"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hao Yu</span>

  
</div>

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<span id="busuanzi_container_site_uv"> 
  本站访客数<span id="busuanzi_value_site_uv"></span>人次
</span>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  


  

  


  <script type="text/javascript" src="/js/src/love.js"></script>

</body>
</html>
